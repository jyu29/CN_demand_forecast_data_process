{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Spark application\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>Current session?</th></tr><tr><td>4</td><td>application_1578032391564_0005</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-10-226-39-185.subsidia.org:20888/proxy/application_1578032391564_0005/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-10-226-39-115.subsidia.org:8042/node/containerlogs/container_1578032391564_0005_01_000001/livy\">Link</a></td><td>✔</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6228b9762e594801a4a02dac93775a4f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession available as 'spark'.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bc265de861e141389fad07ab447c4853",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import sys\n",
    "\n",
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import SparkSession, Window\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DateType, BooleanType\n",
    "import pyspark.sql.functions as F\n",
    "#from pyspark.sql.functions import desc\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "\n",
    "import datetime\n",
    "from datetime import datetime, timedelta\n",
    "import numpy as np\n",
    "#import isoweek"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "028c92a1efea470f84f653ee373fede8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "u'2.4.3'"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .appName(\"data_refining_part_2_history_reconstruction\") \\\n",
    "    .config(\"spark.sql.broadcastTimeout\", \"36000\")\\\n",
    "    .getOrCreate()\n",
    "spark.version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d5518e578ed641929680bd274ed79fd8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "horizon = 10\n",
    "horizon_freq = '1W-SUN'\n",
    "\n",
    "prediction_length = horizon\n",
    "prediction_freq = '1W-SUN'\n",
    "season_length = 52\n",
    "\n",
    "dict_scope = {'marco': {'filter_type' : 'family', 'filter_val' : [12151, 230]},\n",
    "              'racket_sports': {'filter_type' : 'department', 'filter_val' : [402, 403, 404, 406, 408, 473, 474]},\n",
    "              'full_scope': {'filter_type' : '', 'filter_val' : []},\n",
    "              \"domyos_nov_2019\": {\"filter_type\" : \"family\", \"filter_val\" : [224, 12072, 600]}}\n",
    "              \n",
    "scope = 'domyos_nov_2019' # change the scope here\n",
    "first_test_cutoff = 201922 # change test period here, should be >= 201922"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "11f7089505454203bbfea6c3b8b17530",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "filter_val, filter_type = dict_scope[scope].values()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load clean data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "06361259eac049698a7370e9ca6ec013",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- week_id: integer (nullable = true)\n",
      " |-- date: date (nullable = true)\n",
      " |-- model: integer (nullable = true)\n",
      " |-- y: integer (nullable = true)\n",
      "\n",
      "+-------+----------+-----+---+\n",
      "|week_id|      date|model|  y|\n",
      "+-------+----------+-----+---+\n",
      "| 201510|2015-03-01|    1|  1|\n",
      "+-------+----------+-----+---+\n",
      "only showing top 1 row"
     ]
    }
   ],
   "source": [
    "actual_sales_schema = StructType([\n",
    "    StructField('week_id', IntegerType()),\n",
    "    StructField('date', DateType()),\n",
    "    StructField('model', IntegerType()),\n",
    "    StructField('y', IntegerType())])\n",
    "\n",
    "actual_sales = spark.read.csv('s3://fcst-workspace/qlik/data/clean/actual_sales.csv',\n",
    "                              schema=actual_sales_schema, sep='|', header=True)\n",
    "actual_sales.cache()\n",
    "actual_sales.printSchema()\n",
    "actual_sales.show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8f99de329fcf4e2cb31326b148de7b98",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- week_id: integer (nullable = true)\n",
      " |-- date: date (nullable = true)\n",
      " |-- model: integer (nullable = true)\n",
      " |-- y: integer (nullable = true)\n",
      "\n",
      "+-------+----------+------+---+\n",
      "|week_id|      date| model|  y|\n",
      "+-------+----------+------+---+\n",
      "| 201502|2015-01-04|226919| 52|\n",
      "+-------+----------+------+---+\n",
      "only showing top 1 row"
     ]
    }
   ],
   "source": [
    "active_sales = StructType([\n",
    "    StructField('week_id', IntegerType()),\n",
    "    StructField('date', DateType()),\n",
    "    StructField('model', IntegerType()),\n",
    "    StructField('y', IntegerType())])\n",
    "\n",
    "active_sales = spark.read.csv('s3://fcst-workspace/qlik/data/clean/active_sales.csv',\n",
    "                              schema=actual_sales_schema, sep='|', header=True)\n",
    "active_sales.cache()\n",
    "active_sales.printSchema()\n",
    "active_sales.show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2a416f79fd1749f58b45de858df34b2c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- model: integer (nullable = true)\n",
      " |-- model_label: string (nullable = true)\n",
      " |-- family: integer (nullable = true)\n",
      " |-- family_label: string (nullable = true)\n",
      " |-- sub_department: integer (nullable = true)\n",
      " |-- sub_department_label: string (nullable = true)\n",
      " |-- department: integer (nullable = true)\n",
      " |-- department_label: string (nullable = true)\n",
      " |-- univers: integer (nullable = true)\n",
      " |-- univers_label: string (nullable = true)\n",
      " |-- product_nature: integer (nullable = true)\n",
      " |-- product_nature_label: string (nullable = true)\n",
      " |-- category_label: string (nullable = true)\n",
      " |-- range_level: integer (nullable = true)\n",
      "\n",
      "+-----+-----------+------+-------------+--------------+--------------------+----------+----------------+-------+-------------+--------------+--------------------+--------------+-----------+\n",
      "|model|model_label|family| family_label|sub_department|sub_department_label|department|department_label|univers|univers_label|product_nature|product_nature_label|category_label|range_level|\n",
      "+-----+-----------+------+-------------+--------------+--------------------+----------+----------------+-------+-------------+--------------+--------------------+--------------+-----------+\n",
      "| 2697|    UNKNOWN|  5152|DIRECT ORDERS|          4993|       DIRECT ORDERS|       142| Decapro and dir|     10|     FOOTBALL|             1|             UNKNOWN|       UNKNOWN|          4|\n",
      "+-----+-----------+------+-------------+--------------+--------------------+----------+----------------+-------+-------------+--------------+--------------------+--------------+-----------+\n",
      "only showing top 1 row"
     ]
    }
   ],
   "source": [
    "model_info = spark.read.csv('s3://fcst-workspace/qlik/data/clean/model_info.csv',\n",
    "                            inferSchema=True, sep='|', header=True)\n",
    "model_info.cache()\n",
    "model_info.printSchema()\n",
    "model_info.show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7c9e06d3f4aa42ea80e8fe8e07e11645",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if scope != 'full_scope':\n",
    "    \n",
    "    actual_sales = actual_sales.join(model_info.select(model_info['model'], model_info[filter_type]), 'model', how='left')\n",
    "\n",
    "    actual_sales = actual_sales.filter(actual_sales[filter_type].isin(filter_val))\\\n",
    "                               .drop(filter_type)\n",
    "    \n",
    "    active_sales = active_sales.join(model_info.select(model_info['model'], model_info[filter_type]), 'model', how='left')\n",
    "\n",
    "    active_sales = active_sales.filter(active_sales[filter_type].isin(filter_val))\\\n",
    "                               .drop(filter_type)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define History Reconstruction Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "afebefece4c54314b52480c29b650290",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "cutoff_week_id_test = 201834\n",
    "min_ts_len = 160\n",
    "horizon = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7b2f608cf26048df87b0e4fd7eb2971a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def sup_week(week_id):\n",
    "    \n",
    "    week_id = str(week_id)\n",
    "    y, w = int(week_id[:4]), int(week_id[4:])\n",
    "    \n",
    "    if w == 1:\n",
    "        w = '52'\n",
    "        y = str(y - 1)\n",
    "        \n",
    "    elif len(str(w))==1:\n",
    "        w = w - 1\n",
    "        y = str(y)\n",
    "        w = '0' + str(w)\n",
    "        \n",
    "    elif w == 10:\n",
    "        w = w - 1\n",
    "        y = str(y)\n",
    "        w = '0' + str(w)\n",
    "    \n",
    "    else:\n",
    "        w = str(w - 1)\n",
    "        y = str(y)\n",
    "    \n",
    "    n_wk = y + w\n",
    "    return int(n_wk)\n",
    "\n",
    "\n",
    "def next_week(week_id):\n",
    "    week_id = str(week_id)\n",
    "    y, w = int(week_id[:4]), int(week_id[4:])\n",
    "    \n",
    "    if w == 9:\n",
    "        w = '10'\n",
    "        y = str(y)\n",
    "    elif len(str(w))==1:\n",
    "        w = w + 1\n",
    "        y = str(y)\n",
    "        w = '0' + str(w)\n",
    "    elif w == 52:\n",
    "        w = '01'\n",
    "        y = str(y + 1)\n",
    "    else:\n",
    "        w = str(w + 1)\n",
    "        y = str(y)\n",
    "    n_wk = y + w\n",
    "    return int(n_wk)\n",
    "\n",
    "\n",
    "def __add_week(week, nb):\n",
    "    if nb < 0 :\n",
    "        for i in range(abs(nb)):\n",
    "            week = sup_week(week)\n",
    "    else:\n",
    "        for i in range(nb):\n",
    "            week = next_week(week)\n",
    "    \n",
    "    return week\n",
    "\n",
    "\n",
    "def find_weeks(start, end):\n",
    "    l = []\n",
    "    l.append(start)\n",
    "    start = str(start)+'0'\n",
    "    start = datetime.strptime(start, '%Y%W%w')\n",
    "    end = sup_week(end)\n",
    "    end = str(end)+'0'\n",
    "    end = datetime.strptime(end, '%Y%W%w')\n",
    "      \n",
    "    \n",
    "    for i in range((end - start).days + 1):\n",
    "        d = (start + timedelta(days=i)).isocalendar()[:2] # e.g. (2011, 52)\n",
    "        yearweek = '{}{:02}'.format(*d) # e.g. \"201152\"\n",
    "        l.append(int(yearweek))\n",
    "    \n",
    "    \n",
    "    return sorted(set(l))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "218ff06918a943bcb92982cbfca665af",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def reconstruct_history(train_data_cutoff, actual_sales, model_info,\n",
    "                        cluster_keys=['product_nature', 'family'], min_ts_len=160):\n",
    "\n",
    "\n",
    "    last_week = train_data_cutoff.agg(F.max('week_id').alias('last_week'))\n",
    "    model_to_keep = train_data_cutoff.groupBy('model').agg(F.max('week_id').alias('last_active_week'))\n",
    "    model_to_keep = model_to_keep.join(last_week, last_week.last_week == model_to_keep.last_active_week, 'inner').select(model_to_keep.model)\n",
    "    train_data_cutoff = train_data_cutoff.join(model_to_keep, 'model', how='inner')\n",
    "\n",
    "\n",
    "    df_date = actual_sales.select(['week_id', 'date']).distinct()\n",
    "    y_not_null = train_data_cutoff.where(train_data_cutoff.y.isNotNull())\n",
    "\n",
    "    max_week = train_data_cutoff.select(F.max('week_id')).collect()[0][0]\n",
    "    min_week = train_data_cutoff.select(F.min('week_id')).collect()[0][0]\n",
    "\n",
    "    list_weeks = find_weeks(min_week, max_week)\n",
    "    list_weeks = spark.createDataFrame(list_weeks, IntegerType()).selectExpr('value as week_id')\n",
    "    list_models = train_data_cutoff.select(train_data_cutoff.model).distinct()\n",
    "\n",
    "    full = list_weeks.crossJoin(list_models)\n",
    "    full_actives_sales = full.join(train_data_cutoff, ['week_id', 'model'], how='left')\n",
    "    full_actives_sales = full_actives_sales[full_actives_sales['week_id'] < cutoff_week_id_test]\n",
    "\n",
    "\n",
    "    # add cluster infos\n",
    "\n",
    "    mdl_inf = model_info.select(['model'] + cluster_keys)\n",
    "\n",
    "    complete_ts = full_actives_sales.join(mdl_inf, 'model', how='left').drop('date')\n",
    "    complete_ts = complete_ts.join(df_date, 'week_id', how='inner')\n",
    "\n",
    "\n",
    "    # Calculate the average sales per cluster and week from actual_sales\n",
    "    all_sales = actual_sales.join(mdl_inf, 'model', how='left')\n",
    "    all_sales = all_sales.dropna()\n",
    "    join_key = ['week_id', 'date'] + cluster_keys\n",
    "    all_sales = all_sales.groupBy(join_key).agg(F.mean('y').alias('mean_cluster_y'))\n",
    "\n",
    "\n",
    "    # ad it to complete_ts\n",
    "    complete_ts = complete_ts.join(all_sales, ['week_id', 'date', 'product_nature', 'family'], how='left')\n",
    "\n",
    "\n",
    "    #SCale factor\n",
    "    complete_ts = complete_ts.withColumn('row_scale_factor', complete_ts.y / complete_ts.mean_cluster_y)\n",
    "\n",
    "    model_scale_factor = complete_ts.groupBy('model').agg(F.mean('row_scale_factor').alias('model_scale_factor'))\n",
    "\n",
    "    complete_ts = complete_ts.join(model_scale_factor, ['model'], how='left')\n",
    "\n",
    "    #assert complete_ts.where(complete_ts.model_scale_factor.isNull()).count() == 0\n",
    "\n",
    "\n",
    "    #compute fake Y\n",
    "    complete_ts = complete_ts.withColumn('fake_y', complete_ts.mean_cluster_y * complete_ts.model_scale_factor)\n",
    "    complete_ts = complete_ts.fillna(0, subset=['fake_y'])\n",
    "\n",
    "\n",
    "    start_end = y_not_null.groupBy('model').agg(F.min('date').alias('start_date'), F.max('date').alias('end_date'))\n",
    "    complete_ts = complete_ts.join(start_end, 'model', how='left')\n",
    "\n",
    "\n",
    "    complete_ts = complete_ts.withColumn('age', (F.datediff(F.col('date'), F.col('start_date'))) / (7) + 1 )\\\n",
    "                             .withColumn('length', (F.datediff(F.col('end_date'), F.col('date'))) / (7) + 1 )\\\n",
    "                             .withColumn('is_y_sup', complete_ts.y > complete_ts.fake_y)\n",
    "\n",
    "    complete_ts = complete_ts.withColumn('is_y_sup', complete_ts.y > complete_ts.fake_y)\n",
    "    complete_ts = complete_ts.na.fill({'y': 0, 'is_y_sup': False})\n",
    "\n",
    "\n",
    "    end_impl_period = complete_ts.filter(complete_ts.is_y_sup == True).select(['model', 'age']).groupBy('model').agg(F.min('age').alias('end_impl_period'))\n",
    "\n",
    "    complete_ts = complete_ts.join(end_impl_period, on=['model'], how='left')\n",
    "\n",
    "    \"\"\"\n",
    "    complete_ts = complete_ts.withColumn('y', \n",
    "                F.when((complete_ts.age <= 0) & (complete_ts.length <= min_ts_len), complete_ts.fake_y)\\\n",
    "                .when((complete_ts.age > 0) & (complete_ts.age < complete_ts.end_impl_period), complete_ts.y)\\\n",
    "                .otherwise(None))\n",
    "    \"\"\"\n",
    "\n",
    "    complete_ts = complete_ts.withColumn('y', \n",
    "                F.when(\n",
    "                    ((complete_ts.age <= 0) & (complete_ts.length <= min_ts_len)) | \\\n",
    "                    ((complete_ts.age > 0) & (complete_ts.age < complete_ts.end_impl_period)), complete_ts.fake_y)\\\n",
    "                 .otherwise(complete_ts.y))\n",
    "\n",
    "\n",
    "    complete_ts = complete_ts.select(['week_id', 'date', 'model', 'y']).dropna(subset=('week_id', 'date', 'model', 'y'))\n",
    "\n",
    "    complete_ts = complete_ts.where(complete_ts.y > 0).orderBy(['week_id', 'model'])\n",
    "\n",
    "\n",
    "    return complete_ts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate training data used to forecast validation & test cutoffs\n",
    "- For each cutoff, keep only models active the week before the cutoff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "417a584a01a846a29ec5229f33234784",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+\n",
      "|week_id|\n",
      "+-------+\n",
      "| 201922|\n",
      "| 201923|\n",
      "| 201924|\n",
      "+-------+\n",
      "only showing top 3 rows"
     ]
    }
   ],
   "source": [
    "current_cutoff = next_week(actual_sales.select(F.max('week_id')).collect()[0][0])\n",
    "\n",
    "\n",
    "cutoff_week_test = active_sales.where(active_sales.week_id >= first_test_cutoff).select(active_sales.week_id).distinct().orderBy('week_id')\n",
    "\n",
    "nRow = spark.createDataFrame([[current_cutoff]])\n",
    "cutoff_week_test = cutoff_week_test.union(nRow)\n",
    "\n",
    "cutoff_week_test.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "34d96f7aef5f4f1faa356018ee26587c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+\n",
      "|week_id|\n",
      "+-------+\n",
      "| 201902|\n",
      "| 201814|\n",
      "| 201834|\n",
      "+-------+\n",
      "only showing top 3 rows"
     ]
    }
   ],
   "source": [
    "cutoff_week_val = active_sales.where(active_sales.week_id < first_test_cutoff).select(active_sales.week_id).distinct().orderBy('week_id')\n",
    "\n",
    "max_cutoff_week_val = cutoff_week_val.select(F.max('week_id')).collect()[0][0]\n",
    "\n",
    "# keep the last 60 dates before test set for validation\n",
    "\n",
    "_sup_week = __add_week(max_cutoff_week_val, (-horizon +1))\n",
    "_inf_week = __add_week(max_cutoff_week_val, -(60 + horizon -1))\n",
    "\n",
    "\n",
    "cutoff_week_val = cutoff_week_val.where((cutoff_week_val.week_id > _inf_week) & (cutoff_week_val.week_id <= _sup_week))\n",
    "\n",
    "# keep only one cutoff every 10 dates\n",
    "idx_to_keep = ((np.arange(cutoff_week_val.count()) + 1) % 10 == 0)\n",
    "idx = np.arange(1, len(idx_to_keep)+1)\n",
    "idx_to_keep = np.c_[idx_to_keep, idx]\n",
    "\n",
    "nRow = spark.createDataFrame(idx_to_keep.tolist()).selectExpr('_1 as value', '_2 as id')\n",
    "cutoff_week_val = cutoff_week_val.withColumn('id', F.row_number().over(Window.orderBy('week_id')))\n",
    "cutoff_week_val = cutoff_week_val.join(nRow, 'id', how='inner').drop('id')\n",
    "cutoff_week_val = cutoff_week_val.where(cutoff_week_val.value == 1).drop('value')\n",
    "\n",
    "cutoff_week_val.show(3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7038d527db4344a78659399cd8d05250",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#weeks cutoff\n",
    "iterate_week = cutoff_week_val.union(cutoff_week_test)\n",
    "iterate_week = [row.week_id for row in iterate_week.collect()]\n",
    "\n",
    "test = sorted(iterate_week)[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "89c3db5bad8849cfad9a6e63b53d9e3b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[201814]"
     ]
    }
   ],
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for cutoff_week_id in sorted(test):\n",
    "\n",
    "    #print('Generating train data for cutoff', str(cutoff_week_id))\n",
    "            \n",
    "    train_data_cutoff = active_sales.where(active_sales.week_id < cutoff_week_id)\n",
    "    \n",
    "    #print('Initial nb of models:', train_data_cutoff.select(train_data_cutoff.model).distinct().count())\n",
    "    \n",
    "    last_week = train_data_cutoff.agg(F.max('week_id').alias('last_week'))\n",
    "    \n",
    "    model_to_keep = train_data_cutoff.groupBy('model').agg(F.max('week_id').alias('last_active_week'))\n",
    "    \n",
    "    model_to_keep = model_to_keep.join(last_week, last_week.last_week == model_to_keep.last_active_week, 'inner').select(model_to_keep.model)\n",
    "    \n",
    "    train_data_cutoff = train_data_cutoff.join(model_to_keep, 'model', how='inner')\n",
    "    \n",
    "    # Reconstruct a fake history\n",
    "    train_data_cutoff = reconstruct_history(train_data_cutoff, actual_sales, model_info)\n",
    "    \n",
    "    train_data_cutoff.write.parquet('s3://fcst-refined-demand-forecast-dev/part_3/train_data_cutoff/{}'.format(str(cutoff_week_id)), mode=\"overwrite\")\n",
    "    \n",
    "    #print('Nb model kept:', train_data_cutoff.select(train_data_cutoff.model).distinct().count())\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0d7dac9c00f54421b0dc2852e5f1396d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('1', 0.07133197784423828)\n",
      "('2', 0.00950312614440918)\n",
      "('3', 0.01076507568359375)\n",
      "('4', 0.006669044494628906)\n",
      "('5', 0.01723003387451172)\n",
      "('6', 4.875576019287109)\n",
      "('7', 81.77478790283203)\n",
      "('8', 2.5987625122070312e-05)"
     ]
    }
   ],
   "source": [
    "import time\n",
    "#print('Generating train data for cutoff', str(cutoff_week_id))\n",
    "\n",
    "t0 = time.time()\n",
    "train_data_cutoff = active_sales.where(active_sales.week_id < 201814)\n",
    "t1 = time.time()\n",
    "total = t1-t0\n",
    "print('1', total)\n",
    "\n",
    "t0 = time.time()\n",
    "#print('Initial nb of models:', train_data_cutoff.select(train_data_cutoff.model).distinct().count())\n",
    "last_week = train_data_cutoff.agg(F.max('week_id').alias('last_week'))\n",
    "t1 = time.time()\n",
    "total = t1-t0\n",
    "print('2', total)\n",
    "\n",
    "\n",
    "t0 = time.time()\n",
    "model_to_keep = train_data_cutoff.groupBy('model').agg(F.max('week_id').alias('last_active_week'))\n",
    "t1 = time.time()\n",
    "total = t1-t0\n",
    "print('3', total)\n",
    "\n",
    "t0 = time.time()\n",
    "model_to_keep = model_to_keep.join(last_week, last_week.last_week == model_to_keep.last_active_week, 'inner').select(model_to_keep.model)\n",
    "t1 = time.time()\n",
    "total = t1-t0\n",
    "print('4', total)\n",
    "\n",
    "t0 = time.time()\n",
    "train_data_cutoff = train_data_cutoff.join(model_to_keep, 'model', how='inner')\n",
    "t1 = time.time()\n",
    "total = t1-t0\n",
    "print('5', total)\n",
    "\n",
    "# Reconstruct a fake history\n",
    "t0 = time.time()\n",
    "train_data_cutoff = reconstruct_history(train_data_cutoff, actual_sales, model_info)\n",
    "t1 = time.time()\n",
    "total = t1-t0\n",
    "print('6', total)\n",
    "\n",
    "t0 = time.time()\n",
    "train_data_cutoff.write.parquet('s3://fcst-refined-demand-forecast-dev/part_3/train_data_cutoff/201814', mode=\"overwrite\")\n",
    "t1 = time.time()\n",
    "total = t1-t0\n",
    "print('7', total)\n",
    "\n",
    "t0 = time.time()\n",
    "#print('Nb model kept:', train_data_cutoff.select(train_data_cutoff.model).distinct().count())\n",
    "t1 = time.time()\n",
    "total = t1-t0\n",
    "print('8', total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "('1', 0.07133197784423828)\n",
    "('2', 0.00950312614440918)\n",
    "('3', 0.01076507568359375)\n",
    "('4', 0.006669044494628906)\n",
    "('5', 0.01723003387451172)\n",
    "('6', 4.875576019287109)\n",
    "('7', 81.77478790283203)\n",
    "('8', 2.5987625122070312e-05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Sparkmagic (PySpark)",
   "language": "",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 2
   },
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
