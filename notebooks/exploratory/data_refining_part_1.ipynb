{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Spark application\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>Current session?</th></tr><tr><td>6</td><td>application_1579768906817_0007</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-10-226-39-223.subsidia.org:20888/proxy/application_1579768906817_0007/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-10-226-39-185.subsidia.org:8042/node/containerlogs/container_1579768906817_0007_01_000001/livy\">Link</a></td><td>✔</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession available as 'spark'.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pyspark import SparkContext, SparkConf, StorageLevel\n",
    "from pyspark.sql import SparkSession, Window\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "\n",
    "import pyspark.sql.functions as F\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#conf=SparkConf() \\\n",
    "#    .setAll([('spark.executor.memory', '70g'),\n",
    "#             ('spark.driver.memory', '50g')])\n",
    "#\n",
    "#spark = SparkSession.builder. \\\n",
    "#        config(conf=conf). \\\n",
    "#        getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "conf=SparkConf() \\\n",
    "    .setAll([('spark.sql.shuffle.partitions', '80'),\n",
    "             ('spark.default.parallelism', '80'),\n",
    "             ('spark.autoBroadcastJoinThreshold', '15485760'),\n",
    "             ('spark.dynamicAllocation.enabled', 'false'),\n",
    "             ('spark.executor.instances', '8'),\n",
    "             ('spark.executor.memory', '38g'),\n",
    "             ('spark.driver.memory', '38g'),\n",
    "             ('spark.driver.cores', '5'),\n",
    "             ('spark.memory.storageFraction', '0.3'),\n",
    "             ('spark.memory.fraction', '0.7'),\n",
    "             ('spark.executor.memoryOverhead', '4g'),\n",
    "             ('spark.executor.cores', '5'),\n",
    "             ('spark.hadoop.mapreduce.fileoutputcommitter.algorithm.version', '2')\n",
    "])\n",
    "\n",
    "spark = SparkSession.builder. \\\n",
    "        config(conf=conf). \\\n",
    "        getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#for x in spark.sparkContext.getConf().getAll():\n",
    "#    print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def read_parquet_s3(app, s3_path):\n",
    "    \n",
    "    df = app.read.parquet(s3_path)\n",
    "    path_sinature = \">> Parquet file read from \" + s3_path\n",
    "    \n",
    "    #print(\"{:<32}\".format(path_sinature) + '\\n')\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "def write_parquet_s3(spark_df, bucket, file_path):\n",
    "    \"\"\"Writing spark Dataframe into s3 as a parquet file.\n",
    "    \n",
    "    Parameters:\n",
    "    spark_df (pyspark.sql.dataframe): the spark Dataframe.\n",
    "    bucket (string): the s3 bucket name.\n",
    "    file_path (string) : the table name or directory.\n",
    "    \n",
    "    Returns:\n",
    "    \"\"\"\n",
    "    s3_path = 's3://{}/{}'.format(bucket, file_path)\n",
    "    spark_df.write.parquet(s3_path, mode=\"overwrite\")\n",
    "    \n",
    "    #print(\">> Parquet file written on {}\".format(s3_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "first_week_id = 201501\n",
    "purch_org = 'Z001'\n",
    "sales_org = 'Z002'\n",
    "bucket = 's3://fcst-clean-dev/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load all needed clean data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tdt = read_parquet_s3(spark, bucket + 'f_transaction_detail/*/')\n",
    "dyd = read_parquet_s3(spark, bucket + 'f_delivery_detail/*/')\n",
    "\n",
    "sku = read_parquet_s3(spark, bucket + 'd_sku/')\n",
    "bu = read_parquet_s3(spark, bucket + 'd_business_unit/')\n",
    "\n",
    "sapb = read_parquet_s3(spark, bucket + 'sites_attribut_0plant_branches_h/')\n",
    "sdm = read_parquet_s3(spark, bucket + 'd_sales_data_material_h/')\n",
    "\n",
    "day = read_parquet_s3(spark, bucket + 'd_day/')\n",
    "week = read_parquet_s3(spark, bucket + 'd_week/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Actual_sales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "actual_sales_offline = tdt \\\n",
    "    .join(day,\n",
    "          on=F.to_date(tdt.tdt_date_to_ordered, 'yyyy-MM-dd') == day.day_id_day,\n",
    "          how='inner') \\\n",
    "    .join(week, \n",
    "          on=day.wee_id_week == week.wee_id_week, \n",
    "          how='inner') \\\n",
    "    .join(sku,\n",
    "          on=tdt.sku_idr_sku == sku.sku_idr_sku, \n",
    "          how='inner') \\\n",
    "    .join(bu, \n",
    "          on=tdt.but_idr_business_unit == bu.but_idr_business_unit, \n",
    "          how='inner') \\\n",
    "    .join(sapb,\n",
    "          on=bu.but_num_business_unit.cast('string') == \\\n",
    "             F.regexp_replace(sapb.plant_id, '^0*|\\s',''),\n",
    "          how='inner') \\\n",
    "    .filter(tdt.the_to_type == 'offline') \\\n",
    "    .filter(week.wee_id_week >= first_week_id) \\\n",
    "    .filter(~sku.unv_num_univers.isin([0, 14, 89, 90])) \\\n",
    "    .filter(sku.mdl_num_model_r3.isNotNull()) \\\n",
    "    .filter(sapb.purch_org == purch_org) \\\n",
    "    .filter(sapb.sapsrc == 'PRT') \\\n",
    "    .filter(F.current_timestamp().between(sapb.date_begin, sapb.date_end)) \\\n",
    "    .select(week.wee_id_week.cast('int').alias('week_id'),\n",
    "            week.day_first_day_week.alias('date'),\n",
    "            sku.mdl_num_model_r3.alias('model'),\n",
    "            tdt.f_qty_item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "actual_sales_online = dyd \\\n",
    "    .join(day,\n",
    "          on=F.to_date(dyd.tdt_date_to_ordered, 'yyyy-MM-dd') == day.day_id_day,\n",
    "          how='inner') \\\n",
    "    .join(week, \n",
    "          on=day.wee_id_week == week.wee_id_week, \n",
    "          how='inner') \\\n",
    "    .join(sku, \n",
    "          on=dyd.sku_idr_sku == sku.sku_idr_sku, \n",
    "          how='inner') \\\n",
    "    .join(bu, \n",
    "          on=dyd.but_idr_business_unit_economical == bu.but_idr_business_unit, \n",
    "          how='inner') \\\n",
    "    .join(sapb,\n",
    "          on=bu.but_num_business_unit.cast('string') == \\\n",
    "             F.regexp_replace(sapb.plant_id, '^0*|\\s',''),\n",
    "          how='inner') \\\n",
    "    .filter(dyd.the_to_type == 'online') \\\n",
    "    .filter(week.wee_id_week >= first_week_id) \\\n",
    "    .filter(~sku.unv_num_univers.isin([0, 14, 89, 90])) \\\n",
    "    .filter(sku.mdl_num_model_r3.isNotNull()) \\\n",
    "    .filter(sapb.purch_org == purch_org) \\\n",
    "    .filter(sapb.sapsrc == 'PRT') \\\n",
    "    .filter(F.current_timestamp().between(sapb.date_begin, sapb.date_end)) \\\n",
    "    .select(week.wee_id_week.cast('int').alias('week_id'),\n",
    "            week.day_first_day_week.alias('date'),\n",
    "            sku.mdl_num_model_r3.alias('model'),\n",
    "            dyd.f_qty_item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame[week_id: int, date: date, model: bigint, y: bigint]"
     ]
    }
   ],
   "source": [
    "actual_sales = actual_sales_offline.union(actual_sales_online) \\\n",
    "    .groupby(['week_id', 'date', 'model']) \\\n",
    "    .agg(F.sum('f_qty_item').alias('y')) \\\n",
    "    .filter(F.col('y') > 0) \\\n",
    "    .repartition(200)\n",
    "\n",
    "actual_sales.cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Lifestage_update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame[model: bigint, date_begin: date, date_end: date, lifestage: int]"
     ]
    }
   ],
   "source": [
    "lifestage_update = sdm \\\n",
    "    .join(sku, \n",
    "          on=F.regexp_replace(sdm.material_id, '^0*|\\s','') == \\\n",
    "             sku.mdl_num_model_r3.cast('string'),\n",
    "          how='inner') \\\n",
    "    .filter(sdm.sales_org == sales_org) \\\n",
    "    .filter(sdm.sap_source == 'PRT') \\\n",
    "    .filter(sdm.lifestage != '') \\\n",
    "    .filter(sdm.distrib_channel == '02') \\\n",
    "    .filter(sku.mdl_num_model_r3.isNotNull()) \\\n",
    "    .filter(~sku.unv_num_univers.isin([0, 14, 89, 90])) \\\n",
    "    .filter(F.current_timestamp().between(sku.sku_date_begin, sku.sku_date_end)) \\\n",
    "    .withColumn(\"date_end\",\n",
    "                F.when(sdm.date_end == '2999-12-31',\n",
    "                       F.to_date(F.lit('2100-12-31'), 'yyyy-MM-dd')) \\\n",
    "                       .otherwise(sdm.date_end)) \\\n",
    "    .select(sku.mdl_num_model_r3.alias('model'), \n",
    "            sdm.date_begin,\n",
    "            \"date_end\",\n",
    "            sdm.lifestage.cast('int').alias('lifestage')) \\\n",
    "    .drop_duplicates() \\\n",
    "    .repartition(200)\n",
    "\n",
    "lifestage_update.cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Model_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame[model: bigint, model_label: string, family: bigint, family_label: string, sub_department: bigint, sub_department_label: string, department: bigint, department_label: string, univers: bigint, univers_label: string, product_nature: decimal(7,0), product_nature_label: string, category_label: string]"
     ]
    }
   ],
   "source": [
    "model_info = sku \\\n",
    "    .filter(sku.mdl_num_model_r3.isNotNull()) \\\n",
    "    .filter(~sku.unv_num_univers.isin([0, 14, 89, 90])) \\\n",
    "    .filter(F.current_timestamp().between(sku.sku_date_begin, sku.sku_date_end)) \\\n",
    "    .select(sku.mdl_num_model_r3.alias('model'),\n",
    "            sku.mdl_label.alias('model_label'),\n",
    "            sku.fam_num_family.alias('family'),\n",
    "            sku.family_label.alias('family_label'),\n",
    "            sku.sdp_num_sub_department.alias('sub_department'),\n",
    "            sku.sdp_label.alias('sub_department_label'),\n",
    "            sku.dpt_num_department.alias('department'),\n",
    "            sku.unv_label.alias('department_label'),\n",
    "            sku.unv_num_univers.alias('univers'),\n",
    "            sku.mdl_label.alias('univers_label'),\n",
    "            sku.pnt_num_product_nature.alias('product_nature'),\n",
    "            sku.product_nature_label.alias('product_nature_label'),\n",
    "            sku.category_label.alias('category_label')) \\\n",
    "    .drop_duplicates() \\\n",
    "    .repartition(200)\n",
    "\n",
    "model_info.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10513053\n",
      "316270\n",
      "377747"
     ]
    }
   ],
   "source": [
    "print(actual_sales.count())\n",
    "print(lifestage_update.count())\n",
    "print(model_info.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Delete incomplete last week\n",
    "- To be sure to have complete weeks (Sunday --> Saturday) regardless raw data extraction date, the last week of sales are deleted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#actual_sales = actual_sales \\\n",
    "#    .filter(actual_sales.week_id >= 201838) \\\n",
    "#    .filter(actual_sales.week_id <= 201841)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#print(actual_sales.count())\n",
    "\n",
    "max_week_id = actual_sales.select(F.max('week_id')).collect()[0][0]\n",
    "\n",
    "actual_sales = actual_sales \\\n",
    "    .filter(actual_sales.week_id < max_week_id) \\\n",
    "    .orderBy('model', 'date')\n",
    "\n",
    "#actual_sales.cache()\n",
    "\n",
    "#print(actual_sales.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Format life stages values\n",
    "/!\\ life stage values are only historized since September 10, 2018\n",
    "\n",
    "##### 1) Keep only usefull life stage values: models in actual sales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#print(lifestage_update.count())\n",
    "\n",
    "lifestage_update = lifestage_update.join(actual_sales.select('model').drop_duplicates(), \n",
    "                                         on='model', how='inner')\n",
    "\n",
    "#lifestage_update.cache()\n",
    "\n",
    "#print(lifestage_update.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2) Life stage updates ==> Life stage by week"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Calculates all possible date/model combinations associated with a life stage update\n",
    "min_date = lifestage_update.select(F.min('date_begin')).collect()[0][0]\n",
    "\n",
    "all_lifestage_date = actual_sales \\\n",
    "    .filter(actual_sales.date >= min_date) \\\n",
    "    .select('date') \\\n",
    "    .drop_duplicates() \\\n",
    "    .orderBy('date')\n",
    "\n",
    "all_lifestage_model = lifestage_update.select('model').drop_duplicates().orderBy('model')\n",
    "\n",
    "date_model = all_lifestage_date.crossJoin(all_lifestage_model)\n",
    "\n",
    "# Calculate lifestage by date\n",
    "model_lifestage = date_model.join(lifestage_update, on='model', how='left')\n",
    "model_lifestage = model_lifestage \\\n",
    "    .filter((model_lifestage.date >= model_lifestage.date_begin) &\n",
    "            (model_lifestage.date <= model_lifestage.date_end)) \\\n",
    "    .drop('date_begin', 'date_end')\n",
    "\n",
    "# The previous filter removes combinations that do not match the update dates.\n",
    "# But sometimes the update dates do not cover all periods, \n",
    "# which causes some dates to disappear, even during the model's activity periods.\n",
    "# To avoid this problem, we must merge again with all combinations to be sure \n",
    "# not to lose anything.\n",
    "model_lifestage = date_model.join(model_lifestage, on=['date', 'model'], how='left')\n",
    "\n",
    "#model_lifestage.cache()\n",
    "\n",
    "#print(model_lifestage.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3) Deal with duplicates rows & NA\n",
    "- If we have several life stage information for the same model and date, then we take the minimum\n",
    "- If no life stage is filled in, we take the last known value (if exists)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_lifestage = model_lifestage \\\n",
    "    .groupby(['date', 'model']) \\\n",
    "    .agg(F.min('lifestage').alias('lifestage'))\n",
    "\n",
    "# This is a ffil by group in pyspark ==> OMG\n",
    "window = Window.partitionBy('model')\\\n",
    "               .orderBy('date')\\\n",
    "               .rowsBetween(-sys.maxsize, 0)\n",
    "\n",
    "ffilled_lifestage = F.last(model_lifestage['lifestage'], ignorenulls=True).over(window)\n",
    "\n",
    "model_lifestage = model_lifestage.withColumn('lifestage', ffilled_lifestage)\n",
    "\n",
    "#model_lifestage.cache()\n",
    "\n",
    "#print(model_lifestage.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 4) Deal with zombie models\n",
    "If the life stage changes from active (1) to inactive (2 or more) and then back active, we consider the model is a zombie.  \n",
    "This new life may have a different sales behaviour than the previous one, so it's better to pretend that the latter is the only one that never existed.  \n",
    "\n",
    "For example, if the life stage looks like this: **1 1 1 3 3 3 1 1 1**, we only keep that: **3 1 1 1**.  \n",
    "Note that we still keep the last inactive value (here 3) before life stages 1 in order to avoid that the model life stages are considered incomplete in the following session."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#print(model_lifestage.count())\n",
    "\n",
    "model_lifestage = model_lifestage \\\n",
    "    .withColumn('lifestage_shift', \n",
    "                F.lag(model_lifestage['lifestage']) \\\n",
    "                      .over(Window.partitionBy(\"model\").orderBy(F.desc('date'))))\n",
    "\n",
    "model_lifestage = model_lifestage \\\n",
    "    .withColumn('diff_shift', model_lifestage['lifestage'] - \\\n",
    "                              model_lifestage['lifestage_shift'])\n",
    "\n",
    "df_cut_date = model_lifestage.filter(model_lifestage.diff_shift > 0)\n",
    "\n",
    "df_cut_date = df_cut_date \\\n",
    "    .groupBy('model') \\\n",
    "    .agg(F.max('date').alias('cut_date'))\n",
    "\n",
    "model_lifestage = model_lifestage.join(df_cut_date, on=['model'], how='left')\n",
    "\n",
    "# if no cut_date, fill by an old one\n",
    "model_lifestage = model_lifestage \\\n",
    "    .withColumn('cut_date', F.when(F.col('cut_date').isNull(),\n",
    "                                   F.to_date(F.lit('1993-04-15'), 'yyyy-MM-dd')) \\\n",
    "                                   .otherwise(F.col('cut_date')))\n",
    "\n",
    "model_lifestage = model_lifestage \\\n",
    "    .filter(model_lifestage.date >= model_lifestage.cut_date) \\\n",
    "    .select(['date', 'model', 'lifestage'])\n",
    "\n",
    "#model_lifestage.cache()\n",
    "\n",
    "#print(model_lifestage.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Match sales and life stages & rebuild incomplete life stages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1) Complete sales\n",
    "- Fill missing quantities by 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Calculates all possible date/model combinations from actual sales\n",
    "all_sales_model = actual_sales.select('model').orderBy('model').drop_duplicates()\n",
    "all_sales_date = actual_sales.select('date').orderBy('date').drop_duplicates()\n",
    "\n",
    "date_model = all_sales_model.crossJoin(all_sales_date)\n",
    "\n",
    "# Add corresponding week id\n",
    "date_model = date_model.join(actual_sales.select(['date', 'week_id']).drop_duplicates(), \n",
    "                             on=['date'], how='inner')\n",
    "\n",
    "# Add actual sales\n",
    "complete_ts = date_model.join(actual_sales, on=['date', 'model', 'week_id'], how='left')\n",
    "complete_ts = complete_ts.select(actual_sales.columns)\n",
    "\n",
    "# Fill NaN (no sales recorded) by 0\n",
    "complete_ts = complete_ts.fillna(0, subset=['y'])\n",
    "\n",
    "#complete_ts.cache()\n",
    "\n",
    "#print(complete_ts.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2) Add model life stage by week"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "complete_ts = complete_ts.join(model_lifestage, ['date', 'model'], how='left')\n",
    "\n",
    "#complete_ts.cache()\n",
    "\n",
    "#print(complete_ts.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3) Rebuild incomplete life stages\n",
    "/!\\ Reminder: the life stage values are only historized since September 10, 2018\n",
    "- If the life stage value is 1 at the first historized date \n",
    "- And we observe sales in the previous and consecutive weeks\n",
    "- Then we fill the life stage values of these weeks with 1 as well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def add_column_index(df, col_name): \n",
    "    new_schema = StructType(df.schema.fields + [StructField(col_name, LongType(), False),])\n",
    "    return df.rdd.zipWithIndex().map(lambda row: row[0] + (row[1], )).toDF(schema=new_schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "931caff7f5de417bb2ff8b06e2356ef0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "An error was encountered:\n",
      "Invalid status code '400' from http://myemr.subsidia.org:8998/sessions/6/statements/22 with error payload: {\"msg\":\"requirement failed: Session isn't active.\"}\n"
     ]
    }
   ],
   "source": [
    "#print(complete_ts.filter(complete_ts.lifestage == 1).count())\n",
    "\n",
    "# find models respecting the first condition\n",
    "w = Window.partitionBy('model').orderBy('date')\n",
    "\n",
    "first_lifestage = complete_ts.filter(complete_ts.lifestage.isNotNull()) \\\n",
    "                             .withColumn('rn', F.row_number().over(w))\n",
    "\n",
    "first_lifestage = first_lifestage.filter(first_lifestage.rn == 1).drop('rn')\n",
    "\n",
    "\n",
    "first_lifestage = first_lifestage \\\n",
    "    .filter(first_lifestage.lifestage == 1) \\\n",
    "    .select(first_lifestage.model, \n",
    "            first_lifestage.date.alias('first_lifestage_date'))\n",
    "\n",
    "# Create the mask (rows to be completed) for theses models\n",
    "complete_ts = add_column_index(complete_ts, 'idx') # save original indexes\n",
    "complete_ts.cache()\n",
    "\n",
    "mask = complete_ts\n",
    "\n",
    "# keep only models respecting the first condition\n",
    "mask = mask.join(first_lifestage, on='model', how='inner')\n",
    "\n",
    "# Look only before the first historized lifestage date\n",
    "mask = mask.filter(mask.date <= mask.first_lifestage_date)\n",
    "\n",
    "w = Window.partitionBy('model').orderBy(F.desc('date'))\n",
    "\n",
    "mask = mask \\\n",
    "    .withColumn('cumsum_y', F.sum('y').over(w)) \\\n",
    "    .withColumn('lag_cumsum_y', F.lag('cumsum_y').over(w)) \\\n",
    "    .fillna(0, subset=['lag_cumsum_y']) \\\n",
    "    .withColumn('is_active', F.col('cumsum_y') > F.col('lag_cumsum_y'))\n",
    "\n",
    "ts_start_date = mask \\\n",
    "    .filter(mask.is_active == False) \\\n",
    "    .withColumn('rn', F.row_number().over(w)) \\\n",
    "    .filter(F.col('rn') == 1) \\\n",
    "    .select('model', F.col('date').alias('start_date'))\n",
    "\n",
    "mask = mask.join(ts_start_date, on='model', how='left')\n",
    "\n",
    "# Case model start date unknown (older than first week recorded here)\n",
    "# ==> fill by an old date\n",
    "mask = mask \\\n",
    "    .withColumn('start_date', F.when(F.col('start_date').isNull(),\n",
    "                                     F.to_date(F.lit('1993-04-15'), 'yyyy-MM-dd')) \\\n",
    "                                     .otherwise(F.col('start_date'))) \\\n",
    "    .withColumn('is_model_start', F.col('date') > F.col('start_date')) \\\n",
    "    .withColumn('to_fill', F.col('is_active') & \\\n",
    "                           F.col('is_model_start') & \\\n",
    "                           F.col('lifestage').isNull())\n",
    "\n",
    "mask = mask.filter(mask.to_fill == True).select(['idx', 'to_fill'])\n",
    "\n",
    "# Fill the eligible rows under all conditions\n",
    "complete_ts = complete_ts.join(mask, on='idx', how='left')\n",
    "complete_ts = complete_ts \\\n",
    "    .withColumn('lifestage', \n",
    "                F.when(F.col('to_fill') == True, F.lit(1)).otherwise(F.col('lifestage')))\n",
    "\n",
    "complete_ts = complete_ts.select(['week_id', 'date', 'model', 'y', 'lifestage'])\n",
    "\n",
    "#complete_ts.cache()\n",
    "\n",
    "#print(complete_ts.filter(complete_ts.lifestage == 1).count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create active sales data set\n",
    "\n",
    "##### 1) Keep in memory first sales dates by model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = Window.partitionBy('model').orderBy('date')\n",
    "\n",
    "model_start_date = actual_sales.withColumn('rn', F.row_number().over(w))\n",
    "\n",
    "model_start_date = model_start_date \\\n",
    "    .filter(model_start_date.rn == 1) \\\n",
    "    .drop('rn', 'week_id', 'y') \\\n",
    "    .select(F.col(\"model\"), F.col(\"date\").alias(\"first_date\"))\n",
    "\n",
    "#print(model_start_date.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2) Construct active sales\n",
    "- Filtered on active life stage \n",
    "- After the first actual sales date\n",
    "- Padded with zeros (already done in complete sales)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(active_sales.count())\n",
    "\n",
    "active_sales = complete_ts \\\n",
    "    .filter(complete_ts.lifestage == 1) \\\n",
    "    .join(model_start_date, on='model', how='inner') \\\n",
    "    .filter(complete_ts.date >= model_start_date.first_date) \\\n",
    "    .drop('lifestage', 'first_date')\n",
    "                           \n",
    "#active_sales.cache()\n",
    "\n",
    "#print(active_sales.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean model info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_info = model_info \\\n",
    "    .withColumn('category_label', \n",
    "                F.when(model_info.category_label == 'SOUS RAYON POUB', F.lit(None)) \\\n",
    "                .otherwise(model_info.category_label)) \\\n",
    "    .fillna('UNKNOWN')\n",
    "\n",
    "# Due to a discrepant seasonal behaviour between LOW SOCKS and HIGH SOCKS, we chose to split\n",
    "# the product nature 'SOCKS' into two different product natures 'LOW SOCKS' and 'HIGH SOCKS'\n",
    "model_info = model_info \\\n",
    "    .withColumn('product_nature_label', \n",
    "                F.when((model_info.product_nature_label == 'SOCKS') & \\\n",
    "                       (model_info.model_label.contains(' LOW')), \n",
    "                       F.lit('LOW SOCKS')) \\\n",
    "                 .when((model_info.product_nature_label == 'SOCKS') & \\\n",
    "                       (model_info.model_label.contains(' MID')), \n",
    "                       F.lit('MID SOCKS')) \\\n",
    "                 .when((model_info.product_nature_label == 'SOCKS') & \\\n",
    "                       (model_info.model_label.contains(' HIGH')), \n",
    "                       F.lit('HIGH SOCKS')) \\\n",
    "                 .otherwise(model_info.product_nature_label)) \\\n",
    "    .drop('product_nature')\n",
    "    \n",
    "indexer = StringIndexer(inputCol='product_nature_label', outputCol='product_nature')\n",
    "model_info = indexer \\\n",
    "    .fit(model_info) \\\n",
    "    .transform(model_info) \\\n",
    "    .withColumn('product_nature', F.col('product_nature').cast('integer'))\n",
    "\n",
    "#print(model_info.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unit tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check duplicates rows\n",
    "assert active_sales.groupBy(['date', 'model']).count().select(F.max(\"count\")).collect()[0][0] == 1\n",
    "assert model_info.count() == model_info.select('model').drop_duplicates().count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_parquet_s3(model_info, 'fcst-refined-demand-forecast-dev', 'part_1/model_info')\n",
    "write_parquet_s3(actual_sales, 'fcst-refined-demand-forecast-dev', 'part_1/actual_sales')\n",
    "write_parquet_s3(active_sales, 'fcst-refined-demand-forecast-dev', 'part_1/active_sales')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Sparkmagic (PySpark)",
   "language": "",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 2
   },
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
