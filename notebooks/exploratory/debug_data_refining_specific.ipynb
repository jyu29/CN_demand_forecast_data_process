{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Spark application\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>Current session?</th></tr><tr><td>4</td><td>application_1588065598250_0005</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-10-226-39-14.subsidia.org:20888/proxy/application_1588065598250_0005/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-10-226-39-193.subsidia.org:8042/node/containerlogs/container_1588065598250_0005_01_000001/livy\">Link</a></td><td>✔</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession available as 'spark'.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Spark application\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>Current session?</th></tr><tr><td>5</td><td>application_1588065598250_0006</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-10-226-39-14.subsidia.org:20888/proxy/application_1588065598250_0006/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-10-226-39-193.subsidia.org:8042/node/containerlogs/container_1588065598250_0006_01_000001/livy\">Link</a></td><td>✔</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession available as 'spark'.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Current session configs: <tt>{'driverMemory': '5G', 'executorCores': 8, 'numExecutors': 2, 'executorMemory': '50G', 'conf': {'spark.memory.fraction': 0.3, 'spark.memory.storageFraction': 0.7, 'spark.executor.memoryOverhead': '6g', 'spark.dynamicAllocation.enabled': 'false', 'spark.default.parallelism': 16, 'spark.sql.shuffle.partitions': 16, 'spark.yarn.am.cores': 8, 'spark.yarn.am.memory': '50g', 'spark.yarn.am.memoryOverhead': '6g', 'spark.executor.extraJavaOptions': \"-verbose:gc -XX:+PrintGCDetails -XX:+PrintGCDateStamps -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:MaxHeapFreeRatio=70 -XX:+CMSClassUnloadingEnabled -XX:OnOutOfMemoryError='kill -9 %p'\", 'spark.driver.extraJavaOptions': \"-XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:MaxHeapFreeRatio=70 -XX:+CMSClassUnloadingEnabled -XX:OnOutOfMemoryError='kill -9 %p'\"}, 'kind': 'pyspark'}</tt><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>Current session?</th></tr><tr><td>5</td><td>application_1588065598250_0006</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-10-226-39-14.subsidia.org:20888/proxy/application_1588065598250_0006/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-10-226-39-193.subsidia.org:8042/node/containerlogs/container_1588065598250_0006_01_000001/livy\">Link</a></td><td>✔</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%configure -f\n",
    "{\n",
    " \"driverMemory\": \"5G\",\n",
    " \"executorCores\": 8,\n",
    " \"numExecutors\": 2,\n",
    " \"executorMemory\" : \"50G\",\n",
    " \"conf\" : \n",
    "    {\"spark.memory.fraction\" : 0.3,\n",
    "     \"spark.memory.storageFraction\" : 0.7,\n",
    "     \"spark.executor.memoryOverhead\" : \"6g\",\n",
    "     \"spark.dynamicAllocation.enabled\" : \"false\",\n",
    "     \"spark.default.parallelism\" : 16,\n",
    "     \"spark.sql.shuffle.partitions\" : 16,\n",
    "     \"spark.yarn.am.cores\" : 8,\n",
    "     \"spark.yarn.am.memory\" : \"50g\",\n",
    "     \"spark.yarn.am.memoryOverhead\" : \"6g\",\n",
    "     \"spark.executor.extraJavaOptions\" : \"-verbose:gc -XX:+PrintGCDetails -XX:+PrintGCDateStamps -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:MaxHeapFreeRatio=70 -XX:+CMSClassUnloadingEnabled -XX:OnOutOfMemoryError='kill -9 %p'\",\n",
    "     \"spark.driver.extraJavaOptions\" : \"-XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:MaxHeapFreeRatio=70 -XX:+CMSClassUnloadingEnabled -XX:OnOutOfMemoryError='kill -9 %p'\"\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pyspark import SparkContext, SparkConf, StorageLevel\n",
    "from pyspark.sql import SparkSession, Window\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "\n",
    "import pyspark.sql.functions as F\n",
    "import sys\n",
    "import time\n",
    "import os\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## Utils\n",
    "def read_parquet_s3(app, bucket, file_path):\n",
    "    \"\"\" \"\"\"\n",
    "    df = app.read.parquet(bucket + file_path)\n",
    "    return df\n",
    "\n",
    "def write_parquet_s3(spark_df, bucket, file_path):\n",
    "    \"\"\" \"\"\"\n",
    "    spark_df.write.parquet(bucket + file_path, mode=\"overwrite\")\n",
    "    \n",
    "def get_current_week_id():\n",
    "    shifted_date = datetime.today() + timedelta(days=1)\n",
    "    current_week_id = int(str(shifted_date.isocalendar()[0]) + str(shifted_date.isocalendar()[1]).zfill(2))\n",
    "    return current_week_id\n",
    "\n",
    "def get_timer(starting_time):\n",
    "    end = time.time()\n",
    "    minutes, seconds = divmod(int(end - starting_time), 60)\n",
    "    print(\"{} minute(s) {} second(s)\".format(int(minutes), seconds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## Run Configs\n",
    "only_last = True\n",
    "s3_path_refine_global = \"s3://fcst-refined-demand-forecast-dev/global/\"\n",
    "s3_path_refine_specific = \"s3://fcst-refined-demand-forecast-dev/specific/\"\n",
    "filter_type, filter_val = '', []\n",
    "scope = 'full_scope'\n",
    "first_test_cutoff = 201922"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## Read and cache data\n",
    "def read_clean_data():\n",
    "    actual_sales = read_parquet_s3(spark, s3_path_refine_global, 'actual_sales/')\n",
    "    actual_sales.persist(StorageLevel.MEMORY_ONLY)\n",
    "\n",
    "    active_sales = read_parquet_s3(spark, s3_path_refine_global, 'active_sales/')\n",
    "    active_sales.persist(StorageLevel.MEMORY_ONLY)\n",
    "\n",
    "    model_info = read_parquet_s3(spark, s3_path_refine_global, 'model_info/')\n",
    "    model_info.persist(StorageLevel.MEMORY_ONLY)\n",
    "\n",
    "    return actual_sales, active_sales, model_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## Reconstruction function\n",
    "def reconstruct_history(train_data_cutoff, actual_sales, model_info,\n",
    "                        cluster_keys=['product_nature', 'family'], min_ts_len=160):\n",
    "    # Create a complete TS dataframe\n",
    "    max_week = train_data_cutoff.select(F.max('week_id')).collect()[0][0]\n",
    "\n",
    "    all_model = train_data_cutoff.select('model').orderBy('model').drop_duplicates()\n",
    "    all_week = actual_sales \\\n",
    "        .filter(actual_sales.week_id <= max_week) \\\n",
    "        .select('week_id') \\\n",
    "        .orderBy('week_id') \\\n",
    "        .drop_duplicates()\n",
    "\n",
    "    complete_ts = all_model.crossJoin(all_week)\n",
    "\n",
    "    # Add corresponding date\n",
    "    complete_ts = complete_ts \\\n",
    "        .join(actual_sales.select(['week_id', 'date']).drop_duplicates(),\n",
    "              on=['week_id'],\n",
    "              how='inner')\n",
    "\n",
    "    # Add cluster_keys info from model_info\n",
    "    # /!\\ drop_na because in very rare cases, the models are too old or too recent\n",
    "    #     and do not have descriptions in d_sku\n",
    "\n",
    "    cluster_info = model_info.select(['model'] + cluster_keys)\n",
    "\n",
    "    complete_ts = complete_ts \\\n",
    "        .join(cluster_info, on='model', how=\"left\") \\\n",
    "        .dropna(subset=cluster_keys)\n",
    "\n",
    "    # Add active sales from train_data_cutoff\n",
    "    complete_ts = complete_ts.join(train_data_cutoff,\n",
    "                                   on=['model', 'week_id', 'date'],\n",
    "                                   how=\"left\")\n",
    "\n",
    "    # Calculate the average sales per cluster and week from actual_sales\n",
    "    all_sales = actual_sales \\\n",
    "        .join(cluster_info, on='model', how='left') \\\n",
    "        .dropna() \\\n",
    "        .groupBy(['week_id', 'date'] + cluster_keys) \\\n",
    "        .agg(F.mean('y').alias('mean_cluster_y'))\n",
    "\n",
    "    # Add it to complete_ts\n",
    "    complete_ts = complete_ts.join(all_sales,\n",
    "                                   on=['week_id', 'date', 'product_nature', 'family'],\n",
    "                                   how='left')\n",
    "\n",
    "    # Compute the scale factor by row\n",
    "    complete_ts = complete_ts \\\n",
    "        .withColumn('row_scale_factor', complete_ts.y / complete_ts.mean_cluster_y)\n",
    "\n",
    "    # Compute the scale factor by model\n",
    "    model_scale_factor = complete_ts \\\n",
    "        .groupBy('model') \\\n",
    "        .agg(F.mean('row_scale_factor').alias('model_scale_factor'))\n",
    "\n",
    "    complete_ts = complete_ts.join(model_scale_factor, on='model', how='left')\n",
    "\n",
    "    # have each model a scale factor?\n",
    "    assert complete_ts.filter(complete_ts.model_scale_factor.isNull()).count() == 0\n",
    "\n",
    "    # Compute a fake Y by row (if unknow fill by 0)\n",
    "    complete_ts = complete_ts \\\n",
    "        .withColumn('fake_y',\n",
    "                    (complete_ts.mean_cluster_y * complete_ts.model_scale_factor).cast('int'))\n",
    "    complete_ts = complete_ts.fillna(0, subset=['fake_y'])\n",
    "\n",
    "    # Calculate real age & total length of each TS\n",
    "    # And estimate the implementation period: while fake y > y\n",
    "    ts_start_end_date = complete_ts \\\n",
    "        .filter(complete_ts.y.isNotNull()) \\\n",
    "        .groupBy('model') \\\n",
    "        .agg(F.min('date').alias('start_date'), F.max('date').alias('end_date'))\n",
    "\n",
    "    complete_ts = complete_ts.join(ts_start_end_date, on='model', how='left')\n",
    "\n",
    "    complete_ts = complete_ts \\\n",
    "        .withColumn('age', (F.datediff(F.col('date'), F.col('start_date')) / 7) + 1) \\\n",
    "        .withColumn('length', (F.datediff(F.col('end_date'), F.col('date')) / 7) + 1) \\\n",
    "        .withColumn('is_y_sup', F.when(complete_ts.y.isNull(), 'false') \\\n",
    "                    .when(complete_ts.y > complete_ts.fake_y, 'true') \\\n",
    "                    .otherwise('false'))\n",
    "\n",
    "    end_impl_period = complete_ts \\\n",
    "        .filter(complete_ts.is_y_sup == True) \\\n",
    "        .groupBy('model') \\\n",
    "        .agg(F.min('age').alias('end_impl_period'))\n",
    "\n",
    "    complete_ts = complete_ts.join(end_impl_period, on='model', how='left')\n",
    "\n",
    "    # Update y from \"min_ts_len\" weeks ago to the end of the implementation period\n",
    "    complete_ts = complete_ts \\\n",
    "        .withColumn('y',\n",
    "                    F.when(((complete_ts.age <= 0) & (complete_ts.length <= min_ts_len)) | \\\n",
    "                           ((complete_ts.age > 0) & (complete_ts.age < complete_ts.end_impl_period)),\n",
    "                           complete_ts.fake_y.cast('int')) \\\n",
    "                    .otherwise(complete_ts.y).cast('int'))\n",
    "\n",
    "    complete_ts = complete_ts \\\n",
    "        .select(['week_id', 'date', 'model', 'y']) \\\n",
    "        .dropna() \\\n",
    "        .orderBy(['week_id', 'model'])\n",
    "\n",
    "    return complete_ts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Generate training data used to forecast validation & test cutoffs\n",
    "def generate_cutoff_train_data(actual_sales, active_sales, model_info, only_last):\n",
    "    current_cutoff = get_current_week_id()\n",
    "\n",
    "    if only_last:\n",
    "        l_cutoff_week_id = [current_cutoff]\n",
    "    else:\n",
    "        cutoff_week_id = active_sales \\\n",
    "            .filter(active_sales.week_id >= first_test_cutoff) \\\n",
    "            .select('week_id') \\\n",
    "            .drop_duplicates() \\\n",
    "            .orderBy('week_id')\n",
    "\n",
    "        l_cutoff_week_id = [row['week_id'] for row in cutoff_week_id.collect()] + [current_cutoff]\n",
    "\n",
    "    # loop generate cutoffs\n",
    "    for cutoff_week_id in l_cutoff_week_id:\n",
    "        t0 = time.time()\n",
    "\n",
    "        print('Generating train data for cutoff', str(cutoff_week_id))\n",
    "\n",
    "        train_data_cutoff = active_sales.filter(active_sales.week_id < cutoff_week_id)\n",
    "\n",
    "        # Models sold at least once before the cutoff\n",
    "        model_sold = train_data_cutoff \\\n",
    "            .groupBy('model') \\\n",
    "            .agg(F.sum('y').alias('qty_sold')) \\\n",
    "            .filter(F.col('qty_sold') > 0) \\\n",
    "            .select('model')\n",
    "\n",
    "        # Active the last week before the cutoff\n",
    "        last_week = train_data_cutoff.agg(F.max('week_id').alias('last_week'))\n",
    "\n",
    "        model_active = train_data_cutoff \\\n",
    "            .groupBy('model') \\\n",
    "            .agg(F.max('week_id').alias('last_active_week'))\n",
    "\n",
    "        model_active = model_active \\\n",
    "            .join(last_week,\n",
    "                  on=last_week.last_week == model_active.last_active_week,\n",
    "                  how='inner') \\\n",
    "            .select('model')\n",
    "\n",
    "        # Keep only sold & active models\n",
    "        model_to_keep = model_active.join(model_sold, 'model', 'inner')\n",
    "        train_data_cutoff = train_data_cutoff.join(model_to_keep, on='model', how='inner')\n",
    "\n",
    "        # Reconstruct a fake history\n",
    "        train_data_cutoff = reconstruct_history(train_data_cutoff, actual_sales, model_info)\n",
    "\n",
    "        cutoff_path = 'train_data_cutoff/train_data_cutoff_{}'.format(str(cutoff_week_id))\n",
    "\n",
    "        write_parquet_s3(train_data_cutoff, s3_path_refine_specific, cutoff_path)\n",
    "\n",
    "        t1 = time.time()\n",
    "        total = t1 - t0\n",
    "        print('Loop time {} {}:'.format(str(cutoff_week_id), total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('only_last: ', True)\n",
      "====> Counting(cache) [actual_sales] took \n",
      "0 minute(s) 7 second(s)\n",
      "Count actual_sales = 11016486\n",
      "====> Counting(cache) actual_sales took \n",
      "0 minute(s) 3 second(s)\n",
      "Count active_sales = 2666082\n",
      "====> Counting(cache) actual_sales took \n",
      "0 minute(s) 3 second(s)\n",
      "Count model_info = 381803\n",
      "('Generating train data for cutoff', '202018')\n",
      "Loop time 202018 74.7860460281:"
     ]
    }
   ],
   "source": [
    "print('only_last: ', only_last)\n",
    "\n",
    "actual_sales, active_sales, model_info = read_clean_data()\n",
    "\n",
    "print(\"====> Counting(cache) [actual_sales] took \")\n",
    "start = time.time()\n",
    "actual_sales_count = actual_sales.count()\n",
    "get_timer(starting_time=start)\n",
    "print(\"Count actual_sales = {}\".format(actual_sales_count))\n",
    "\n",
    "print(\"====> Counting(cache) actual_sales took \")\n",
    "start = time.time()\n",
    "active_sales_count = active_sales.count()\n",
    "get_timer(starting_time=start)\n",
    "print(\"Count active_sales = {}\".format(active_sales_count))\n",
    "\n",
    "print(\"====> Counting(cache) actual_sales took \")\n",
    "start = time.time()\n",
    "model_info_count = model_info.count()\n",
    "get_timer(starting_time=start)\n",
    "print(\"Count model_info = {}\".format(model_info_count))\n",
    "\n",
    "assert actual_sales.count() > 0\n",
    "assert active_sales.count() > 0\n",
    "assert model_info.count() > 0\n",
    "\n",
    "generate_cutoff_train_data(actual_sales, active_sales, model_info, only_last=only_last)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Sparkmagic (PySpark)",
   "language": "",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 3
   },
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
