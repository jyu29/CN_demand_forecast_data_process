{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext, SparkConf, StorageLevel\n",
    "from pyspark.sql import SparkSession, Window\n",
    "from pyspark.sql.types import *\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "import pyspark.sql.functions as F\n",
    "import sys\n",
    "import time\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf = SparkConf().setAll([\n",
    "    ('spark.app.name', 'data_refining_specific'),\n",
    "    ('spark.maximizeResourceAllocation', 'false'),\n",
    "    ('spark.dynamicAllocation.enabled', 'false'),\n",
    "    ('spark.executors.cores', 5),\n",
    "    ('spark.executor.memory', '37g'),\n",
    "    ('spark.executor.memoryOverhead', '5g'),\n",
    "    ('spark.executor.instances', 8),\n",
    "    ('spark.default.parallelism', 80),\n",
    "    ('spark.sql.shuffle.partitions', 80),\n",
    "    ('spark.yarn.am.cores' , 5),\n",
    "    ('spark.yarn.am.memory' , '37g'),\n",
    "    ('spark.yarn.am.memoryOverhead' , '5g'),\n",
    "    ('spark.executor.extraJavaOptions' , \"-XX:+UseG1GC -XX:+UnlockDiagnosticVMOptions -XX:+G1SummarizeConcMark -XX:InitiatingHeapOccupancyPercent=35 -verbose:gc -XX:+PrintGCDetails -XX:+PrintGCDateStamps -XX:OnOutOfMemoryError='kill -9 %p'\"),\n",
    "    ('spark.yarn.am.extraJavaOptions' , \"-XX:+UseG1GC -XX:+UnlockDiagnosticVMOptions -XX:+G1SummarizeConcMark -XX:InitiatingHeapOccupancyPercent=35 -verbose:gc -XX:+PrintGCDetails -XX:+PrintGCDateStamps -XX:OnOutOfMemoryError='kill -9 %p'\"),\n",
    "    ('yarn.nodemanager.vmem-check-enabled' , 'false'),\n",
    "    ('yarn.nodemanager.pmem-check-enabled' , 'false')\n",
    "])\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .config(conf=conf) \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for item in sorted(sc._conf.getAll()): print(item)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_parquet_s3(app, bucket, file_path):\n",
    "    \"\"\" \"\"\"\n",
    "    df = app.read.parquet(bucket + file_path)\n",
    "    return df\n",
    "\n",
    "def write_parquet_s3(spark_df, bucket, file_path):\n",
    "    \"\"\" \"\"\"\n",
    "    spark_df.write.parquet(bucket + file_path, mode=\"overwrite\")\n",
    "    \n",
    "def get_current_week_id():\n",
    "    shifted_date = datetime.today() + timedelta(days=1)\n",
    "    current_week_id = int(str(shifted_date.isocalendar()[0]) + str(shifted_date.isocalendar()[1]).zfill(2))\n",
    "    return current_week_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#only_last = True\n",
    "#s3_path_refine_global = \"s3://fcst-refined-demand-forecast-prod/global/\"\n",
    "#s3_path_refine_specific = \"s3://fcst-refined-demand-forecast-dev/specific/\"\n",
    "#filter_type, filter_val = 'family', [224, 12072, 600]\n",
    "#scope = 'domyos_nov_2019'\n",
    "#first_test_cutoff = 201922"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#only_last = False\n",
    "#s3_path_refine_global = \"s3://fcst-refined-demand-forecast-prod/global/\"\n",
    "#s3_path_refine_specific = \"s3://fcst-refined-demand-forecast-dev/specific/\"\n",
    "#filter_type, filter_val = 'department', [402, 403, 404, 406, 408, 473, 474]\n",
    "#scope = 'racket_sports'\n",
    "#first_test_cutoff = 201922"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "only_last = False\n",
    "s3_path_refine_global = \"s3://fcst-refined-demand-forecast-dev/global/\"\n",
    "s3_path_refine_specific = \"s3://fcst-refined-demand-forecast-dev/specific/\"\n",
    "filter_type, filter_val = '', []\n",
    "scope = 'full_scope'\n",
    "first_test_cutoff = 201922"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load global refined data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actual_sales = read_parquet_s3(spark, s3_path_refine_global, 'actual_sales/')\n",
    "actual_sales.persist(StorageLevel.MEMORY_ONLY)\n",
    "\n",
    "active_sales = read_parquet_s3(spark, s3_path_refine_global, 'active_sales/')\n",
    "active_sales.persist(StorageLevel.MEMORY_ONLY)\n",
    "\n",
    "model_info = read_parquet_s3(spark, s3_path_refine_global, 'model_info/')\n",
    "model_info.persist(StorageLevel.MEMORY_ONLY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if scope != 'full_scope':\n",
    "    \n",
    "    actual_sales = actual_sales \\\n",
    "        .join(model_info.select(['model'] + [filter_type]), on='model', how='left') \\\n",
    "        .filter(F.col(filter_type).isin(filter_val)) \\\n",
    "        .drop(filter_type)\n",
    "    \n",
    "    active_sales = active_sales \\\n",
    "        .join(model_info.select(['model'] + [filter_type]), on='model', how='left') \\\n",
    "        .filter(F.col(filter_type).isin(filter_val)) \\\n",
    "        .drop(filter_type)\n",
    "    \n",
    "    active_sales.persist(StorageLevel.MEMORY_ONLY)\n",
    "    actual_sales.persist(StorageLevel.MEMORY_ONLY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(actual_sales.count())\n",
    "print(active_sales.count())\n",
    "print(model_info.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define History Reconstruction Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reconstruct_history(train_data_cutoff, actual_sales, model_info,\n",
    "                        cluster_keys=['product_nature', 'family'], min_ts_len=160):\n",
    "\n",
    "    # Create a complete TS dataframe\n",
    "    max_week = train_data_cutoff.select(F.max('week_id')).collect()[0][0]\n",
    "    \n",
    "    all_model = train_data_cutoff.select('model').orderBy('model').drop_duplicates()\n",
    "    all_week = actual_sales \\\n",
    "        .filter(actual_sales.week_id <= max_week) \\\n",
    "        .select('week_id') \\\n",
    "        .orderBy('week_id') \\\n",
    "        .drop_duplicates()\n",
    "\n",
    "    complete_ts = all_model.crossJoin(all_week)\n",
    "    \n",
    "    # Add corresponding date\n",
    "    complete_ts = complete_ts \\\n",
    "        .join(actual_sales.select(['week_id', 'date']).drop_duplicates(), \n",
    "              on=['week_id'], \n",
    "              how='inner')\n",
    "\n",
    "    # Add cluster_keys info from model_info\n",
    "    # /!\\ drop_na because in very rare cases, the models are too old or too recent \n",
    "    #     and do not have descriptions in d_sku\n",
    "    \n",
    "    cluster_info = model_info.select(['model'] + cluster_keys)\n",
    "    \n",
    "    complete_ts = complete_ts \\\n",
    "        .join(cluster_info, on='model', how=\"left\") \\\n",
    "        .dropna(subset=cluster_keys)\n",
    "    \n",
    "    # Add active sales from train_data_cutoff\n",
    "    complete_ts = complete_ts.join(train_data_cutoff, \n",
    "                                   on=['model', 'week_id', 'date'], \n",
    "                                   how=\"left\")\n",
    "\n",
    "    \n",
    "    # Calculate the average sales per cluster and week from actual_sales\n",
    "    all_sales = actual_sales \\\n",
    "        .join(cluster_info, on='model', how='left') \\\n",
    "        .dropna() \\\n",
    "        .groupBy(['week_id', 'date'] + cluster_keys) \\\n",
    "        .agg(F.mean('y').alias('mean_cluster_y'))\n",
    "\n",
    "    # Add it to complete_ts\n",
    "    complete_ts = complete_ts.join(all_sales, \n",
    "                                   on=['week_id', 'date', 'product_nature', 'family'], \n",
    "                                   how='left')\n",
    "\n",
    "    # Compute the scale factor by row\n",
    "    complete_ts = complete_ts \\\n",
    "        .withColumn('row_scale_factor', complete_ts.y / complete_ts.mean_cluster_y)\n",
    "\n",
    "    # Compute the scale factor by model\n",
    "    model_scale_factor = complete_ts \\\n",
    "        .groupBy('model') \\\n",
    "        .agg(F.mean('row_scale_factor').alias('model_scale_factor'))\n",
    "\n",
    "    complete_ts = complete_ts.join(model_scale_factor, on='model', how='left')\n",
    "\n",
    "    # have each model a scale factor?\n",
    "    assert complete_ts.filter(complete_ts.model_scale_factor.isNull()).count() == 0\n",
    "\n",
    "    # Compute a fake Y by row (if unknow fill by 0)\n",
    "    complete_ts = complete_ts \\\n",
    "        .withColumn('fake_y', \n",
    "                    (complete_ts.mean_cluster_y * complete_ts.model_scale_factor).cast('int'))\n",
    "    complete_ts = complete_ts.fillna(0, subset=['fake_y'])\n",
    "\n",
    "    # Calculate real age & total length of each TS\n",
    "    # And estimate the implementation period: while fake y > y\n",
    "    ts_start_end_date = complete_ts \\\n",
    "        .filter(complete_ts.y.isNotNull()) \\\n",
    "        .groupBy('model') \\\n",
    "        .agg(F.min('date').alias('start_date'), F.max('date').alias('end_date'))\n",
    "    \n",
    "    complete_ts = complete_ts.join(ts_start_end_date, on='model', how='left')\n",
    "\n",
    "    complete_ts = complete_ts \\\n",
    "        .withColumn('age', (F.datediff(F.col('date'), F.col('start_date')) / 7) + 1) \\\n",
    "        .withColumn('length', (F.datediff(F.col('end_date'), F.col('date')) / 7) + 1) \\\n",
    "        .withColumn('is_y_sup', F.when(complete_ts.y.isNull(), 'false') \\\n",
    "                                 .when(complete_ts.y > complete_ts.fake_y, 'true') \\\n",
    "                                 .otherwise('false'))\n",
    "\n",
    "    end_impl_period = complete_ts \\\n",
    "        .filter(complete_ts.is_y_sup == True) \\\n",
    "        .groupBy('model') \\\n",
    "        .agg(F.min('age').alias('end_impl_period'))\n",
    "\n",
    "    complete_ts = complete_ts.join(end_impl_period, on='model', how='left')\n",
    "\n",
    "    # Update y from \"min_ts_len\" weeks ago to the end of the implementation period\n",
    "    complete_ts = complete_ts \\\n",
    "        .withColumn('y', \n",
    "                    F.when(((complete_ts.age <= 0) & (complete_ts.length <= min_ts_len)) | \\\n",
    "                           ((complete_ts.age > 0) & (complete_ts.age < complete_ts.end_impl_period)),\n",
    "                           complete_ts.fake_y.cast('int')) \\\n",
    "                    .otherwise(complete_ts.y).cast('int'))\n",
    "\n",
    "    complete_ts = complete_ts \\\n",
    "        .select(['week_id', 'date', 'model', 'y']) \\\n",
    "        .dropna() \\\n",
    "        .orderBy(['week_id', 'model'])\n",
    "\n",
    "    return complete_ts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate training data used to forecast cutoffs\n",
    "- For each cutoff, keep only models sold at least once before the cutoff and active the last past week"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_cutoff_train_data(actual_sales, active_sales, model_info, only_last):\n",
    "\n",
    "    current_cutoff = get_current_week_id()\n",
    "\n",
    "    if only_last:\n",
    "        l_cutoff_week_id = [current_cutoff]\n",
    "    else:\n",
    "        cutoff_week_id = active_sales \\\n",
    "            .filter(active_sales.week_id >= first_test_cutoff) \\\n",
    "            .select('week_id') \\\n",
    "            .drop_duplicates() \\\n",
    "            .orderBy('week_id')\n",
    "        \n",
    "        l_cutoff_week_id = [row['week_id'] for row in cutoff_week_id.collect()] + [current_cutoff]\n",
    "        \n",
    "    # loop generate cutoffs\n",
    "    for cutoff_week_id in l_cutoff_week_id:\n",
    "\n",
    "        print('Generating train data for cutoff', str(cutoff_week_id))\n",
    "\n",
    "        t0 = time.time()\n",
    "        \n",
    "    \n",
    "        train_data_cutoff = active_sales.filter(active_sales.week_id < cutoff_week_id)\n",
    "    \n",
    "        # Models sold at least once before the cutoff\n",
    "        model_sold = train_data_cutoff \\\n",
    "            .groupBy('model') \\\n",
    "            .agg(F.sum('y').alias('qty_sold')) \\\n",
    "            .filter(F.col('qty_sold') > 0) \\\n",
    "            .select('model')\n",
    "    \n",
    "        # Active the last week before the cutoff\n",
    "        last_week = train_data_cutoff.agg(F.max('week_id').alias('last_week'))\n",
    "    \n",
    "        model_active = train_data_cutoff \\\n",
    "            .groupBy('model') \\\n",
    "            .agg(F.max('week_id').alias('last_active_week'))\n",
    "        \n",
    "        model_active = model_active \\\n",
    "            .join(last_week, \n",
    "                  on=last_week.last_week == model_active.last_active_week, \n",
    "                  how='inner') \\\n",
    "            .select('model')\n",
    "    \n",
    "        # Keep only sold & active models\n",
    "        model_to_keep = model_active.join(model_sold, 'model', 'inner')\n",
    "        train_data_cutoff = train_data_cutoff.join(model_to_keep, on='model', how='inner')\n",
    "    \n",
    "        # Reconstruct a fake history\n",
    "        train_data_cutoff = reconstruct_history(train_data_cutoff, actual_sales, model_info)\n",
    "        \n",
    "        cutoff_path = '{}/train_data_cutoff/train_data_cutoff_{}'.format(scope, str(cutoff_week_id))\n",
    "        \n",
    "        write_parquet_s3(train_data_cutoff, s3_path_refine_specific, cutoff_path)\n",
    "        \n",
    "        t1 = time.time()\n",
    "        total = t1-t0\n",
    "        print('Loop time {} {}:'.format(str(cutoff_week_id), total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_cutoff_train_data(actual_sales, active_sales, model_info, only_last)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_cutoff = get_current_week_id()\n",
    "\n",
    "if only_last:\n",
    "    l_cutoff_week_id = [current_cutoff]\n",
    "else:\n",
    "    cutoff_week_id = active_sales \\\n",
    "        .filter(active_sales.week_id >= first_test_cutoff) \\\n",
    "        .select('week_id') \\\n",
    "        .drop_duplicates() \\\n",
    "        .orderBy('week_id')\n",
    "    \n",
    "    l_cutoff_week_id = [row['week_id'] for row in cutoff_week_id.collect()] + [current_cutoff]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#l_cutoff_week_id = [202004, 202005, 202006, 202007]\n",
    "l_cutoff_week_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_cutoff_train_data(cutoff_week_id):\n",
    "    t0 = time.time()\n",
    "        \n",
    "    print('Generating train data for cutoff', str(cutoff_week_id))\n",
    "\n",
    "    train_data_cutoff = active_sales.filter(active_sales.week_id < cutoff_week_id)\n",
    "\n",
    "    # Models sold at least once before the cutoff\n",
    "    model_sold = train_data_cutoff \\\n",
    "        .groupBy('model') \\\n",
    "        .agg(F.sum('y').alias('qty_sold')) \\\n",
    "        .filter(F.col('qty_sold') > 0) \\\n",
    "        .select('model')\n",
    "\n",
    "    # Active the last week before the cutoff\n",
    "    last_week = train_data_cutoff.agg(F.max('week_id').alias('last_week'))\n",
    "\n",
    "    model_active = train_data_cutoff \\\n",
    "        .groupBy('model') \\\n",
    "        .agg(F.max('week_id').alias('last_active_week'))\n",
    "    \n",
    "    model_active = model_active \\\n",
    "        .join(last_week, \n",
    "              on=last_week.last_week == model_active.last_active_week, \n",
    "              how='inner') \\\n",
    "        .select('model')\n",
    "\n",
    "    # Keep only sold & active models\n",
    "    model_to_keep = model_active.join(model_sold, 'model', 'inner')\n",
    "    train_data_cutoff = train_data_cutoff.join(model_to_keep, on='model', how='inner')\n",
    "\n",
    "    # Reconstruct a fake history\n",
    "    train_data_cutoff = reconstruct_history(train_data_cutoff, actual_sales, model_info)\n",
    "    \n",
    "    path_cutoff = '{}/train_data_cutoff/train_data_cutoff_{}'.format(scope, str(cutoff_week_id))\n",
    "    \n",
    "    #ut.write_parquet_s3(train_data_cutoff, s3_path_refine_specific, path_cutoff)\n",
    "    \n",
    "    t1 = time.time()\n",
    "    total = t1-t0\n",
    "    print('Loop time {} {}:'.format(str(cutoff_week_id), total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t0 = time.time()\n",
    "map(generate_cutoff_train_data, l_cutoff_week_id)\n",
    "t1 = time.time()\n",
    "print(t1-t0)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "FULL SCOPE:\n",
    "\n",
    "('Generating train data for cutoff', '202004')\n",
    "Loop time 202004 87.7386569977:\n",
    "('Generating train data for cutoff', '202005')\n",
    "Loop time 202005 56.6484761238:\n",
    "('Generating train data for cutoff', '202006')\n",
    "Loop time 202006 47.5503311157:\n",
    "('Generating train data for cutoff', '202007')\n",
    "Loop time 202007 45.5616528988:\n",
    "237.499705076"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Sparkmagic (PySpark)",
   "language": "",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 3
   },
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}