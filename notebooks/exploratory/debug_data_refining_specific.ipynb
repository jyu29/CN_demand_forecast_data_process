{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "from pyspark import SparkContext, SparkConf, StorageLevel\n",
    "from pyspark.sql import SparkSession, Window\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DateType, BooleanType\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "\n",
    "import datetime\n",
    "from datetime import datetime, timedelta\n",
    "import numpy as np\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf = SparkConf().setAll([\n",
    " ('spark.sql.shuffle.partitions', 110),\n",
    " ('spark.default.parallelism', 110),\n",
    " ('spark.autoBroadcastJoinThreshold', 15485760),\n",
    " ('spark.dynamicAllocation.enabled', 'false'),\n",
    " ('spark.executor.instances', 11),\n",
    " ('spark.executor.memory', '19g'),\n",
    " ('spark.driver.memory', '19g'),\n",
    " ('spark.driver.cores', 5),\n",
    " ('spark.memory.storageFraction', 0.4),   \n",
    " ('spark.memory.fraction', 0.6),\n",
    " ('spark.executor.memoryOverhead', '2g'),\n",
    " ('spark.executor.cores', 5),\n",
    " ('spark.hadoop.mapreduce.fileoutputcommitter.algorithm.version', 2)\n",
    "])\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"data_refining_part_2_history_reconstruction\") \\\n",
    "    .config(conf=conf)\\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_parquet_s3(app, bucket, file_path):\n",
    "    \"\"\" \"\"\"\n",
    "    df = app.read.parquet(bucket + file_path)\n",
    "    return df\n",
    "\n",
    "\n",
    "def write_parquet_s3(spark_df, bucket, file_path):\n",
    "    \"\"\" \"\"\"\n",
    "    spark_df.write.parquet(bucket + file_path, mode=\"overwrite\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "horizon = 10\n",
    "horizon_freq = '1W-SUN'\n",
    "\n",
    "prediction_length = horizon\n",
    "prediction_freq = '1W-SUN'\n",
    "season_length = 52\n",
    "\n",
    "dict_scope = {'marco': {'filter_type' : 'family', 'filter_val' : [12151, 230]},\n",
    "              'racket_sports': {'filter_type' : 'department', 'filter_val' : [402, 403, 404, 406, 408, 473, 474]},\n",
    "              'full_scope': {'filter_type' : '', 'filter_val' : []},\n",
    "              \"domyos_nov_2019\": {\"filter_type\" : \"family\", \"filter_val\" : [224, 12072, 600]}}\n",
    "              \n",
    "scope = 'domyos_nov_2019' # change the scope here\n",
    "first_test_cutoff = 201922 # change test period here, should be >= 201922\n",
    "\n",
    "filter_val, filter_type = dict_scope[scope].values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actual_sales = read_parquet_s3(spark, 's3://fcst-refined-demand-forecast-dev/part_1/actual_sales/')\n",
    "actual_sales.persist(StorageLevel.MEMORY_ONLY)\n",
    "actual_sales.printSchema()\n",
    "actual_sales.show(1)\n",
    "\n",
    "active_sales = read_parquet_s3(spark, 's3://fcst-refined-demand-forecast-dev/part_1/active_sales/')\n",
    "active_sales.persist(StorageLevel.MEMORY_ONLY)\n",
    "active_sales.printSchema()\n",
    "active_sales.show(1)\n",
    "\n",
    "model_info = read_parquet_s3(spark, 's3://fcst-refined-demand-forecast-dev/part_1/model_info/')\n",
    "model_info.persist(StorageLevel.MEMORY_ONLY)\n",
    "model_info.printSchema()\n",
    "model_info.show(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load clean data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if scope != 'full_scope':\n",
    "    actual_sales = actual_sales.join(model_info.select(model_info['model'], model_info[filter_type]), 'model', how='left')\n",
    "\n",
    "    actual_sales = actual_sales.filter(actual_sales[filter_type].isin(filter_val))\\\n",
    "                               .drop(filter_type)\n",
    "    \n",
    "    active_sales = active_sales.join(model_info.select(model_info['model'], model_info[filter_type]), 'model', how='left')\n",
    "\n",
    "    active_sales = active_sales.filter(active_sales[filter_type].isin(filter_val))\\\n",
    "                               .drop(filter_type)\n",
    "    \n",
    "    active_sales.persist(StorageLevel.MEMORY_ONLY)\n",
    "    actual_sales.persist(StorageLevel.MEMORY_ONLY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define History Reconstruction Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sup_week(week_id):\n",
    "    \n",
    "    week_id = str(week_id)\n",
    "    y, w = int(week_id[:4]), int(week_id[4:])\n",
    "    \n",
    "    if w == 1:\n",
    "        w = '52'\n",
    "        y = str(y - 1)\n",
    "        \n",
    "    elif len(str(w))==1:\n",
    "        w = w - 1\n",
    "        y = str(y)\n",
    "        w = '0' + str(w)\n",
    "        \n",
    "    elif w == 10:\n",
    "        w = w - 1\n",
    "        y = str(y)\n",
    "        w = '0' + str(w)\n",
    "    \n",
    "    else:\n",
    "        w = str(w - 1)\n",
    "        y = str(y)\n",
    "    \n",
    "    n_wk = y + w\n",
    "    return int(n_wk)\n",
    "\n",
    "\n",
    "def next_week(week_id):\n",
    "    week_id = str(week_id)\n",
    "    y, w = int(week_id[:4]), int(week_id[4:])\n",
    "    \n",
    "    if w == 9:\n",
    "        w = '10'\n",
    "        y = str(y)\n",
    "    elif len(str(w))==1:\n",
    "        w = w + 1\n",
    "        y = str(y)\n",
    "        w = '0' + str(w)\n",
    "    elif w == 52:\n",
    "        w = '01'\n",
    "        y = str(y + 1)\n",
    "    else:\n",
    "        w = str(w + 1)\n",
    "        y = str(y)\n",
    "    n_wk = y + w\n",
    "    return int(n_wk)\n",
    "\n",
    "\n",
    "def __add_week(week, nb):\n",
    "    if nb < 0 :\n",
    "        for i in range(abs(nb)):\n",
    "            week = sup_week(week)\n",
    "    else:\n",
    "        for i in range(nb):\n",
    "            week = next_week(week)\n",
    "    \n",
    "    return week\n",
    "\n",
    "\n",
    "def find_weeks(start, end):\n",
    "    l = [int(start), int(end)]\n",
    "    start = str(start)+'0'\n",
    "    start = datetime.strptime(start, '%Y%W%w')\n",
    "    end = sup_week(end)\n",
    "    end = str(end)+'0'\n",
    "    end = datetime.strptime(end, '%Y%W%w')\n",
    "      \n",
    "    \n",
    "    for i in range((end - start).days + 1):\n",
    "        d = (start + timedelta(days=i)).isocalendar()[:2] # e.g. (2011, 52)\n",
    "        yearweek = '{}{:02}'.format(*d) # e.g. \"201152\"\n",
    "        l.append(int(yearweek))\n",
    "    \n",
    "    \n",
    "    return sorted(set(l))\n",
    "\n",
    "\n",
    "def reconstruct_history(train_data_cutoff, actual_sales, model_info,\n",
    "                        cluster_keys=['product_nature', 'family'], min_ts_len=160):\n",
    "\n",
    "\n",
    "    last_week = train_data_cutoff.agg(F.max('week_id').alias('last_week'))\n",
    "    model_to_keep = train_data_cutoff.groupBy('model').agg(F.max('week_id').alias('last_active_week'))\n",
    "    model_to_keep = model_to_keep.join(last_week, last_week.last_week == model_to_keep.last_active_week, 'inner').select(model_to_keep.model)\n",
    "    train_data_cutoff = train_data_cutoff.join(model_to_keep, 'model', how='inner')\n",
    "\n",
    "\n",
    "    df_date = actual_sales.select(['week_id', 'date']).distinct()\n",
    "    y_not_null = train_data_cutoff.where(train_data_cutoff.y.isNotNull())\n",
    "\n",
    "    max_week = train_data_cutoff.select(F.max('week_id')).collect()[0][0]\n",
    "    min_week = train_data_cutoff.select(F.min('week_id')).collect()[0][0]\n",
    "\n",
    "    \n",
    "    list_weeks = find_weeks(min_week, max_week)\n",
    "    list_weeks = spark.createDataFrame(list_weeks, IntegerType()).selectExpr('value as week_id')\n",
    "    list_models = train_data_cutoff.select(train_data_cutoff.model).distinct()\n",
    "\n",
    "    full = list_weeks.crossJoin(list_models)\n",
    "    full_actives_sales = full.join(train_data_cutoff, ['week_id', 'model'], how='left')\n",
    "    #full_actives_sales = full_actives_sales[full_actives_sales['week_id'] < cutoff_week_id_test]\n",
    "\n",
    "    #full_actives_sales.describe().show()\n",
    "    # add cluster infos\n",
    "\n",
    "    mdl_inf = model_info.select(['model'] + cluster_keys)\n",
    "\n",
    "    complete_ts = full_actives_sales.join(mdl_inf, 'model', how='left').drop('date')\n",
    "    complete_ts = complete_ts.join(df_date, 'week_id', how='inner')\n",
    "\n",
    "    \n",
    "    # Calculate the average sales per cluster and week from actual_sales\n",
    "    all_sales = actual_sales.join(mdl_inf, 'model', how='left')\n",
    "    all_sales = all_sales.dropna()\n",
    "    join_key = ['week_id', 'date'] + cluster_keys\n",
    "    all_sales = all_sales.groupBy(join_key).agg(F.mean('y').alias('mean_cluster_y'))\n",
    "\n",
    "\n",
    "    # ad it to complete_ts\n",
    "    complete_ts = complete_ts.join(all_sales, ['week_id', 'date', 'product_nature', 'family'], how='left')\n",
    "\n",
    "    \n",
    "    #SCale factor\n",
    "    complete_ts = complete_ts.withColumn('row_scale_factor', complete_ts.y / complete_ts.mean_cluster_y)\n",
    "\n",
    "    model_scale_factor = complete_ts.groupBy('model').agg(F.mean('row_scale_factor').alias('model_scale_factor'))\n",
    "\n",
    "    complete_ts = complete_ts.join(model_scale_factor, ['model'], how='left')\n",
    "\n",
    "    # assert complete_ts.where(complete_ts.model_scale_factor.isNull()).count() == 0\n",
    "\n",
    "    \n",
    "    #compute fake Y\n",
    "    complete_ts = complete_ts.withColumn('fake_y', (complete_ts.mean_cluster_y * complete_ts.model_scale_factor).cast('int'))\n",
    "    complete_ts = complete_ts.fillna(0, subset=['fake_y'])\n",
    "    \n",
    "    \n",
    "    start_end = y_not_null.groupBy('model').agg(F.min('date').alias('start_date'), F.max('date').alias('end_date'))\n",
    "    complete_ts = complete_ts.join(start_end, 'model', how='left')\n",
    "    \n",
    "\n",
    "    complete_ts = complete_ts.withColumn('age', (F.datediff(F.col('date'), F.col('start_date'))) / (7) + 1 )\\\n",
    "                             .withColumn('length', (F.datediff(F.col('end_date'), F.col('date'))) / (7) + 1 )\\\n",
    "                             .withColumn('is_y_sup', F.when(complete_ts.y.isNull(), 'false')\\\n",
    "                                                      .when(complete_ts.y > complete_ts.fake_y, 'true')\\\n",
    "                                                      .otherwise('false'))\n",
    "    \n",
    "    \n",
    "    end_impl_period = complete_ts.filter(complete_ts.is_y_sup == True).select(['model', 'age']).groupBy('model').agg(F.min('age').alias('end_impl_period'))\n",
    "\n",
    "    complete_ts = complete_ts.join(end_impl_period, on=['model'], how='left')\n",
    "\n",
    "    \n",
    "    complete_ts = complete_ts.withColumn('y', \n",
    "                F.when(\n",
    "                    ((complete_ts.age <= 0) & (complete_ts.length <= min_ts_len)) | \\\n",
    "                    ((complete_ts.age > 0) & (complete_ts.age < complete_ts.end_impl_period)), complete_ts.fake_y.cast('int'))\\\n",
    "                 .otherwise(complete_ts.y).cast('int'))\n",
    "    \n",
    "    \n",
    "    \n",
    "    complete_ts = complete_ts.select(['week_id', 'date', 'model', 'y']).dropna(subset=('week_id', 'date', 'model', 'y'))\n",
    "\n",
    "\n",
    "    complete_ts = complete_ts.orderBy(['week_id', 'model'])\n",
    "    \n",
    "    return complete_ts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate training data used to forecast validation & test cutoffs\n",
    "- For each cutoff, keep only models active the week before the cutoff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_cutoff = next_week(actual_sales.select(F.max('week_id')).collect()[0][0])\n",
    "\n",
    "\n",
    "cutoff_week_test = active_sales.where(active_sales.week_id >= first_test_cutoff).select(active_sales.week_id).distinct().orderBy('week_id')\n",
    "\n",
    "nRow = spark.createDataFrame([[current_cutoff]])\n",
    "cutoff_week_test = cutoff_week_test.union(nRow)\n",
    "\n",
    "cutoff_week_test.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cutoff_week_val = active_sales.filter(active_sales.week_id < first_test_cutoff).select(active_sales.week_id).distinct().orderBy('week_id')\n",
    "\n",
    "max_cutoff_week_val = cutoff_week_val.select(F.max('week_id')).collect()[0][0]\n",
    "\n",
    "# keep the last 60 dates before test set for validation\n",
    "\n",
    "_sup_week = __add_week(max_cutoff_week_val, (-horizon +1))\n",
    "_inf_week = __add_week(max_cutoff_week_val, -(60 + horizon -1))\n",
    "\n",
    "\n",
    "cutoff_week_val = cutoff_week_val.filter((cutoff_week_val.week_id > _inf_week) & (cutoff_week_val.week_id <= _sup_week))\n",
    "\n",
    "# keep only one cutoff every 10 dates\n",
    "idx_to_keep = ((np.arange(cutoff_week_val.count()) + 1) % 10 == 0)\n",
    "idx = np.arange(1, len(idx_to_keep)+1)\n",
    "idx_to_keep = np.c_[idx_to_keep, idx]\n",
    "\n",
    "nRow = spark.createDataFrame(idx_to_keep.tolist()).selectExpr('_1 as value', '_2 as id')\n",
    "cutoff_week_val = cutoff_week_val.withColumn('id', F.row_number().over(Window.orderBy('week_id')))\n",
    "cutoff_week_val = cutoff_week_val.join(nRow, 'id', how='inner').drop('id')\n",
    "cutoff_week_val = cutoff_week_val.filter(cutoff_week_val.value == 1).drop('value')\n",
    "\n",
    "cutoff_week_val.show(3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#weeks cutoff\n",
    "iterate_week = cutoff_week_val.union(cutoff_week_test)\n",
    "iterate_week = [row.week_id for row in iterate_week.collect()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for cutoff_week_id in sorted(iterate_week[:3]):\n",
    "    \n",
    "    t0 = time.time()\n",
    "    print('Generating train data for cutoff', str(cutoff_week_id))\n",
    "            \n",
    "    train_data_cutoff = active_sales.filter(active_sales.week_id < cutoff_week_id)\n",
    "       \n",
    "    model_sold = train_data_cutoff.select(['model', 'y'])\\\n",
    "                 .groupBy('model')\\\n",
    "                 .agg(F.sum(train_data_cutoff.y).alias('qty_sold'))\\\n",
    "                 .orderBy('model')\n",
    "\n",
    "    model_sold = model_sold.filter(model_sold.qty_sold > 0).select('model').orderBy('model')\n",
    "    \n",
    "    last_week = train_data_cutoff.agg(F.max('week_id').alias('last_week'))\n",
    "    \n",
    "    model_active = train_data_cutoff.groupBy('model').agg(F.max('week_id').alias('last_active_week'))\n",
    "    \n",
    "    model_active = model_active.join(last_week, last_week.last_week == model_active.last_active_week, 'inner').select(model_active.model)\n",
    "    \n",
    "    model_to_keep = model_active.join(model_sold, 'model', 'inner')\n",
    "    \n",
    "    train_data_cutoff = train_data_cutoff.join(model_to_keep, 'model', how='inner')\n",
    "    \n",
    "    # Reconstruct a fake history\n",
    "    train_data_cutoff = reconstruct_history(train_data_cutoff, actual_sales, model_info)\n",
    "    \n",
    "    train_data_cutoff.write.parquet('s3://fcst-refined-demand-forecast-dev/scope/{}/train_data_cutoff/train_data_cutoff_{}'.format(scope, str(cutoff_week_id)), mode=\"overwrite\")    \n",
    "\n",
    "    t1 = time.time()\n",
    "    total = t1-t0\n",
    "    \n",
    "    print('temps boucle {} {}:'.format(str(cutoff_week_id), total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Sparkmagic (PySpark)",
   "language": "",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 3
   },
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
