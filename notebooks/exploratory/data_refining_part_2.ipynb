{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Spark application\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>Current session?</th></tr><tr><td>6</td><td>application_1580280484752_0007</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-10-226-39-36.subsidia.org:20888/proxy/application_1580280484752_0007/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-10-226-39-113.subsidia.org:8042/node/containerlogs/container_1580280484752_0007_01_000001/livy\">Link</a></td><td>✔</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c27fe1fac2ba4467a6c12f87f0db65c8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession available as 'spark'.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0d8a7a8057524df681d609dc844b2c57",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import sys\n",
    "\n",
    "from pyspark import SparkContext, SparkConf, StorageLevel\n",
    "from pyspark.sql import SparkSession, Window\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DateType, BooleanType\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "\n",
    "import datetime\n",
    "from datetime import datetime, timedelta\n",
    "import numpy as np\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "474d244e380148af947b499c123dcc01",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "conf = SparkConf().setAll([\n",
    " ('spark.sql.shuffle.partitions', 110),\n",
    " ('spark.default.parallelism', 110),\n",
    " ('spark.autoBroadcastJoinThreshold', 15485760),\n",
    " ('spark.dynamicAllocation.enabled', 'false'),\n",
    " ('spark.executor.instances', 11),\n",
    " ('spark.executor.memory', '19g'),\n",
    " ('spark.driver.memory', '19g'),\n",
    " ('spark.driver.cores', 5),\n",
    " ('spark.memory.storageFraction', 0.4),   \n",
    " ('spark.memory.fraction', 0.6),\n",
    " ('spark.executor.memoryOverhead', '2g'),\n",
    " ('spark.executor.cores', 5),\n",
    " ('spark.hadoop.mapreduce.fileoutputcommitter.algorithm.version', 2)\n",
    "])\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"data_refining_part_2_history_reconstruction\") \\\n",
    "    .config(conf=conf)\\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7589c05d3f804416906e93775b3376c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def read_parquet_s3(app, s3_path):\n",
    "    \n",
    "    df = app.read.parquet(s3_path)\n",
    "    path_sinature = \">> Parquet file read from \" + s3_path\n",
    "        \n",
    "    return df\n",
    "\n",
    "\n",
    "def write_parquet_s3(spark_df, bucket, file_path):\n",
    "    \"\"\"Writing spark Dataframe into s3 as a parquet file.\n",
    "    \n",
    "    Parameters:\n",
    "    spark_df (pyspark.sql.dataframe): the spark Dataframe.\n",
    "    bucket (string): the s3 bucket name.\n",
    "    file_path (string) : the table name or directory.\n",
    "    \n",
    "    Returns:\n",
    "    \"\"\"\n",
    "    s3_path = 's3://{}/{}'.format(bucket, file_path)\n",
    "    spark_df.write.parquet(s3_path, mode=\"overwrite\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "516e5f70993244eca378685f96f3266d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "horizon = 10\n",
    "horizon_freq = '1W-SUN'\n",
    "\n",
    "prediction_length = horizon\n",
    "prediction_freq = '1W-SUN'\n",
    "season_length = 52\n",
    "\n",
    "dict_scope = {'marco': {'filter_type' : 'family', 'filter_val' : [12151, 230]},\n",
    "              'racket_sports': {'filter_type' : 'department', 'filter_val' : [402, 403, 404, 406, 408, 473, 474]},\n",
    "              'full_scope': {'filter_type' : '', 'filter_val' : []},\n",
    "              \"domyos_nov_2019\": {\"filter_type\" : \"family\", \"filter_val\" : [224, 12072, 600]}}\n",
    "              \n",
    "scope = 'domyos_nov_2019' # change the scope here\n",
    "first_test_cutoff = 201922 # change test period here, should be >= 201922\n",
    "\n",
    "filter_val, filter_type = dict_scope[scope].values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6f4c74b234c44e5a8a9ae70a030b274e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- week_id: integer (nullable = true)\n",
      " |-- date: date (nullable = true)\n",
      " |-- model: long (nullable = true)\n",
      " |-- y: long (nullable = true)\n",
      "\n",
      "+-------+----------+-------+---+\n",
      "|week_id|      date|  model|  y|\n",
      "+-------+----------+-------+---+\n",
      "| 201629|2016-07-17|8048316|  1|\n",
      "+-------+----------+-------+---+\n",
      "only showing top 1 row\n",
      "\n",
      "root\n",
      " |-- model: long (nullable = true)\n",
      " |-- week_id: integer (nullable = true)\n",
      " |-- date: date (nullable = true)\n",
      " |-- y: long (nullable = true)\n",
      "\n",
      "+-------+-------+----------+---+\n",
      "|  model|week_id|      date|  y|\n",
      "+-------+-------+----------+---+\n",
      "|8330598| 201520|2015-05-10|104|\n",
      "+-------+-------+----------+---+\n",
      "only showing top 1 row\n",
      "\n",
      "root\n",
      " |-- model: long (nullable = true)\n",
      " |-- model_label: string (nullable = true)\n",
      " |-- family: long (nullable = true)\n",
      " |-- family_label: string (nullable = true)\n",
      " |-- sub_department: long (nullable = true)\n",
      " |-- sub_department_label: string (nullable = true)\n",
      " |-- department: long (nullable = true)\n",
      " |-- department_label: string (nullable = true)\n",
      " |-- univers: long (nullable = true)\n",
      " |-- univers_label: string (nullable = true)\n",
      " |-- product_nature_label: string (nullable = true)\n",
      " |-- category_label: string (nullable = true)\n",
      " |-- product_nature: integer (nullable = true)\n",
      "\n",
      "+-------+--------------------+------+------------+--------------+--------------------+----------+--------------------+-------+--------------------+--------------------+--------------+--------------+\n",
      "|  model|         model_label|family|family_label|sub_department|sub_department_label|department|    department_label|univers|       univers_label|product_nature_label|category_label|product_nature|\n",
      "+-------+--------------------+------+------------+--------------+--------------------+----------+--------------------+-------+--------------------+--------------------+--------------+--------------+\n",
      "|8361946|*IN SHOULDER BAG ...|  3521|      ENERGY|          1904|     TREK BACKPACKS,|       415|TREKKING SEVERAL ...|     49|*IN SHOULDER BAG ...|                 BAG|     ACCESSORY|           203|\n",
      "+-------+--------------------+------+------------+--------------+--------------------+----------+--------------------+-------+--------------------+--------------------+--------------+--------------+\n",
      "only showing top 1 row"
     ]
    }
   ],
   "source": [
    "actual_sales = read_parquet_s3(spark, 's3://fcst-refined-demand-forecast-dev/part_1/actual_sales/')\n",
    "actual_sales.persist(StorageLevel.MEMORY_ONLY)\n",
    "actual_sales.printSchema()\n",
    "actual_sales.show(1)\n",
    "\n",
    "active_sales = read_parquet_s3(spark, 's3://fcst-refined-demand-forecast-dev/part_1/active_sales/')\n",
    "active_sales.persist(StorageLevel.MEMORY_ONLY)\n",
    "active_sales.printSchema()\n",
    "active_sales.show(1)\n",
    "\n",
    "model_info = read_parquet_s3(spark, 's3://fcst-refined-demand-forecast-dev/part_1/model_info/')\n",
    "model_info.persist(StorageLevel.MEMORY_ONLY)\n",
    "model_info.printSchema()\n",
    "model_info.show(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load clean data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f6d4f1d498d24bbb8c6086f286b46a93",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame[model: bigint, week_id: int, date: date, y: bigint]\n",
      "DataFrame[model: bigint, week_id: int, date: date, y: bigint]"
     ]
    }
   ],
   "source": [
    "if scope != 'full_scope':\n",
    "    actual_sales = actual_sales.join(model_info.select(model_info['model'], model_info[filter_type]), 'model', how='left')\n",
    "\n",
    "    actual_sales = actual_sales.filter(actual_sales[filter_type].isin(filter_val))\\\n",
    "                               .drop(filter_type)\n",
    "    \n",
    "    active_sales = active_sales.join(model_info.select(model_info['model'], model_info[filter_type]), 'model', how='left')\n",
    "\n",
    "    active_sales = active_sales.filter(active_sales[filter_type].isin(filter_val))\\\n",
    "                               .drop(filter_type)\n",
    "    \n",
    "    active_sales.persist(StorageLevel.MEMORY_ONLY)\n",
    "    actual_sales.persist(StorageLevel.MEMORY_ONLY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define History Reconstruction Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d2556fc33ac74c8db7aead4ec25327b0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def sup_week(week_id):\n",
    "    \n",
    "    week_id = str(week_id)\n",
    "    y, w = int(week_id[:4]), int(week_id[4:])\n",
    "    \n",
    "    if w == 1:\n",
    "        w = '52'\n",
    "        y = str(y - 1)\n",
    "        \n",
    "    elif len(str(w))==1:\n",
    "        w = w - 1\n",
    "        y = str(y)\n",
    "        w = '0' + str(w)\n",
    "        \n",
    "    elif w == 10:\n",
    "        w = w - 1\n",
    "        y = str(y)\n",
    "        w = '0' + str(w)\n",
    "    \n",
    "    else:\n",
    "        w = str(w - 1)\n",
    "        y = str(y)\n",
    "    \n",
    "    n_wk = y + w\n",
    "    return int(n_wk)\n",
    "\n",
    "\n",
    "def next_week(week_id):\n",
    "    week_id = str(week_id)\n",
    "    y, w = int(week_id[:4]), int(week_id[4:])\n",
    "    \n",
    "    if w == 9:\n",
    "        w = '10'\n",
    "        y = str(y)\n",
    "    elif len(str(w))==1:\n",
    "        w = w + 1\n",
    "        y = str(y)\n",
    "        w = '0' + str(w)\n",
    "    elif w == 52:\n",
    "        w = '01'\n",
    "        y = str(y + 1)\n",
    "    else:\n",
    "        w = str(w + 1)\n",
    "        y = str(y)\n",
    "    n_wk = y + w\n",
    "    return int(n_wk)\n",
    "\n",
    "\n",
    "def __add_week(week, nb):\n",
    "    if nb < 0 :\n",
    "        for i in range(abs(nb)):\n",
    "            week = sup_week(week)\n",
    "    else:\n",
    "        for i in range(nb):\n",
    "            week = next_week(week)\n",
    "    \n",
    "    return week\n",
    "\n",
    "\n",
    "def find_weeks(start, end):\n",
    "    l = [int(start), int(end)]\n",
    "    start = str(start)+'0'\n",
    "    start = datetime.strptime(start, '%Y%W%w')\n",
    "    end = sup_week(end)\n",
    "    end = str(end)+'0'\n",
    "    end = datetime.strptime(end, '%Y%W%w')\n",
    "      \n",
    "    \n",
    "    for i in range((end - start).days + 1):\n",
    "        d = (start + timedelta(days=i)).isocalendar()[:2] # e.g. (2011, 52)\n",
    "        yearweek = '{}{:02}'.format(*d) # e.g. \"201152\"\n",
    "        l.append(int(yearweek))\n",
    "    \n",
    "    \n",
    "    return sorted(set(l))\n",
    "\n",
    "\n",
    "def reconstruct_history(train_data_cutoff, actual_sales, model_info,\n",
    "                        cluster_keys=['product_nature', 'family'], min_ts_len=160):\n",
    "\n",
    "\n",
    "    last_week = train_data_cutoff.agg(F.max('week_id').alias('last_week'))\n",
    "    model_to_keep = train_data_cutoff.groupBy('model').agg(F.max('week_id').alias('last_active_week'))\n",
    "    model_to_keep = model_to_keep.join(last_week, last_week.last_week == model_to_keep.last_active_week, 'inner').select(model_to_keep.model)\n",
    "    train_data_cutoff = train_data_cutoff.join(model_to_keep, 'model', how='inner')\n",
    "\n",
    "\n",
    "    df_date = actual_sales.select(['week_id', 'date']).distinct()\n",
    "    y_not_null = train_data_cutoff.where(train_data_cutoff.y.isNotNull())\n",
    "\n",
    "    max_week = train_data_cutoff.select(F.max('week_id')).collect()[0][0]\n",
    "    min_week = train_data_cutoff.select(F.min('week_id')).collect()[0][0]\n",
    "\n",
    "    \n",
    "    list_weeks = find_weeks(min_week, max_week)\n",
    "    list_weeks = spark.createDataFrame(list_weeks, IntegerType()).selectExpr('value as week_id')\n",
    "    list_models = train_data_cutoff.select(train_data_cutoff.model).distinct()\n",
    "\n",
    "    full = list_weeks.crossJoin(list_models)\n",
    "    full_actives_sales = full.join(train_data_cutoff, ['week_id', 'model'], how='left')\n",
    "    #full_actives_sales = full_actives_sales[full_actives_sales['week_id'] < cutoff_week_id_test]\n",
    "\n",
    "    #full_actives_sales.describe().show()\n",
    "    # add cluster infos\n",
    "\n",
    "    mdl_inf = model_info.select(['model'] + cluster_keys)\n",
    "\n",
    "    complete_ts = full_actives_sales.join(mdl_inf, 'model', how='left').drop('date')\n",
    "    complete_ts = complete_ts.join(df_date, 'week_id', how='inner')\n",
    "\n",
    "    \n",
    "    # Calculate the average sales per cluster and week from actual_sales\n",
    "    all_sales = actual_sales.join(mdl_inf, 'model', how='left')\n",
    "    all_sales = all_sales.dropna()\n",
    "    join_key = ['week_id', 'date'] + cluster_keys\n",
    "    all_sales = all_sales.groupBy(join_key).agg(F.mean('y').alias('mean_cluster_y'))\n",
    "\n",
    "\n",
    "    # ad it to complete_ts\n",
    "    complete_ts = complete_ts.join(all_sales, ['week_id', 'date', 'product_nature', 'family'], how='left')\n",
    "\n",
    "    \n",
    "    #SCale factor\n",
    "    complete_ts = complete_ts.withColumn('row_scale_factor', complete_ts.y / complete_ts.mean_cluster_y)\n",
    "\n",
    "    model_scale_factor = complete_ts.groupBy('model').agg(F.mean('row_scale_factor').alias('model_scale_factor'))\n",
    "\n",
    "    complete_ts = complete_ts.join(model_scale_factor, ['model'], how='left')\n",
    "\n",
    "    # assert complete_ts.where(complete_ts.model_scale_factor.isNull()).count() == 0\n",
    "\n",
    "    \n",
    "    #compute fake Y\n",
    "    complete_ts = complete_ts.withColumn('fake_y', (complete_ts.mean_cluster_y * complete_ts.model_scale_factor).cast('int'))\n",
    "    complete_ts = complete_ts.fillna(0, subset=['fake_y'])\n",
    "    \n",
    "    \n",
    "    start_end = y_not_null.groupBy('model').agg(F.min('date').alias('start_date'), F.max('date').alias('end_date'))\n",
    "    complete_ts = complete_ts.join(start_end, 'model', how='left')\n",
    "    \n",
    "\n",
    "    complete_ts = complete_ts.withColumn('age', (F.datediff(F.col('date'), F.col('start_date'))) / (7) + 1 )\\\n",
    "                             .withColumn('length', (F.datediff(F.col('end_date'), F.col('date'))) / (7) + 1 )\\\n",
    "                             .withColumn('is_y_sup', F.when(complete_ts.y.isNull(), 'false')\\\n",
    "                                                      .when(complete_ts.y > complete_ts.fake_y, 'true')\\\n",
    "                                                      .otherwise('false'))\n",
    "    \n",
    "    \n",
    "    end_impl_period = complete_ts.filter(complete_ts.is_y_sup == True).select(['model', 'age']).groupBy('model').agg(F.min('age').alias('end_impl_period'))\n",
    "\n",
    "    complete_ts = complete_ts.join(end_impl_period, on=['model'], how='left')\n",
    "\n",
    "    \n",
    "    complete_ts = complete_ts.withColumn('y', \n",
    "                F.when(\n",
    "                    ((complete_ts.age <= 0) & (complete_ts.length <= min_ts_len)) | \\\n",
    "                    ((complete_ts.age > 0) & (complete_ts.age < complete_ts.end_impl_period)), complete_ts.fake_y.cast('int'))\\\n",
    "                 .otherwise(complete_ts.y).cast('int'))\n",
    "    \n",
    "    \n",
    "    \n",
    "    complete_ts = complete_ts.select(['week_id', 'date', 'model', 'y']).dropna(subset=('week_id', 'date', 'model', 'y'))\n",
    "\n",
    "\n",
    "    complete_ts = complete_ts.orderBy(['week_id', 'model'])\n",
    "    \n",
    "    return complete_ts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate training data used to forecast validation & test cutoffs\n",
    "- For each cutoff, keep only models active the week before the cutoff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b7a40aa654704e8aba31358f486b4a8c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+\n",
      "|week_id|\n",
      "+-------+\n",
      "| 201922|\n",
      "| 201923|\n",
      "| 201924|\n",
      "+-------+\n",
      "only showing top 3 rows"
     ]
    }
   ],
   "source": [
    "current_cutoff = next_week(actual_sales.select(F.max('week_id')).collect()[0][0])\n",
    "\n",
    "\n",
    "cutoff_week_test = active_sales.where(active_sales.week_id >= first_test_cutoff).select(active_sales.week_id).distinct().orderBy('week_id')\n",
    "\n",
    "nRow = spark.createDataFrame([[current_cutoff]])\n",
    "cutoff_week_test = cutoff_week_test.union(nRow)\n",
    "\n",
    "cutoff_week_test.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "77fdd3bb71bb42588e0e958fd1124977",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+\n",
      "|week_id|\n",
      "+-------+\n",
      "| 201814|\n",
      "| 201824|\n",
      "| 201834|\n",
      "+-------+\n",
      "only showing top 3 rows"
     ]
    }
   ],
   "source": [
    "cutoff_week_val = active_sales.filter(active_sales.week_id < first_test_cutoff).select(active_sales.week_id).distinct().orderBy('week_id')\n",
    "\n",
    "max_cutoff_week_val = cutoff_week_val.select(F.max('week_id')).collect()[0][0]\n",
    "\n",
    "# keep the last 60 dates before test set for validation\n",
    "\n",
    "_sup_week = __add_week(max_cutoff_week_val, (-horizon +1))\n",
    "_inf_week = __add_week(max_cutoff_week_val, -(60 + horizon -1))\n",
    "\n",
    "\n",
    "cutoff_week_val = cutoff_week_val.filter((cutoff_week_val.week_id > _inf_week) & (cutoff_week_val.week_id <= _sup_week))\n",
    "\n",
    "# keep only one cutoff every 10 dates\n",
    "idx_to_keep = ((np.arange(cutoff_week_val.count()) + 1) % 10 == 0)\n",
    "idx = np.arange(1, len(idx_to_keep)+1)\n",
    "idx_to_keep = np.c_[idx_to_keep, idx]\n",
    "\n",
    "nRow = spark.createDataFrame(idx_to_keep.tolist()).selectExpr('_1 as value', '_2 as id')\n",
    "cutoff_week_val = cutoff_week_val.withColumn('id', F.row_number().over(Window.orderBy('week_id')))\n",
    "cutoff_week_val = cutoff_week_val.join(nRow, 'id', how='inner').drop('id')\n",
    "cutoff_week_val = cutoff_week_val.filter(cutoff_week_val.value == 1).drop('value')\n",
    "\n",
    "cutoff_week_val.show(3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "39141260e2064a99865299373af8b90a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#weeks cutoff\n",
    "iterate_week = cutoff_week_val.union(cutoff_week_test)\n",
    "iterate_week = [row.week_id for row in iterate_week.collect()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2221daef5e994d07a75061e0e8d84234",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Generating train data for cutoff', '201814')\n",
      "temps boucle 201814 27.1882758141:\n",
      "('Generating train data for cutoff', '201824')\n",
      "temps boucle 201824 28.7923500538:\n",
      "('Generating train data for cutoff', '201834')\n",
      "temps boucle 201834 28.1657111645:"
     ]
    }
   ],
   "source": [
    "for cutoff_week_id in sorted(iterate_week[:3]):\n",
    "    \n",
    "    t0 = time.time()\n",
    "    print('Generating train data for cutoff', str(cutoff_week_id))\n",
    "            \n",
    "    train_data_cutoff = active_sales.filter(active_sales.week_id < cutoff_week_id)\n",
    "       \n",
    "    model_sold = train_data_cutoff.select(['model', 'y'])\\\n",
    "                 .groupBy('model')\\\n",
    "                 .agg(F.sum(train_data_cutoff.y).alias('qty_sold'))\\\n",
    "                 .orderBy('model')\n",
    "\n",
    "    model_sold = model_sold.filter(model_sold.qty_sold > 0).select('model').orderBy('model')\n",
    "    \n",
    "    last_week = train_data_cutoff.agg(F.max('week_id').alias('last_week'))\n",
    "    \n",
    "    model_active = train_data_cutoff.groupBy('model').agg(F.max('week_id').alias('last_active_week'))\n",
    "    \n",
    "    model_active = model_active.join(last_week, last_week.last_week == model_active.last_active_week, 'inner').select(model_active.model)\n",
    "    \n",
    "    model_to_keep = model_active.join(model_sold, 'model', 'inner')\n",
    "    \n",
    "    train_data_cutoff = train_data_cutoff.join(model_to_keep, 'model', how='inner')\n",
    "    \n",
    "    # Reconstruct a fake history\n",
    "    train_data_cutoff = reconstruct_history(train_data_cutoff, actual_sales, model_info)\n",
    "    \n",
    "    train_data_cutoff.write.parquet('s3://fcst-refined-demand-forecast-dev/scope/{}/train_data_cutoff/train_data_cutoff_{}'.format(scope, str(cutoff_week_id)), mode=\"overwrite\")    \n",
    "\n",
    "    t1 = time.time()\n",
    "    total = t1-t0\n",
    "    \n",
    "    print('temps boucle {} {}:'.format(str(cutoff_week_id), total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3://fcst-refined-demand-forecast-dev/scope/domyos_nov_2019/train_data_cutoff/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "210a2a9ecb71431ca459c4f28d4ed422",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Sparkmagic (PySpark)",
   "language": "",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 3
   },
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
