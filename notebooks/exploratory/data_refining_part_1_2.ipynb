{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import SparkSession, Window\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "\n",
    "#import numpy as np\n",
    "import sys\n",
    "#import datetime\n",
    "#from datetime import datetime, timedelta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_parquet_s3(app, s3_path):\n",
    "    \n",
    "    df = app.read.parquet(s3_path)\n",
    "    path_sinature = \">> Parquet file read from \" + s3_path\n",
    "    \n",
    "    print(\"{:<32}\".format(path_sinature) + '\\n')\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "def write_parquet_s3(spark_df, bucket, file_path):\n",
    "    \"\"\"Writing spark Dataframe into s3 as a parquet file.\n",
    "    \n",
    "    Parameters:\n",
    "    spark_df (pyspark.sql.dataframe): the spark Dataframe.\n",
    "    bucket (string): the s3 bucket name.\n",
    "    file_path (string) : the table name or directory.\n",
    "    \n",
    "    Returns:\n",
    "    \"\"\"\n",
    "    s3_path = 's3://{}/{}'.format(bucket, file_path)\n",
    "    spark_df.write.parquet(s3_path, mode=\"overwrite\")\n",
    "    \n",
    "    print(\">> Parquet file written on {}\".format(s3_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data from refining part 1_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actual_sales = read_parquet_s3(spark, 's3://fcst-refined-demand-forecast-dev/part_1_1/actual_sales/')\n",
    "\n",
    "actual_sales.cache()\n",
    "actual_sales.printSchema()\n",
    "actual_sales.show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lifestage_update = read_parquet_s3(spark, 's3://fcst-refined-demand-forecast-dev/part_1_1/lifestage_update/')\n",
    "\n",
    "lifestage_update.cache()\n",
    "lifestage_update.printSchema()\n",
    "lifestage_update.show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_info = read_parquet_s3(spark, 's3://fcst-refined-demand-forecast-dev/part_1_1/model_info/')\n",
    "\n",
    "model_info.cache()\n",
    "model_info.printSchema()\n",
    "#model_info.show(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Delete incomplete weeks ?\n",
    "- To be sure to have complete weeks (Sunday --> Saturday) regardless raw data extraction date, the last week of sales are deleted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########## TO DEL ##########\n",
    "#actual_sales = actual_sales \\\n",
    "#    .filter(actual_sales.week_id >= 201838) \\\n",
    "#    .filter(actual_sales.week_id <= 201841)\n",
    "actual_sales.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_week_id = actual_sales.select(F.max('week_id')).collect()[0][0]\n",
    "\n",
    "actual_sales = actual_sales \\\n",
    "    .filter(actual_sales.week_id < max_week_id) \\\n",
    "    .orderBy('model', 'date')\n",
    "\n",
    "actual_sales.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actual_sales.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Format life stages values\n",
    "/!\\ life stage values are only historized since September 10, 2018\n",
    "\n",
    "##### 1) Keep only usefull life stage values: models in actual sales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lifestage_update.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lifestage_update = lifestage_update.join(actual_sales.select('model').drop_duplicates(), \n",
    "                                         on='model', how='inner')\n",
    "\n",
    "lifestage_update.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lifestage_update.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2) Life stage updates ==> Life stage by week"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculates all possible date/model combinations associated with a life stage update\n",
    "min_date = lifestage_update.select(F.min('date_begin')).collect()[0][0]\n",
    "\n",
    "all_lifestage_date = actual_sales \\\n",
    "    .filter(actual_sales.date >= min_date) \\\n",
    "    .select('date') \\\n",
    "    .drop_duplicates() \\\n",
    "    .orderBy('date')\n",
    "\n",
    "all_lifestage_model = lifestage_update.select('model').drop_duplicates().orderBy('model')\n",
    "\n",
    "date_model = all_lifestage_date.crossJoin(all_lifestage_model)\n",
    "\n",
    "# Calculate lifestage by date\n",
    "model_lifestage = date_model.join(lifestage_update, on='model', how='left')\n",
    "model_lifestage = model_lifestage \\\n",
    "    .filter((model_lifestage.date >= model_lifestage.date_begin) &\n",
    "            (model_lifestage.date <= model_lifestage.date_end)) \\\n",
    "    .drop('date_begin', 'date_end')\n",
    "\n",
    "# The previous filter removes combinations that do not match the update dates.\n",
    "# But sometimes the update dates do not cover all periods, \n",
    "# which causes some dates to disappear, even during the model's activity periods.\n",
    "# To avoid this problem, we must merge again with all combinations to be sure \n",
    "# not to lose anything.\n",
    "model_lifestage = date_model.join(model_lifestage, on=['date', 'model'], how='left')\n",
    "\n",
    "model_lifestage.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_lifestage.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3) Deal with duplicates rows & NA\n",
    "- If we have several life stage information for the same model and date, then we take the minimum\n",
    "- If no life stage is filled in, we take the last known value (if exists)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_lifestage = model_lifestage \\\n",
    "    .groupby(['date', 'model']) \\\n",
    "    .agg(F.min('lifestage').alias('lifestage'))\n",
    "\n",
    "# This is a ffil by group in pyspark ==> OMG\n",
    "window = Window.partitionBy('model')\\\n",
    "               .orderBy('date')\\\n",
    "               .rowsBetween(-sys.maxsize, 0)\n",
    "\n",
    "ffilled_lifestage = F.last(model_lifestage['lifestage'], ignorenulls=True).over(window)\n",
    "\n",
    "model_lifestage = model_lifestage.withColumn('lifestage', ffilled_lifestage)\n",
    "\n",
    "model_lifestage.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_lifestage.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 4) Deal with zombie models\n",
    "If the life stage changes from active (1) to inactive (2 or more) and then back active, we consider the model is a zombie.  \n",
    "This new life may have a different sales behaviour than the previous one, so it's better to pretend that the latter is the only one that never existed.  \n",
    "\n",
    "For example, if the life stage looks like this: **1 1 1 3 3 3 1 1 1**, we only keep that: **3 1 1 1**.  \n",
    "Note that we still keep the last inactive value (here 3) before life stages 1 in order to avoid that the model life stages are considered incomplete in the following session."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('NB row before: ', model_lifestage.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_lifestage = model_lifestage \\\n",
    "    .withColumn('lifestage_shift', \n",
    "                F.lag(model_lifestage['lifestage']) \\\n",
    "                      .over(Window.partitionBy(\"model\").orderBy(F.desc('date'))))\n",
    "\n",
    "model_lifestage = model_lifestage \\\n",
    "    .withColumn('diff_shift', model_lifestage['lifestage'] - \\\n",
    "                              model_lifestage['lifestage_shift'])\n",
    "\n",
    "df_cut_date = model_lifestage.filter(model_lifestage.diff_shift > 0)\n",
    "\n",
    "df_cut_date = df_cut_date \\\n",
    "    .groupBy('model') \\\n",
    "    .agg(F.max('date').alias('cut_date'))\n",
    "\n",
    "model_lifestage = model_lifestage.join(df_cut_date, on=['model'], how='left')\n",
    "\n",
    "# if no cut_date, fill by an old one\n",
    "model_lifestage = model_lifestage \\\n",
    "    .withColumn('cut_date', F.when(F.col('cut_date').isNull(),\n",
    "                                   F.to_date(F.lit('1993-04-15'), 'yyyy-MM-dd')) \\\n",
    "                                   .otherwise(F.col('cut_date')))\n",
    "\n",
    "model_lifestage = model_lifestage \\\n",
    "    .filter(model_lifestage.date >= model_lifestage.cut_date) \\\n",
    "    .select(['date', 'model', 'lifestage'])\n",
    "\n",
    "model_lifestage.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('NB row after: ', model_lifestage.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Match sales and life stages & rebuild incomplete life stages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1) Complete sales\n",
    "- Fill missing quantities by 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculates all possible date/model combinations from actual sales\n",
    "all_sales_model = actual_sales.select('model').orderBy('model').drop_duplicates()\n",
    "all_sales_date = actual_sales.select('date').orderBy('date').drop_duplicates()\n",
    "\n",
    "date_model = all_sales_model.crossJoin(all_sales_date)\n",
    "\n",
    "# Add corresponding week id\n",
    "date_model = date_model.join(actual_sales.select(['date', 'week_id']).drop_duplicates(), \n",
    "                             on=['date'], how='inner')\n",
    "\n",
    "# Add actual sales\n",
    "complete_ts = date_model.join(actual_sales, on=['date', 'model', 'week_id'], how='left')\n",
    "complete_ts = complete_ts.select(actual_sales.columns)\n",
    "\n",
    "# Fill NaN (no sales recorded) by 0\n",
    "complete_ts = complete_ts.fillna(0, subset=['y'])\n",
    "complete_ts.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "complete_ts.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2) Add model life stage by week"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "complete_ts = complete_ts.join(model_lifestage, ['date', 'model'], how='left')\n",
    "complete_ts.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "complete_ts.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3) Rebuild incomplete life stages\n",
    "/!\\ Reminder: the life stage values are only historized since September 10, 2018\n",
    "- If the life stage value is 1 at the first historized date \n",
    "- And we observe sales in the previous and consecutive weeks\n",
    "- Then we fill the life stage values of these weeks with 1 as well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_column_index(df, col_name): \n",
    "    new_schema = StructType(df.schema.fields + [StructField(col_name, LongType(), False),])\n",
    "    return df.rdd.zipWithIndex().map(lambda row: row[0] + (row[1], )).toDF(schema=new_schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "complete_ts.filter(complete_ts.lifestage == 1).count() #49688232"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find models respecting the first condition\n",
    "w = Window.partitionBy('model').orderBy('date')\n",
    "\n",
    "first_lifestage = complete_ts.filter(complete_ts.lifestage.isNotNull()) \\\n",
    "                             .withColumn('rn', F.row_number().over(w))\n",
    "\n",
    "first_lifestage = first_lifestage.where(first_lifestage.rn == 1).drop('rn')\n",
    "\n",
    "\n",
    "first_lifestage = first_lifestage \\\n",
    "    .filter(first_lifestage.lifestage == 1) \\\n",
    "    .select(first_lifestage.model, \n",
    "            first_lifestage.date.alias('first_lifestage_date'))\n",
    "\n",
    "# Create the mask (rows to be completed) for theses models\n",
    "complete_ts = add_column_index(complete_ts, 'idx') # save original indexes\n",
    "complete_ts.cache()\n",
    "\n",
    "mask = complete_ts\n",
    "\n",
    "# keep only models respecting the first condition\n",
    "mask = mask.join(first_lifestage, on='model', how='inner')\n",
    "\n",
    "# Look only before the first historized lifestage date\n",
    "mask = mask.filter(mask.date <= mask.first_lifestage_date)\n",
    "\n",
    "w = Window.partitionBy('model').orderBy(F.desc('date'))\n",
    "\n",
    "mask = mask \\\n",
    "    .withColumn('cumsum_y', F.sum('y').over(w)) \\\n",
    "    .withColumn('lag_cumsum_y', F.lag('cumsum_y').over(w)) \\\n",
    "    .fillna(0, subset=['lag_cumsum_y']) \\\n",
    "    .withColumn('is_active', F.col('cumsum_y') > F.col('lag_cumsum_y'))\n",
    "\n",
    "ts_start_date = mask \\\n",
    "    .filter(mask.is_active == False) \\\n",
    "    .withColumn('rn', F.row_number().over(w)) \\\n",
    "    .filter(F.col('rn') == 1) \\\n",
    "    .select('model', F.col('date').alias('start_date'))\n",
    "\n",
    "mask = mask.join(ts_start_date, on='model', how='left')\n",
    "\n",
    "# Case model start date unknown (older than first week recorded here)\n",
    "# ==> fill by an old date\n",
    "mask = mask \\\n",
    "    .withColumn('start_date', F.when(F.col('start_date').isNull(),\n",
    "                                     F.to_date(F.lit('1993-04-15'), 'yyyy-MM-dd')) \\\n",
    "                                     .otherwise(F.col('start_date'))) \\\n",
    "    .withColumn('is_model_start', F.col('date') > F.col('start_date')) \\\n",
    "    .withColumn('to_fill', F.col('is_active') & \\\n",
    "                           F.col('is_model_start') & \\\n",
    "                           F.col('lifestage').isNull())\n",
    "\n",
    "\n",
    "mask = mask.filter(mask.to_fill == True).select(['idx', 'to_fill'])\n",
    "\n",
    "# Fill the eligible rows under all conditions\n",
    "complete_ts = complete_ts.join(mask, on='idx', how='left')\n",
    "complete_ts = complete_ts \\\n",
    "    .withColumn('lifestage', \n",
    "                F.when(F.col('to_fill') == True, F.lit(1)).otherwise(F.col('lifestage')))\n",
    "\n",
    "complete_ts = complete_ts.select(['week_id', 'date', 'model', 'y', 'lifestage'])\n",
    "\n",
    "complete_ts.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "complete_ts.filter(complete_ts.lifestage == 1).count() #1432218 --> 2811460 --> 3178307 xxxx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create active sales data set\n",
    "\n",
    "##### 1) Keep in memory first sales dates by model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = Window.partitionBy('model').orderBy('date')\n",
    "\n",
    "model_start_date = actual_sales.withColumn('rn', F.row_number().over(w))\n",
    "\n",
    "model_start_date = model_start_date \\\n",
    "    .filter(model_start_date.rn == 1) \\\n",
    "    .drop('rn', 'week_id', 'y') \\\n",
    "    .select(F.col(\"model\"), F.col(\"date\").alias(\"first_date\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_start_date.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2) Construct active sales\n",
    "- Filtered on active life stage \n",
    "- After the first actual sales date\n",
    "- Padded with zeros (already done in complete sales)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Nb rows before:', complete_ts.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "active_sales = complete_ts \\\n",
    "    .filter(complete_ts.lifestage == 1) \\\n",
    "    .join(model_start_date, on='model', how='inner') \\\n",
    "    .filter(complete_ts.date >= model_start_date.first_date) \\\n",
    "    .drop('lifestage', 'first_date')\n",
    "                           \n",
    "active_sales.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Nb rows after: ', active_sales.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean model info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_info = model_info \\\n",
    "    .withColumn('category_label', \n",
    "                F.when(model_info.category_label == 'SOUS RAYON POUB', F.lit(None)) \\\n",
    "                .otherwise(model_info.category_label)) \\\n",
    "    .fillna('UNKNOWN')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Due to a discrepant seasonal behaviour between LOW SOCKS and HIGH SOCKS, we chose to split\n",
    "# the product nature 'SOCKS' into two different product natures 'LOW SOCKS' and 'HIGH SOCKS'\n",
    "\n",
    "model_info = model_info \\\n",
    "    .withColumn('product_nature_label', \n",
    "                F.when((model_info.product_nature_label == 'SOCKS') & \\\n",
    "                       (model_info.model_label.contains(' LOW')), \n",
    "                       F.lit('LOW SOCKS')) \\\n",
    "                 .when((model_info.product_nature_label == 'SOCKS') & \\\n",
    "                       (model_info.model_label.contains(' MID')), \n",
    "                       F.lit('MID SOCKS')) \\\n",
    "                 .when((model_info.product_nature_label == 'SOCKS') & \\\n",
    "                       (model_info.model_label.contains(' HIGH')), \n",
    "                       F.lit('HIGH SOCKS')) \\\n",
    "                 .otherwise(model_info.product_nature_label)) \\\n",
    "    .drop('product_nature')\n",
    "    \n",
    "indexer = StringIndexer(inputCol='product_nature_label', outputCol='product_nature')\n",
    "model_info = indexer \\\n",
    "    .fit(model_info) \\\n",
    "    .transform(model_info) \\\n",
    "    .withColumn('product_nature', F.col('product_nature').cast('integer'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unit tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check duplicates rows\n",
    "assert active_sales.groupBy(['date', 'model']).count().select(F.max(\"count\")).collect()[0][0] == 1\n",
    "assert model_info.count() == model_info.select('model').drop_duplicates().count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_parquet_s3(model_info, 'fcst-refined-demand-forecast-dev', 'part_1_2/model_info')\n",
    "write_parquet_s3(actual_sales, 'fcst-refined-demand-forecast-dev', 'part_1_2/actual_sales')\n",
    "write_parquet_s3(active_sales, 'fcst-refined-demand-forecast-dev', 'part_1_2/active_sales')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Sparkmagic (PySpark)",
   "language": "",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 2
   },
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
