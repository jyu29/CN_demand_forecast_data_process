{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.functions as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#conf = SparkConf().setAll([\n",
    "# ('spark.sql.shuffle.partitions', 110),\n",
    "# ('spark.default.parallelism', 110),\n",
    "# ('spark.autoBroadcastJoinThreshold', 15485760),\n",
    "# ('spark.dynamicAllocation.enabled', 'false'),\n",
    "# ('spark.executor.instances', 11),\n",
    "# ('spark.executor.memory', '36g'),\n",
    "# ('spark.driver.memory', '36g'),\n",
    "# ('spark.driver.cores', 5),\n",
    "# ('spark.memory.storageFraction', 0.1),\n",
    "# ('spark.memory.fraction', 0.9),\n",
    "# ('spark.executor.memoryOverhead', '4g'),\n",
    "# ('spark.executor.cores', 5),\n",
    "# ('spark.hadoop.mapreduce.fileoutputcommitter.algorithm.version', 2)\n",
    "#])\n",
    "#\n",
    "#spark = SparkSession.builder \\\n",
    "#    .config(conf=conf) \\\n",
    "#    .appName(\"sql\") \\\n",
    "#    .getOrCreate()\n",
    "#for x in spark.sparkContext.getConf().getAll() :\n",
    "#    print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_parquet_s3(app, s3_path):\n",
    "    \n",
    "    df = app.read.parquet(s3_path)\n",
    "    path_sinature = \">> Parquet file read from \" + s3_path\n",
    "    \n",
    "    print(\"{:<32}\".format(path_sinature) + '\\n')\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "def write_parquet_s3(spark_df, bucket, file_path):\n",
    "    \"\"\"Writing spark Dataframe into s3 as a parquet file.\n",
    "    \n",
    "    Parameters:\n",
    "    spark_df (pyspark.sql.dataframe): the spark Dataframe.\n",
    "    bucket (string): the s3 bucket name.\n",
    "    file_path (string) : the table name or directory.\n",
    "    \n",
    "    Returns:\n",
    "    \"\"\"\n",
    "    s3_path = 's3://{}/{}'.format(bucket, file_path)\n",
    "    spark_df.write.parquet(s3_path, mode=\"overwrite\")\n",
    "    \n",
    "    print(\">> Parquet file written on {}\".format(s3_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_week_id = 201501\n",
    "first_date = '2015-01-01'#2014-12-28\n",
    "purch_org = 'Z001'\n",
    "sales_org = 'Z002'\n",
    "bucket = 's3://fcst-clean-dev/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load all needed clean data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tdt = read_parquet_s3(spark, bucket + 'f_transaction_detail/*/')\n",
    "dyd = read_parquet_s3(spark, bucket + 'f_delivery_detail/*/')\n",
    "\n",
    "sku = read_parquet_s3(spark, bucket + 'd_sku/')\n",
    "bu = read_parquet_s3(spark, bucket + 'd_business_unit/')\n",
    "\n",
    "sapb = read_parquet_s3(spark, bucket + 'sites_attribut_0plant_branches_h/')\n",
    "sdm = read_parquet_s3(spark, bucket + 'sales_data_material/')\n",
    "sdmh = read_parquet_s3(spark, bucket + 'd_sales_data_material_h/')\n",
    "\n",
    "day = read_parquet_s3(spark, bucket + 'd_day/')\n",
    "week = read_parquet_s3(spark, bucket + 'd_week/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Actual_sales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actual_sales_offline = tdt \\\n",
    "    .join(day,\n",
    "          on=F.to_date(tdt.tdt_date_to_ordered, 'yyyy-MM-dd') == day.day_id_day,\n",
    "          how='inner') \\\n",
    "    .join(week, \n",
    "          on=day.wee_id_week == week.wee_id_week, \n",
    "          how='inner') \\\n",
    "    .join(sku,\n",
    "          on=tdt.sku_idr_sku == sku.sku_idr_sku, \n",
    "          how='inner') \\\n",
    "    .join(bu, \n",
    "          on=tdt.but_idr_business_unit == bu.but_idr_business_unit, \n",
    "          how='inner') \\\n",
    "    .join(sapb,\n",
    "          on=bu.but_num_business_unit.cast('string') == \\\n",
    "             F.regexp_replace(sapb.plant_id, '^0*|\\s',''),\n",
    "          how='inner') \\\n",
    "    .filter(tdt.the_to_type == 'offline') \\\n",
    "    .filter(week.wee_id_week >= first_week_id) \\\n",
    "    .filter(~sku.unv_num_univers.isin([0, 14, 89, 90])) \\\n",
    "    .filter(sku.mdl_num_model_r3.isNotNull()) \\\n",
    "    .filter(sapb.purch_org == purch_org) \\\n",
    "    .filter(sapb.sapsrc == 'PRT') \\\n",
    "    .filter(F.current_timestamp().between(sapb.date_begin, sapb.date_end)) \\\n",
    "    .select(week.wee_id_week.alias('week_id'),\n",
    "            week.day_first_day_week.alias('date'),\n",
    "            sku.mdl_num_model_r3.alias('model'),\n",
    "            tdt.f_qty_item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actual_sales_online = dyd \\\n",
    "    .join(day,\n",
    "          on=F.to_date(dyd.tdt_date_to_ordered, 'yyyy-MM-dd') == day.day_id_day,\n",
    "          how='inner') \\\n",
    "    .join(week, \n",
    "          on=day.wee_id_week == week.wee_id_week, \n",
    "          how='inner') \\\n",
    "    .join(sku, \n",
    "          on=dyd.sku_idr_sku == sku.sku_idr_sku, \n",
    "          how='inner') \\\n",
    "    .join(bu, \n",
    "          on=dyd.but_idr_business_unit_economical == bu.but_idr_business_unit, \n",
    "          how='inner') \\\n",
    "    .join(sapb,\n",
    "          on=bu.but_num_business_unit.cast('string') == \\\n",
    "             F.regexp_replace(sapb.plant_id, '^0*|\\s',''),\n",
    "          how='inner') \\\n",
    "    .filter(dyd.the_to_type == 'online') \\\n",
    "    .filter(week.wee_id_week >= first_week_id) \\\n",
    "    .filter(~sku.unv_num_univers.isin([0, 14, 89, 90])) \\\n",
    "    .filter(sku.mdl_num_model_r3.isNotNull()) \\\n",
    "    .filter(sapb.purch_org == purch_org) \\\n",
    "    .filter(sapb.sapsrc == 'PRT') \\\n",
    "    .filter(F.current_timestamp().between(sapb.date_begin, sapb.date_end)) \\\n",
    "    .select(week.wee_id_week.alias('week_id'),\n",
    "            week.day_first_day_week.alias('date'),\n",
    "            sku.mdl_num_model_r3.alias('model'),\n",
    "            dyd.f_qty_item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actual_sales = actual_sales_offline.union(actual_sales_online) \\\n",
    "    .groupby(['week_id', 'date', 'model']) \\\n",
    "    .agg(F.sum('f_qty_item').alias('y')) \\\n",
    "    .filter(F.col('y') > 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#actual_sales.show(3)\n",
    "#actual_sales.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Lifestage_update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lifestage_update = sdmh \\\n",
    "    .join(sku, \n",
    "          on=F.regexp_replace(sdmh.material_id, '^0*|\\s','') == \\\n",
    "             sku.sku_num_sku_r3.cast('string'),\n",
    "          how='inner') \\\n",
    "    .filter(sdmh.sales_org == sales_org) \\\n",
    "    .filter(sdmh.sap_source == 'PRT') \\\n",
    "    .filter(sdmh.lifestage != '') \\\n",
    "    .filter(sdmh.distrib_channel == '02') \\\n",
    "    .filter(sdmh.date_end >= first_date) \\\n",
    "    .filter(sku.mdl_num_model_r3.isNotNull()) \\\n",
    "    .filter(~sku.unv_num_univers.isin([0, 14, 89, 90])) \\\n",
    "    .filter(F.current_timestamp().between(sku.sku_date_begin, sku.sku_date_end)) \\\n",
    "    .withColumn(\"date_end\", \n",
    "                F.when(sdmh.date_end == '2999-12-31', \n",
    "                       F.to_date(F.lit('2100-12-31'), 'yyyy-MM-dyd')) \\\n",
    "                       .otherwise(sdmh.date_end)) \\\n",
    "    .select(sku.mdl_num_model_r3.alias('model'), \n",
    "            sku.sku_num_sku_r3.alias('sku'),\n",
    "            sdmh.date_begin,\n",
    "            \"date_end\",\n",
    "            sdmh.lifestage.cast('int').alias('lifestage')) \\\n",
    "    .drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lifestage_update.show(3)\n",
    "#lifestage_update.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Model_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_info = sdm \\\n",
    "    .join(sku, \n",
    "          on=F.regexp_replace(sdm.material_id, '^0*|\\s','') == \\\n",
    "             sku.sku_num_sku_r3.cast('string'),\n",
    "          how='inner') \\\n",
    "    .filter(sdm.sales_org == sales_org) \\\n",
    "    .filter(sdm.sap_source == 'PRT') \\\n",
    "    .filter(sdm.assortment_grade != '') \\\n",
    "    .filter(sdm.distrib_channel == '02') \\\n",
    "    .filter(sku.mdl_num_model_r3.isNotNull()) \\\n",
    "    .filter(~sku.unv_num_univers.isin([0, 14, 89, 90])) \\\n",
    "    .filter(F.current_timestamp().between(sku.sku_date_begin, sku.sku_date_end)) \\\n",
    "    .select(sku.mdl_num_model_r3.alias('model'),\n",
    "            sku.mdl_label.alias('model_label'),\n",
    "            sku.fam_num_family.alias('family'),\n",
    "            sku.family_label.alias('family_label'),\n",
    "            sku.sdp_num_sub_department.alias('sub_department'),\n",
    "            sku.sdp_label.alias('sub_department_label'),\n",
    "            sku.dpt_num_department.alias('department'),\n",
    "            sku.unv_label.alias('department_label'),\n",
    "            sku.unv_num_univers.alias('univers'),\n",
    "            sku.mdl_label.alias('univers_label'),\n",
    "            sku.pnt_num_product_nature.alias('product_nature'),\n",
    "            sku.product_nature_label.alias('product_nature_label'),\n",
    "            sku.category_label.alias('category_label'),\n",
    "            sdm.assortment_grade.alias('range_level')) \\\n",
    "    .drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model_info.show(3)\n",
    "#model_info.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actual_sales.cache()\n",
    "model_info.cache()\n",
    "lifestage_update.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(actual_sales.count())\n",
    "print(lifestage_update.count())\n",
    "print(model_info.count())"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "actual_sales:\n",
    "10 699 601  SPARK\n",
    "10 240 892  PY\n",
    "10 650 999  SQL (2j apres)\n",
    "\n",
    "lifestage_update:\n",
    "1 190 988 SPARK\n",
    "1 191 457 PY\n",
    "\n",
    "model_info (non fix):\n",
    "224 326 SPARK\n",
    "224 309 PY\n",
    "\n",
    "model_info (fix):\n",
    "225 770 SPARK\n",
    "225 807 PY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actual_sales.describe().show()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "        week_id\t  model\t    y\n",
    "count\t10240892  10240892\t10240892\n",
    "mean\t201724\t  8095814\t415\n",
    "std\t    144\t      1067305\t1958\n",
    "min     201501\t  1\t        1\n",
    "max\t    202002\t  8605547\t215599"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lifestage_update.describe().show()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "\t    model\tsku\tlifestage\n",
    "count\t1191457\t1191457\t1191457\n",
    "mean\t7890793\t1617896\t4\n",
    "std\t    1391353\t901573\t2\n",
    "min\t    4\t    988\t    0\n",
    "max\t    8607432\t4167879\t9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_info.describe().show()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "\t    model\tfamily\tsub_department\tdepartment\tunivers\tproduct_nature\trange_level\n",
    "count\t225807\t225807\t225807\t225807\t225807\t225807\t225807\n",
    "mean\t7510917\t10580\t2182\t397\t    1720\t18571\t2\n",
    "std\t    1774267\t9233\t851\t    257\t    3517\t10621\t1\n",
    "min\t    4\t    22\t    137\t    13\t    2\t    0\t    0\n",
    "max\t    8607432\t99996\t8011\t904\t    9082\t26603\t5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_parquet_s3(actual_sales, 'fcst-refined-demand-forecast-dev', 'part_1_1/actual_sales')\n",
    "write_parquet_s3(lifestage_update, 'fcst-refined-demand-forecast-dev', 'part_1_1/lifestage_update')\n",
    "write_parquet_s3(model_info, 'fcst-refined-demand-forecast-dev', 'part_1_1/model_info')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Sparkmagic (PySpark)",
   "language": "",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 2
   },
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
