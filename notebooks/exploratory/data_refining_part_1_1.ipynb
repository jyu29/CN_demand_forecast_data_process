{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DateType \n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "from datetime import date, datetime, timedelta\n",
    "import boto3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .appName(\"sql\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_parquet_s3(app, s3_path):\n",
    "    \n",
    "    df = app.read.parquet(s3_path)\n",
    "    path_sinature = \">> Parquet file read from \" + s3_path\n",
    "    \n",
    "    print(\"{:<32}\".format(path_sinature) + '\\n')\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "def write_parquet_s3(spark_df, bucket, file_path):\n",
    "    \"\"\"Writing spark Dataframe into s3 as a parquet file.\n",
    "    \n",
    "    Parameters:\n",
    "    spark_df (pyspark.sql.dataframe): the spark Dataframe.\n",
    "    bucket (string): the s3 bucket name.\n",
    "    file_path (string) : the table name or directory.\n",
    "    \n",
    "    Returns:\n",
    "    \"\"\"\n",
    "    s3_path = 's3://{}/{}'.format(bucket, file_path)\n",
    "    spark_df.write.parquet(s3_path, mode=\"overwrite\")\n",
    "    \n",
    "    print(\">> Parquet file written on {}\".format(s3_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_week_id = 201601\n",
    "first_date = '2016-01-03'\n",
    "purch_org = 'Z001'\n",
    "sales_org = 'Z002'\n",
    "bucket = 's3://fcst-workspace/Z21GABDO/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load all needed clean data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_transaction_detail = read_parquet_s3(spark, bucket + 'f_transaction_detail/201912/')\n",
    "f_delivery_detail = read_parquet_s3(spark, bucket + 'f_delivery_detail/201912/')\n",
    "\n",
    "d_sku = read_parquet_s3(spark, bucket + 'd_sku/')\n",
    "d_business_unit = read_parquet_s3(spark, bucket + 'd_business_unit/')\n",
    "\n",
    "sapb = read_parquet_s3(spark, bucket + 'sites_attribut_0plant_branches_h/')\n",
    "sales_data_material = read_parquet_s3(spark, bucket + 'sales_data_material/')\n",
    "d_sales_data_material = read_parquet_s3(spark, bucket + 'd_sales_data_material_h/')\n",
    "\n",
    "d_day = read_parquet_s3(spark, bucket + 'd_day/')\n",
    "d_week = read_parquet_s3(spark, bucket + 'd_week/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Actual_sales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actual_sales_offline = f_transaction_detail \\\n",
    "    .join(d_day,\n",
    "          on=to_date(f_transaction_detail.tdt_date_to_ordered, 'yyyy-MM-dd') == d_day.day_id_day,\n",
    "          how='inner') \\\n",
    "    .join(d_week, \n",
    "          on=d_day.wee_id_week == d_week.wee_id_week, \n",
    "          how='inner') \\\n",
    "    .join(d_sku, \n",
    "          on=f_transaction_detail.sku_idr_sku == d_sku.sku_idr_sku, \n",
    "          how='inner') \\\n",
    "    .join(d_business_unit, \n",
    "          on=f_transaction_detail.but_idr_business_unit == d_business_unit.but_idr_business_unit, \n",
    "          how='inner') \\\n",
    "    .join(sapb,\n",
    "          on=d_business_unit.but_num_business_unit == sapb.plant_id.cast('int'), \n",
    "          how='inner') \\\n",
    "    .filter(f_transaction_detail.the_to_type == 'offline') \\\n",
    "    .filter(f_transaction_detail.f_qty_item > 0) \\\n",
    "    .filter(d_week.wee_id_week >= first_week_id) \\\n",
    "    .filter(~d_sku.unv_num_univers.isin([0, 14, 89, 90])) \\\n",
    "    .filter(sapb.purch_org == purch_org) \\\n",
    "    .filter(sapb.sapsrc == 'PRT') \\\n",
    "    .select(d_week.wee_id_week.alias('week_id'),\n",
    "            d_week.day_first_day_week.alias('date'),\n",
    "            d_sku.mdl_num_model_r3.alias('model'),\n",
    "            f_transaction_detail.f_qty_item.cast('int')) \\\n",
    "    .groupby(['week_id', 'date', 'model']) \\\n",
    "    .agg(sum('f_qty_item').alias('y_off'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actual_sales_online = f_delivery_detail \\\n",
    "    .join(d_day,\n",
    "          on=to_date(f_delivery_detail.tdt_date_to_ordered, 'yyyy-MM-dd') == d_day.day_id_day,\n",
    "          how='inner') \\\n",
    "    .join(d_week, \n",
    "          on=d_day.wee_id_week == d_week.wee_id_week, \n",
    "          how='inner') \\\n",
    "    .join(d_sku, \n",
    "          on=f_delivery_detail.sku_idr_sku == d_sku.sku_idr_sku, \n",
    "          how='inner') \\\n",
    "    .join(d_business_unit, \n",
    "          on=f_delivery_detail.but_idr_business_unit_economical == d_business_unit.but_idr_business_unit, \n",
    "          how='inner') \\\n",
    "    .join(sapb,\n",
    "          on=d_business_unit.but_num_business_unit == sapb.plant_id.cast('int'), \n",
    "          how='inner') \\\n",
    "    .filter(f_delivery_detail.the_to_type == 'online') \\\n",
    "    .filter(f_delivery_detail.f_qty_item > 0) \\\n",
    "    .filter(d_week.wee_id_week >= first_week_id) \\\n",
    "    .filter(~d_sku.unv_num_univers.isin([0, 14, 89, 90])) \\\n",
    "    .filter(sapb.purch_org == purch_org) \\\n",
    "    .filter(sapb.sapsrc == 'PRT') \\\n",
    "    .select(d_week.wee_id_week.alias('week_id'),\n",
    "            d_week.day_first_day_week.alias('date'),\n",
    "            d_sku.mdl_num_model_r3.alias('model'),\n",
    "            f_delivery_detail.f_qty_item.cast('int')) \\\n",
    "    .groupby(['week_id', 'date', 'model']) \\\n",
    "    .agg(sum('f_qty_item').alias('y_on'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actual_sales = actual_sales_offline \\\n",
    "    .join(actual_sales_online, on=['week_id', 'date', 'model']) \\\n",
    "    .withColumn('y', actual_sales_offline.y_off + actual_sales_online.y_on) \\\n",
    "    .select(['week_id', 'date', 'model', 'y'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#actual_sales.show(3)\n",
    "#actual_sales.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Lifestage_update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lifestage_update = d_sales_data_material \\\n",
    "    .join(d_sku, \n",
    "          on=d_sales_data_material.material_id.cast('int') == d_sku.sku_num_sku_r3,\n",
    "          how='inner') \\\n",
    "    .filter(d_sales_data_material.sales_org == sales_org) \\\n",
    "    .filter(d_sales_data_material.sap_source == 'PRT') \\\n",
    "    .filter(d_sales_data_material.lifestage != '') \\\n",
    "    .filter(d_sales_data_material.distrib_channel == '02') \\\n",
    "    .filter(d_sales_data_material.date_end >= first_date) \\\n",
    "    .filter(~d_sku.unv_num_univers.isin([0, 14, 89, 90])) \\\n",
    "    .withColumn(\"date_end\", \n",
    "                when(d_sales_data_material.date_end == '2999-12-31', to_date(lit('2100-12-31'), 'yyyy-MM-dd')) \\\n",
    "                    .otherwise(d_sales_data_material.date_end)) \\\n",
    "    .select(d_sku.mdl_num_model_r3.alias('model'), \n",
    "            d_sku.sku_num_sku_r3.alias('sku'),\n",
    "            d_sales_data_material.date_begin,\n",
    "            \"date_end\",\n",
    "            d_sales_data_material.lifestage.cast('int').alias('lifestage')) \\\n",
    "    .drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lifestage_update.show(3)\n",
    "#lifestage_update.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Model_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_info = sales_data_material \\\n",
    "    .join(d_sku, \n",
    "          on=sales_data_material.material_id.cast('int') == d_sku.sku_num_sku_r3,\n",
    "          how='inner') \\\n",
    "    .filter(sales_data_material.sales_org == sales_org) \\\n",
    "    .filter(sales_data_material.sap_source == 'PRT') \\\n",
    "    .filter(sales_data_material.lifestage != '') \\\n",
    "    .filter(sales_data_material.distrib_channel == '02') \\\n",
    "    .filter(~d_sku.unv_num_univers.isin([0, 14, 89, 90])) \\\n",
    "    .filter(current_date().between(d_sku.sku_date_begin, d_sku.sku_date_end)) \\\n",
    "    .select(d_sku.mdl_num_model_r3.alias('model'),\n",
    "            d_sku.mdl_label.alias('model_label'),\n",
    "            d_sku.fam_num_family.alias('family'),\n",
    "            d_sku.family_label.alias('family_label'),\n",
    "            d_sku.sdp_num_sub_department.alias('sub_department'),\n",
    "            d_sku.sdp_label.alias('sub_department_label'),\n",
    "            d_sku.dpt_num_department.alias('department'),\n",
    "            d_sku.unv_label.alias('department_label'),\n",
    "            d_sku.unv_num_univers.alias('univers'),\n",
    "            d_sku.mdl_label.alias('univers_label'),\n",
    "            d_sku.pnt_num_product_nature.alias('product_nature'),\n",
    "            d_sku.product_nature_label.alias('product_nature_label'),\n",
    "            d_sku.category_label.alias('category_label'),\n",
    "            sales_data_material.assortment_grade.alias('range_level')) \\\n",
    "    .drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model_info.show(3)\n",
    "#model_info.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_parquet_s3(actual_sales, 'fcst-refined-demand-forecast-dev', 'part_1_1/actual_sales')\n",
    "write_parquet_s3(lifestage_update, 'fcst-refined-demand-forecast-dev', 'part_1_1/lifestage_update')\n",
    "write_parquet_s3(model_info, 'fcst-refined-demand-forecast-dev', 'part_1_1/model_info')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Sparkmagic (PySpark)",
   "language": "",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 2
   },
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
