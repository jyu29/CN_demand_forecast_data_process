{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.functions as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_parquet_s3(app, s3_path):\n",
    "    \n",
    "    df = app.read.parquet(s3_path)\n",
    "    path_sinature = \">> Parquet file read from \" + s3_path\n",
    "    \n",
    "    print(\"{:<32}\".format(path_sinature) + '\\n')\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "def write_parquet_s3(spark_df, bucket, file_path):\n",
    "    \"\"\"Writing spark Dataframe into s3 as a parquet file.\n",
    "    \n",
    "    Parameters:\n",
    "    spark_df (pyspark.sql.dataframe): the spark Dataframe.\n",
    "    bucket (string): the s3 bucket name.\n",
    "    file_path (string) : the table name or directory.\n",
    "    \n",
    "    Returns:\n",
    "    \"\"\"\n",
    "    s3_path = 's3://{}/{}'.format(bucket, file_path)\n",
    "    spark_df.write.parquet(s3_path, mode=\"overwrite\")\n",
    "    \n",
    "    print(\">> Parquet file written on {}\".format(s3_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_week_id = 201501\n",
    "purch_org = 'Z001'\n",
    "sales_org = 'Z002'\n",
    "bucket = 's3://fcst-clean-dev/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load all needed clean data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tdt = read_parquet_s3(spark, bucket + 'f_transaction_detail/*/')\n",
    "dyd = read_parquet_s3(spark, bucket + 'f_delivery_detail/*/')\n",
    "\n",
    "sku = read_parquet_s3(spark, bucket + 'd_sku/')\n",
    "bu = read_parquet_s3(spark, bucket + 'd_business_unit/')\n",
    "\n",
    "sapb = read_parquet_s3(spark, bucket + 'sites_attribut_0plant_branches_h/')\n",
    "sdm = read_parquet_s3(spark, bucket + 'd_sales_data_material_h/')\n",
    "\n",
    "day = read_parquet_s3(spark, bucket + 'd_day/')\n",
    "week = read_parquet_s3(spark, bucket + 'd_week/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Actual_sales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actual_sales_offline = tdt \\\n",
    "    .join(day,\n",
    "          on=F.to_date(tdt.tdt_date_to_ordered, 'yyyy-MM-dd') == day.day_id_day,\n",
    "          how='inner') \\\n",
    "    .join(week, \n",
    "          on=day.wee_id_week == week.wee_id_week, \n",
    "          how='inner') \\\n",
    "    .join(sku,\n",
    "          on=tdt.sku_idr_sku == sku.sku_idr_sku, \n",
    "          how='inner') \\\n",
    "    .join(bu, \n",
    "          on=tdt.but_idr_business_unit == bu.but_idr_business_unit, \n",
    "          how='inner') \\\n",
    "    .join(sapb,\n",
    "          on=bu.but_num_business_unit.cast('string') == \\\n",
    "             F.regexp_replace(sapb.plant_id, '^0*|\\s',''),\n",
    "          how='inner') \\\n",
    "    .filter(tdt.the_to_type == 'offline') \\\n",
    "    .filter(week.wee_id_week >= first_week_id) \\\n",
    "    .filter(~sku.unv_num_univers.isin([0, 14, 89, 90])) \\\n",
    "    .filter(sku.mdl_num_model_r3.isNotNull()) \\\n",
    "    .filter(sapb.purch_org == purch_org) \\\n",
    "    .filter(sapb.sapsrc == 'PRT') \\\n",
    "    .filter(F.current_timestamp().between(sapb.date_begin, sapb.date_end)) \\\n",
    "    .select(week.wee_id_week.cast('int').alias('week_id'),\n",
    "            week.day_first_day_week.alias('date'),\n",
    "            sku.mdl_num_model_r3.alias('model'),\n",
    "            tdt.f_qty_item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actual_sales_online = dyd \\\n",
    "    .join(day,\n",
    "          on=F.to_date(dyd.tdt_date_to_ordered, 'yyyy-MM-dd') == day.day_id_day,\n",
    "          how='inner') \\\n",
    "    .join(week, \n",
    "          on=day.wee_id_week == week.wee_id_week, \n",
    "          how='inner') \\\n",
    "    .join(sku, \n",
    "          on=dyd.sku_idr_sku == sku.sku_idr_sku, \n",
    "          how='inner') \\\n",
    "    .join(bu, \n",
    "          on=dyd.but_idr_business_unit_economical == bu.but_idr_business_unit, \n",
    "          how='inner') \\\n",
    "    .join(sapb,\n",
    "          on=bu.but_num_business_unit.cast('string') == \\\n",
    "             F.regexp_replace(sapb.plant_id, '^0*|\\s',''),\n",
    "          how='inner') \\\n",
    "    .filter(dyd.the_to_type == 'online') \\\n",
    "    .filter(week.wee_id_week >= first_week_id) \\\n",
    "    .filter(~sku.unv_num_univers.isin([0, 14, 89, 90])) \\\n",
    "    .filter(sku.mdl_num_model_r3.isNotNull()) \\\n",
    "    .filter(sapb.purch_org == purch_org) \\\n",
    "    .filter(sapb.sapsrc == 'PRT') \\\n",
    "    .filter(F.current_timestamp().between(sapb.date_begin, sapb.date_end)) \\\n",
    "    .select(week.wee_id_week.cast('int').alias('week_id'),\n",
    "            week.day_first_day_week.alias('date'),\n",
    "            sku.mdl_num_model_r3.alias('model'),\n",
    "            dyd.f_qty_item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actual_sales = actual_sales_offline.union(actual_sales_online) \\\n",
    "    .groupby(['week_id', 'date', 'model']) \\\n",
    "    .agg(F.sum('f_qty_item').alias('y')) \\\n",
    "    .filter(F.col('y') > 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Lifestage_update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lifestage_update = sdm \\\n",
    "    .join(sku, \n",
    "          on=F.regexp_replace(sdm.material_id, '^0*|\\s','') == \\\n",
    "             sku.mdl_num_model_r3.cast('string'),\n",
    "          how='inner') \\\n",
    "    .filter(sdm.sales_org == sales_org) \\\n",
    "    .filter(sdm.sap_source == 'PRT') \\\n",
    "    .filter(sdm.lifestage != '') \\\n",
    "    .filter(sdm.distrib_channel == '02') \\\n",
    "    .filter(sku.mdl_num_model_r3.isNotNull()) \\\n",
    "    .filter(~sku.unv_num_univers.isin([0, 14, 89, 90])) \\\n",
    "    .filter(F.current_timestamp().between(sku.sku_date_begin, sku.sku_date_end)) \\\n",
    "    .withColumn(\"date_end\",\n",
    "                F.when(sdm.date_end == '2999-12-31',\n",
    "                       F.to_date(F.lit('2100-12-31'), 'yyyy-MM-dd')) \\\n",
    "                       .otherwise(sdm.date_end)) \\\n",
    "    .select(sku.mdl_num_model_r3.alias('model'), \n",
    "            sdm.date_begin,\n",
    "            \"date_end\",\n",
    "            sdm.lifestage.cast('int').alias('lifestage')) \\\n",
    "    .drop_duplicates()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Model_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_info = sku \\\n",
    "    .filter(sku.mdl_num_model_r3.isNotNull()) \\\n",
    "    .filter(~sku.unv_num_univers.isin([0, 14, 89, 90])) \\\n",
    "    .filter(F.current_timestamp().between(sku.sku_date_begin, sku.sku_date_end)) \\\n",
    "    .select(sku.mdl_num_model_r3.alias('model'),\n",
    "            sku.mdl_label.alias('model_label'),\n",
    "            sku.fam_num_family.alias('family'),\n",
    "            sku.family_label.alias('family_label'),\n",
    "            sku.sdp_num_sub_department.alias('sub_department'),\n",
    "            sku.sdp_label.alias('sub_department_label'),\n",
    "            sku.dpt_num_department.alias('department'),\n",
    "            sku.unv_label.alias('department_label'),\n",
    "            sku.unv_num_univers.alias('univers'),\n",
    "            sku.mdl_label.alias('univers_label'),\n",
    "            sku.pnt_num_product_nature.alias('product_nature'),\n",
    "            sku.product_nature_label.alias('product_nature_label'),\n",
    "            sku.category_label.alias('category_label')) \\\n",
    "    .drop_duplicates()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actual_sales.cache()\n",
    "model_info.cache()\n",
    "lifestage_update.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(actual_sales.count())\n",
    "print(lifestage_update.count())\n",
    "print(model_info.count())"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "actual_sales:\n",
    "PROD :\n",
    "DEV  :  10513053\n",
    "SPARK : 10513053\n",
    "\n",
    "lifestage_update:\n",
    "PROD :\n",
    "DEV  :  316270\n",
    "SPARK : 316270\n",
    "\n",
    "model_info :\n",
    "PROD :\n",
    "DEV  :  377747\n",
    "SPARK : 377747"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actual_sales.filter(actual_sales.week_id < 201950).describe().show()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "\t    week_id\t  model\t    y\n",
    "count\t10271814  10271814\t10271814\n",
    "mean\t201722\t  8102081\t406\n",
    "std\t    141\t      1055458\t1946\n",
    "min\t    201501\t  1\t        1\n",
    "max\t    201949\t  8601533\t217484"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_parquet_s3(actual_sales, 'fcst-refined-demand-forecast-dev', 'part_1_1/actual_sales')\n",
    "write_parquet_s3(lifestage_update, 'fcst-refined-demand-forecast-dev', 'part_1_1/lifestage_update')\n",
    "write_parquet_s3(model_info, 'fcst-refined-demand-forecast-dev', 'part_1_1/model_info')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Sparkmagic (PySpark)",
   "language": "",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 2
   },
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
