{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext, SparkConf, StorageLevel\n",
    "from pyspark.sql import SparkSession, Window\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "\n",
    "import pyspark.sql.functions as F\n",
    "import sys\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_parquet_s3(app, bucket, file_path):\n",
    "    \"\"\" \"\"\"\n",
    "    df = app.read.parquet(bucket + file_path)\n",
    "    return df\n",
    "\n",
    "\n",
    "def write_parquet_s3(spark_df, bucket, file_path):\n",
    "    \"\"\" \"\"\"\n",
    "    spark_df.write.parquet(bucket + file_path, mode=\"overwrite\")\n",
    "    \n",
    "\n",
    "def next_week(week_id):\n",
    "    week_id = str(week_id)\n",
    "    y, w = int(week_id[:4]), int(week_id[4:])\n",
    "    \n",
    "    if w == 9:\n",
    "        w = '10'\n",
    "        y = str(y)\n",
    "    elif len(str(w))==1:\n",
    "        w = w + 1\n",
    "        y = str(y)\n",
    "        w = '0' + str(w)\n",
    "    elif w == 52:\n",
    "        w = '01'\n",
    "        y = str(y + 1)\n",
    "    else:\n",
    "        w = str(w + 1)\n",
    "        y = str(y)\n",
    "    n_wk = y + w\n",
    "    return int(n_wk)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bucket_clean = 's3://fcst-clean-prod/'\n",
    "bucket_refine_global = 's3://fcst-refined-demand-forecast-dev/global/'\n",
    "first_week_id = 201501\n",
    "purch_org = 'Z001'\n",
    "sales_org = 'Z002'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shifted_date = datetime.datetime.today() + datetime.timedelta(days=1)\n",
    "current_week_id = int(str(shifted_date.isocalendar()[0])+str(shifted_date.isocalendar()[1]).zfill(2))\n",
    "print(current_week_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load all needed clean data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tdt = read_parquet_s3(spark, bucket_clean, 'f_transaction_detail/*/')\n",
    "dyd = read_parquet_s3(spark, bucket_clean, 'f_delivery_detail/*/')\n",
    "\n",
    "sku = read_parquet_s3(spark, bucket_clean, 'd_sku/')\n",
    "bu = read_parquet_s3(spark, bucket_clean, 'd_business_unit/')\n",
    "\n",
    "sapb = read_parquet_s3(spark, bucket_clean, 'sites_attribut_0plant_branches_h/')\n",
    "sdm = read_parquet_s3(spark, bucket_clean, 'd_sales_data_material_h/')\n",
    "\n",
    "day = read_parquet_s3(spark, bucket_clean, 'd_day/')\n",
    "week = read_parquet_s3(spark, bucket_clean, 'd_week/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Actual_sales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actual_sales_offline = tdt \\\n",
    "    .join(day,\n",
    "          on=F.to_date(tdt.tdt_date_to_ordered, 'yyyy-MM-dd') == day.day_id_day,\n",
    "          how='inner') \\\n",
    "    .join(week, \n",
    "          on=day.wee_id_week == week.wee_id_week, \n",
    "          how='inner') \\\n",
    "    .join(sku,\n",
    "          on=tdt.sku_idr_sku == sku.sku_idr_sku, \n",
    "          how='inner') \\\n",
    "    .join(bu, \n",
    "          on=tdt.but_idr_business_unit == bu.but_idr_business_unit, \n",
    "          how='inner') \\\n",
    "    .join(sapb,\n",
    "          on=bu.but_num_business_unit.cast('string') == \\\n",
    "             F.regexp_replace(sapb.plant_id, '^0*|\\s',''),\n",
    "          how='inner') \\\n",
    "    .filter(tdt.the_to_type == 'offline') \\\n",
    "    .filter(week.wee_id_week >= first_week_id) \\\n",
    "    .filter(week.wee_id_week < current_week_id) \\\n",
    "    .filter(~sku.unv_num_univers.isin([0, 14, 89, 90])) \\\n",
    "    .filter(sku.mdl_num_model_r3.isNotNull()) \\\n",
    "    .filter(sapb.purch_org == purch_org) \\\n",
    "    .filter(sapb.sapsrc == 'PRT') \\\n",
    "    .filter(F.current_timestamp().between(sapb.date_begin, sapb.date_end)) \\\n",
    "    .select(week.wee_id_week.cast('int').alias('week_id'),\n",
    "            week.day_first_day_week.alias('date'),\n",
    "            sku.mdl_num_model_r3.alias('model'),\n",
    "            tdt.f_qty_item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actual_sales_online = dyd \\\n",
    "    .join(day,\n",
    "          on=F.to_date(dyd.tdt_date_to_ordered, 'yyyy-MM-dd') == day.day_id_day,\n",
    "          how='inner') \\\n",
    "    .join(week, \n",
    "          on=day.wee_id_week == week.wee_id_week, \n",
    "          how='inner') \\\n",
    "    .join(sku, \n",
    "          on=dyd.sku_idr_sku == sku.sku_idr_sku, \n",
    "          how='inner') \\\n",
    "    .join(bu, \n",
    "          on=dyd.but_idr_business_unit_economical == bu.but_idr_business_unit, \n",
    "          how='inner') \\\n",
    "    .join(sapb,\n",
    "          on=bu.but_num_business_unit.cast('string') == \\\n",
    "             F.regexp_replace(sapb.plant_id, '^0*|\\s',''),\n",
    "          how='inner') \\\n",
    "    .filter(dyd.the_to_type == 'online') \\\n",
    "    .filter(week.wee_id_week >= first_week_id) \\\n",
    "    .filter(week.wee_id_week < current_week_id) \\\n",
    "    .filter(~sku.unv_num_univers.isin([0, 14, 89, 90])) \\\n",
    "    .filter(sku.mdl_num_model_r3.isNotNull()) \\\n",
    "    .filter(sapb.purch_org == purch_org) \\\n",
    "    .filter(sapb.sapsrc == 'PRT') \\\n",
    "    .filter(F.current_timestamp().between(sapb.date_begin, sapb.date_end)) \\\n",
    "    .select(week.wee_id_week.cast('int').alias('week_id'),\n",
    "            week.day_first_day_week.alias('date'),\n",
    "            sku.mdl_num_model_r3.alias('model'),\n",
    "            dyd.f_qty_item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actual_sales = actual_sales_offline.union(actual_sales_online) \\\n",
    "    .groupby(['week_id', 'date', 'model']) \\\n",
    "    .agg(F.sum('f_qty_item').alias('y')) \\\n",
    "    .filter(F.col('y') > 0) \\\n",
    "    .repartition('model')\n",
    "\n",
    "actual_sales.persist(StorageLevel.MEMORY_ONLY)\n",
    "print(actual_sales.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Lifestage_update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lifestage_update = sdm \\\n",
    "    .join(sku, \n",
    "          on=F.regexp_replace(sdm.material_id, '^0*|\\s','') == \\\n",
    "             sku.mdl_num_model_r3.cast('string'),\n",
    "          how='inner') \\\n",
    "    .filter(sdm.sales_org == sales_org) \\\n",
    "    .filter(sdm.sap_source == 'PRT') \\\n",
    "    .filter(sdm.lifestage != '') \\\n",
    "    .filter(sdm.distrib_channel == '02') \\\n",
    "    .filter(sku.mdl_num_model_r3.isNotNull()) \\\n",
    "    .filter(~sku.unv_num_univers.isin([0, 14, 89, 90])) \\\n",
    "    .filter(F.current_timestamp().between(sku.sku_date_begin, sku.sku_date_end)) \\\n",
    "    .withColumn(\"date_end\",\n",
    "                F.when(sdm.date_end == '2999-12-31',\n",
    "                       F.to_date(F.lit('2100-12-31'), 'yyyy-MM-dd')) \\\n",
    "                       .otherwise(sdm.date_end)) \\\n",
    "    .select(sku.mdl_num_model_r3.alias('model'), \n",
    "            sdm.date_begin,\n",
    "            \"date_end\",\n",
    "            sdm.lifestage.cast('int').alias('lifestage')) \\\n",
    "    .drop_duplicates() \\\n",
    "    .repartition('model')\n",
    "\n",
    "lifestage_update.persist(StorageLevel.MEMORY_ONLY)\n",
    "print(lifestage_update.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Model_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_info = sku \\\n",
    "    .filter(sku.mdl_num_model_r3.isNotNull()) \\\n",
    "    .filter(~sku.unv_num_univers.isin([0, 14, 89, 90])) \\\n",
    "    .filter(F.current_timestamp().between(sku.sku_date_begin, sku.sku_date_end)) \\\n",
    "    .select(sku.mdl_num_model_r3.alias('model'),\n",
    "            sku.mdl_label.alias('model_label'),\n",
    "            sku.fam_num_family.alias('family'),\n",
    "            sku.family_label.alias('family_label'),\n",
    "            sku.sdp_num_sub_department.alias('sub_department'),\n",
    "            sku.sdp_label.alias('sub_department_label'),\n",
    "            sku.dpt_num_department.alias('department'),\n",
    "            sku.unv_label.alias('department_label'),\n",
    "            sku.unv_num_univers.alias('univers'),\n",
    "            sku.unv_label.alias('univers_label'),\n",
    "            sku.pnt_num_product_nature.alias('product_nature'),\n",
    "            sku.product_nature_label.alias('product_nature_label'),\n",
    "            sku.category_label.alias('category_label')) \\\n",
    "    .drop_duplicates() \\\n",
    "    .repartition('model')\n",
    "\n",
    "model_info.persist(StorageLevel.MEMORY_ONLY)\n",
    "print(model_info.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Format life stages values\n",
    "/!\\ life stage values are only historized since September 10, 2018\n",
    "\n",
    "##### 1) Keep only usefull life stage values: models in actual sales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(lifestage_update.count())\n",
    "\n",
    "lifestage_update = lifestage_update.join(actual_sales.select('model').drop_duplicates(), \n",
    "                                         on='model', how='inner')\n",
    "\n",
    "#print(lifestage_update.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2) Life stage updates ==> Life stage by week"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculates all possible date/model combinations associated with a life stage update\n",
    "min_date = lifestage_update.select(F.min('date_begin')).collect()[0][0]\n",
    "\n",
    "all_lifestage_date = actual_sales \\\n",
    "    .filter(actual_sales.date >= min_date) \\\n",
    "    .select('date') \\\n",
    "    .drop_duplicates() \\\n",
    "    .orderBy('date')\n",
    "\n",
    "all_lifestage_model = lifestage_update.select('model').drop_duplicates().orderBy('model')\n",
    "\n",
    "date_model = all_lifestage_date.crossJoin(all_lifestage_model)\n",
    "\n",
    "# Calculate lifestage by date\n",
    "model_lifestage = date_model.join(lifestage_update, on='model', how='left')\n",
    "model_lifestage = model_lifestage \\\n",
    "    .filter((model_lifestage.date >= model_lifestage.date_begin) &\n",
    "            (model_lifestage.date <= model_lifestage.date_end)) \\\n",
    "    .drop('date_begin', 'date_end')\n",
    "\n",
    "# The previous filter removes combinations that do not match the update dates.\n",
    "# But sometimes the update dates do not cover all periods, \n",
    "# which causes some dates to disappear, even during the model's activity periods.\n",
    "# To avoid this problem, we must merge again with all combinations to be sure \n",
    "# not to lose anything.\n",
    "model_lifestage = date_model.join(model_lifestage, on=['date', 'model'], how='left')\n",
    "\n",
    "#print(model_lifestage.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3) Deal with duplicates rows & NA\n",
    "- If we have several life stage information for the same model and date, then we take the minimum\n",
    "- If no life stage is filled in, we take the last known value (if exists)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_lifestage = model_lifestage \\\n",
    "    .groupby(['date', 'model']) \\\n",
    "    .agg(F.min('lifestage').alias('lifestage'))\n",
    "\n",
    "# This is a ffil by group in pyspark ==> OMG\n",
    "window = Window.partitionBy('model')\\\n",
    "               .orderBy('date')\\\n",
    "               .rowsBetween(-sys.maxsize, 0)\n",
    "\n",
    "ffilled_lifestage = F.last(model_lifestage['lifestage'], ignorenulls=True).over(window)\n",
    "\n",
    "model_lifestage = model_lifestage.withColumn('lifestage', ffilled_lifestage)\n",
    "\n",
    "#print(model_lifestage.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 4) Deal with zombie models\n",
    "If the life stage changes from active (1) to inactive (2 or more) and then back active, we consider the model is a zombie.  \n",
    "This new life may have a different sales behaviour than the previous one, so it's better to pretend that the latter is the only one that never existed.  \n",
    "\n",
    "For example, if the life stage looks like this: **1 1 1 3 3 3 1 1 1**, we only keep that: **3 1 1 1**.  \n",
    "Note that we still keep the last inactive value (here 3) before life stages 1 in order to avoid that the model life stages are considered incomplete in the following session."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(model_lifestage.count())\n",
    "\n",
    "model_lifestage = model_lifestage \\\n",
    "    .withColumn('lifestage_shift', \n",
    "                F.lag(model_lifestage['lifestage']) \\\n",
    "                      .over(Window.partitionBy(\"model\").orderBy(F.desc('date'))))\n",
    "\n",
    "model_lifestage = model_lifestage \\\n",
    "    .withColumn('diff_shift', model_lifestage['lifestage'] - \\\n",
    "                              model_lifestage['lifestage_shift'])\n",
    "\n",
    "df_cut_date = model_lifestage.filter(model_lifestage.diff_shift > 0)\n",
    "\n",
    "df_cut_date = df_cut_date \\\n",
    "    .groupBy('model') \\\n",
    "    .agg(F.max('date').alias('cut_date'))\n",
    "\n",
    "model_lifestage = model_lifestage.join(df_cut_date, on=['model'], how='left')\n",
    "\n",
    "# if no cut_date, fill by an old one\n",
    "model_lifestage = model_lifestage \\\n",
    "    .withColumn('cut_date', F.when(F.col('cut_date').isNull(),\n",
    "                                   F.to_date(F.lit('1993-04-15'), 'yyyy-MM-dd')) \\\n",
    "                                   .otherwise(F.col('cut_date')))\n",
    "\n",
    "model_lifestage = model_lifestage \\\n",
    "    .filter(model_lifestage.date >= model_lifestage.cut_date) \\\n",
    "    .select(['date', 'model', 'lifestage'])\n",
    "\n",
    "model_lifestage.persist(StorageLevel.MEMORY_ONLY)\n",
    "print(model_lifestage.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Match sales and life stages & rebuild incomplete life stages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1) Complete sales\n",
    "- Fill missing quantities by 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculates all possible date/model combinations from actual sales\n",
    "all_sales_model = actual_sales.select('model').orderBy('model').drop_duplicates()\n",
    "all_sales_date = actual_sales.select('date').orderBy('date').drop_duplicates()\n",
    "\n",
    "date_model = all_sales_model.crossJoin(all_sales_date)\n",
    "\n",
    "# Add corresponding week id\n",
    "date_model = date_model.join(actual_sales.select(['date', 'week_id']).drop_duplicates(), \n",
    "                             on=['date'], how='inner')\n",
    "\n",
    "# Add actual sales\n",
    "complete_ts = date_model.join(actual_sales, on=['date', 'model', 'week_id'], how='left')\n",
    "complete_ts = complete_ts.select(actual_sales.columns)\n",
    "\n",
    "# Fill NaN (no sales recorded) by 0\n",
    "complete_ts = complete_ts.fillna(0, subset=['y'])\n",
    "\n",
    "#print(complete_ts.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2) Add model life stage by week"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "complete_ts = complete_ts.join(model_lifestage, ['date', 'model'], how='left')\n",
    "\n",
    "complete_ts.persist(StorageLevel.MEMORY_ONLY)\n",
    "print(complete_ts.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3) Rebuild incomplete life stages\n",
    "/!\\ Reminder: the life stage values are only historized since September 10, 2018\n",
    "- If the life stage value is 1 at the first historized date \n",
    "- And we observe sales in the previous and consecutive weeks\n",
    "- Then we fill the life stage values of these weeks with 1 as well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_column_index(df, col_name): \n",
    "    new_schema = StructType(df.schema.fields + [StructField(col_name, LongType(), False),])\n",
    "    return df.rdd.zipWithIndex().map(lambda row: row[0] + (row[1], )).toDF(schema=new_schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(complete_ts.filter(complete_ts.lifestage == 1).count())\n",
    "\n",
    "# find models respecting the first condition\n",
    "w = Window.partitionBy('model').orderBy('date')\n",
    "\n",
    "first_lifestage = complete_ts.filter(complete_ts.lifestage.isNotNull()) \\\n",
    "                             .withColumn('rn', F.row_number().over(w))\n",
    "\n",
    "first_lifestage = first_lifestage.filter(first_lifestage.rn == 1).drop('rn')\n",
    "\n",
    "\n",
    "first_lifestage = first_lifestage \\\n",
    "    .filter(first_lifestage.lifestage == 1) \\\n",
    "    .select(first_lifestage.model, \n",
    "            first_lifestage.date.alias('first_lifestage_date'))\n",
    "\n",
    "# Create the mask (rows to be completed) for theses models\n",
    "complete_ts = add_column_index(complete_ts, 'idx') # save original indexes\n",
    "complete_ts.cache()\n",
    "\n",
    "mask = complete_ts\n",
    "\n",
    "# keep only models respecting the first condition\n",
    "mask = mask.join(first_lifestage, on='model', how='inner')\n",
    "\n",
    "# Look only before the first historized lifestage date\n",
    "mask = mask.filter(mask.date <= mask.first_lifestage_date)\n",
    "\n",
    "w = Window.partitionBy('model').orderBy(F.desc('date'))\n",
    "\n",
    "mask = mask \\\n",
    "    .withColumn('cumsum_y', F.sum('y').over(w)) \\\n",
    "    .withColumn('lag_cumsum_y', F.lag('cumsum_y').over(w)) \\\n",
    "    .fillna(0, subset=['lag_cumsum_y']) \\\n",
    "    .withColumn('is_active', F.col('cumsum_y') > F.col('lag_cumsum_y'))\n",
    "\n",
    "ts_start_date = mask \\\n",
    "    .filter(mask.is_active == False) \\\n",
    "    .withColumn('rn', F.row_number().over(w)) \\\n",
    "    .filter(F.col('rn') == 1) \\\n",
    "    .select('model', F.col('date').alias('start_date'))\n",
    "\n",
    "mask = mask.join(ts_start_date, on='model', how='left')\n",
    "\n",
    "# Case model start date unknown (older than first week recorded here)\n",
    "# ==> fill by an old date\n",
    "mask = mask \\\n",
    "    .withColumn('start_date', F.when(F.col('start_date').isNull(),\n",
    "                                     F.to_date(F.lit('1993-04-15'), 'yyyy-MM-dd')) \\\n",
    "                                     .otherwise(F.col('start_date'))) \\\n",
    "    .withColumn('is_model_start', F.col('date') > F.col('start_date')) \\\n",
    "    .withColumn('to_fill', F.col('is_active') & \\\n",
    "                           F.col('is_model_start') & \\\n",
    "                           F.col('lifestage').isNull())\n",
    "\n",
    "mask = mask.filter(mask.to_fill == True).select(['idx', 'to_fill'])\n",
    "\n",
    "# Fill the eligible rows under all conditions\n",
    "complete_ts = complete_ts.join(mask, on='idx', how='left')\n",
    "complete_ts = complete_ts \\\n",
    "    .withColumn('lifestage', \n",
    "                F.when(F.col('to_fill') == True, F.lit(1)).otherwise(F.col('lifestage')))\n",
    "\n",
    "complete_ts = complete_ts.select(['week_id', 'date', 'model', 'y', 'lifestage'])\n",
    "\n",
    "complete_ts.persist(StorageLevel.MEMORY_ONLY)\n",
    "print(complete_ts.count())\n",
    "\n",
    "#print(complete_ts.filter(complete_ts.lifestage == 1).count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create active sales data set\n",
    "\n",
    "##### 1) Keep in memory first sales dates by model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = Window.partitionBy('model').orderBy('date')\n",
    "\n",
    "model_start_date = actual_sales.withColumn('rn', F.row_number().over(w))\n",
    "\n",
    "model_start_date = model_start_date \\\n",
    "    .filter(model_start_date.rn == 1) \\\n",
    "    .drop('rn', 'week_id', 'y') \\\n",
    "    .select(F.col(\"model\"), F.col(\"date\").alias(\"first_date\"))\n",
    "\n",
    "#print(model_start_date.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2) Construct active sales\n",
    "- Filtered on active life stage \n",
    "- After the first actual sales date\n",
    "- Padded with zeros (already done in complete sales)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(active_sales.count())\n",
    "\n",
    "active_sales = complete_ts \\\n",
    "    .filter(complete_ts.lifestage == 1) \\\n",
    "    .join(model_start_date, on='model', how='inner') \\\n",
    "    .filter(complete_ts.date >= model_start_date.first_date) \\\n",
    "    .drop('lifestage', 'first_date')\n",
    "                           \n",
    "active_sales.persist(StorageLevel.MEMORY_ONLY)\n",
    "print(active_sales.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean model info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_info = model_info \\\n",
    "    .withColumn('category_label', \n",
    "                F.when(model_info.category_label == 'SOUS RAYON POUB', F.lit(None)) \\\n",
    "                .otherwise(model_info.category_label)) \\\n",
    "    .fillna('UNKNOWN')\n",
    "\n",
    "# Due to a discrepant seasonal behaviour between LOW SOCKS and HIGH SOCKS, we chose to split\n",
    "# the product nature 'SOCKS' into two different product natures 'LOW SOCKS' and 'HIGH SOCKS'\n",
    "model_info = model_info \\\n",
    "    .withColumn('product_nature_label', \n",
    "                F.when((model_info.product_nature_label == 'SOCKS') & \\\n",
    "                       (model_info.model_label.contains(' LOW')), \n",
    "                       F.lit('LOW SOCKS')) \\\n",
    "                 .when((model_info.product_nature_label == 'SOCKS') & \\\n",
    "                       (model_info.model_label.contains(' MID')), \n",
    "                       F.lit('MID SOCKS')) \\\n",
    "                 .when((model_info.product_nature_label == 'SOCKS') & \\\n",
    "                       (model_info.model_label.contains(' HIGH')), \n",
    "                       F.lit('HIGH SOCKS')) \\\n",
    "                 .otherwise(model_info.product_nature_label)) \\\n",
    "    .drop('product_nature')\n",
    "    \n",
    "indexer = StringIndexer(inputCol='product_nature_label', outputCol='product_nature')\n",
    "model_info = indexer \\\n",
    "    .fit(model_info) \\\n",
    "    .transform(model_info) \\\n",
    "    .withColumn('product_nature', F.col('product_nature').cast('integer'))\n",
    "\n",
    "model_info.persist(StorageLevel.MEMORY_ONLY)\n",
    "print(model_info.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unit tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check duplicates rows\n",
    "assert active_sales.groupBy(['date', 'model']).count().select(F.max(\"count\")).collect()[0][0] == 1\n",
    "assert model_info.count() == model_info.select('model').drop_duplicates().count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_parquet_s3(model_info, 'fcst-refined-demand-forecast-dev', 'part_1/model_info')\n",
    "write_parquet_s3(actual_sales, 'fcst-refined-demand-forecast-dev', 'part_1/actual_sales')\n",
    "write_parquet_s3(active_sales, 'fcst-refined-demand-forecast-dev', 'part_1/active_sales')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Sparkmagic (PySpark)",
   "language": "",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 3
   },
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
