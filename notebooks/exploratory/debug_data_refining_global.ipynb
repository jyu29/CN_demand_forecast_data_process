{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%configure -f\n",
    "{\n",
    " \"driverMemory\": \"5G\",\n",
    " \"conf\" : \n",
    "    {\"spark.memory.fraction\" : 0.2,\n",
    "     \"spark.memory.storageFraction\" : 0.8,\n",
    "     \"spark.dynamicAllocation.enabled\" : \"true\",\n",
    "     \"spark.maximizeResourceAllocation\" : \"true\",\n",
    "     \"spark.dynamicAllocation.minExecutors\" : 2,\n",
    "     \"spark.dynamicAllocation.maxExecutors\" : 50,\n",
    "     \"spark.executor.extraJavaOptions\" : \"-verbose:gc -XX:+PrintGCDetails -XX:+PrintGCDateStamps -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:MaxHeapFreeRatio=70 -XX:+CMSClassUnloadingEnabled -XX:OnOutOfMemoryError='kill -9 %p'\",\n",
    "     \"spark.driver.extraJavaOptions\" : \"-XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:MaxHeapFreeRatio=70 -XX:+CMSClassUnloadingEnabled -XX:OnOutOfMemoryError='kill -9 %p'\"\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext, SparkConf, StorageLevel\n",
    "from pyspark.sql import SparkSession, Window\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "\n",
    "import pyspark.sql.functions as F\n",
    "import sys\n",
    "import time\n",
    "import os\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Utils\n",
    "def read_parquet_s3(app, bucket, file_path):\n",
    "    \"\"\" \"\"\"\n",
    "    df = app.read.parquet(bucket + file_path)\n",
    "    return df\n",
    "\n",
    "def write_parquet_s3(spark_df, bucket, file_path):\n",
    "    \"\"\" \"\"\"\n",
    "    spark_df.write.parquet(bucket + file_path, mode=\"overwrite\")\n",
    "    \n",
    "def get_current_week_id():\n",
    "    shifted_date = datetime.today() + timedelta(days=1)\n",
    "    current_week_id = int(str(shifted_date.isocalendar()[0]) + str(shifted_date.isocalendar()[1]).zfill(2))\n",
    "    return current_week_id\n",
    "\n",
    "def get_timer(starting_time):\n",
    "    end = time.time()\n",
    "    minutes, seconds = divmod(int(end - starting_time), 60)\n",
    "    print(\"{} minute(s) {} second(s)\".format(int(minutes), seconds))\n",
    "    \n",
    "def get_next_week_id(week_id):\n",
    "    '''\n",
    "    ARGUMENTS:\n",
    "    \n",
    "    date ( integer ): week identifier in the format 'year'+'week_number'\n",
    "    \n",
    "    RETURNS:\n",
    "    \n",
    "    next week in the same format as the date argument\n",
    "    '''\n",
    "    if not (isinstance(week_id, (int, np.integer))):\n",
    "        return 'DATE ARGUMENT NOT AN INT'\n",
    "    if len(str(week_id)) != 6:\n",
    "        return 'UNVALID DATE FORMAT'\n",
    "\n",
    "    year = week_id // 100\n",
    "    week = week_id % 100\n",
    "\n",
    "    if week < 52:\n",
    "        return week_id + 1\n",
    "    elif week == 52:\n",
    "        last_week = isoweek.Week.last_week_of_year(year).week\n",
    "        if last_week == 52:\n",
    "            return (week_id // 100 + 1) * 100 + 1\n",
    "        elif last_week == 53:\n",
    "            return week_id + 1\n",
    "        else:\n",
    "            return 'UNVALID ISOWEEK.LASTWEEK NUMBER'\n",
    "    elif week == 53:\n",
    "        if isoweek.Week.last_week_of_year(year).week == 52:\n",
    "            return 'UNVALID WEEK NUMBER'\n",
    "        else:\n",
    "            return (date // 100 + 1) * 100 + 1\n",
    "    else:\n",
    "        return 'UNVALID DATE'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Configs \n",
    "apply_the_sanity_check = False\n",
    "\n",
    "bucket_clean = 's3://fcst-clean-prod/'\n",
    "bucket_refine_global = 's3://fcst-refined-demand-forecast-dev/global/'\n",
    "first_week_id = 201501\n",
    "percentage_of_critical_decrease = -30\n",
    "purch_org = 'Z001'\n",
    "sales_org = 'Z002'\n",
    "\n",
    "current_week_id = get_current_week_id()\n",
    "print(\"Current week id:\", current_week_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load all needed clean data\n",
    "tdt = read_parquet_s3(spark, bucket_clean, 'f_transaction_detail/*/')\n",
    "dyd = read_parquet_s3(spark, bucket_clean, 'f_delivery_detail/*/')\n",
    "\n",
    "sku = read_parquet_s3(spark, bucket_clean, 'd_sku/')\n",
    "bu = read_parquet_s3(spark, bucket_clean, 'd_business_unit/')\n",
    "\n",
    "sapb = read_parquet_s3(spark, bucket_clean, 'sites_attribut_0plant_branches_h/')\n",
    "sdm = read_parquet_s3(spark, bucket_clean, 'd_sales_data_material_h/')\n",
    "\n",
    "day = read_parquet_s3(spark, bucket_clean, 'd_day/')\n",
    "week = read_parquet_s3(spark, bucket_clean, 'd_week/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create Actual_Sales\n",
    "actual_sales_offline = tdt \\\n",
    "    .join(day,\n",
    "          on=F.to_date(tdt.tdt_date_to_ordered, 'yyyy-MM-dd') == day.day_id_day,\n",
    "          how='inner') \\\n",
    "    .join(week,\n",
    "          on=day.wee_id_week == week.wee_id_week,\n",
    "          how='inner') \\\n",
    "    .join(sku,\n",
    "          on=tdt.sku_idr_sku == sku.sku_idr_sku,\n",
    "          how='inner') \\\n",
    "    .join(bu,\n",
    "          on=tdt.but_idr_business_unit == bu.but_idr_business_unit,\n",
    "          how='inner') \\\n",
    "    .join(sapb,\n",
    "          on=bu.but_num_business_unit.cast('string') == \\\n",
    "             F.regexp_replace(sapb.plant_id, '^0*|\\s', ''),\n",
    "          how='inner') \\\n",
    "    .filter(tdt.the_to_type == 'offline') \\\n",
    "    .filter(week.wee_id_week >= first_week_id) \\\n",
    "    .filter(week.wee_id_week < current_week_id) \\\n",
    "    .filter(~sku.unv_num_univers.isin([0, 14, 89, 90])) \\\n",
    "    .filter(sku.mdl_num_model_r3.isNotNull()) \\\n",
    "    .filter(sapb.purch_org == purch_org) \\\n",
    "    .filter(sapb.sapsrc == 'PRT') \\\n",
    "    .filter(F.current_timestamp().between(sapb.date_begin, sapb.date_end)) \\\n",
    "    .select(week.wee_id_week.cast('int').alias('week_id'),\n",
    "            week.day_first_day_week.alias('date'),\n",
    "            sku.mdl_num_model_r3.alias('model'),\n",
    "            tdt.f_qty_item)\n",
    "\n",
    "actual_sales_online = dyd \\\n",
    "    .join(day,\n",
    "          on=F.to_date(dyd.tdt_date_to_ordered, 'yyyy-MM-dd') == day.day_id_day,\n",
    "          how='inner') \\\n",
    "    .join(week,\n",
    "          on=day.wee_id_week == week.wee_id_week,\n",
    "          how='inner') \\\n",
    "    .join(sku,\n",
    "          on=dyd.sku_idr_sku == sku.sku_idr_sku,\n",
    "          how='inner') \\\n",
    "    .join(bu,\n",
    "          on=dyd.but_idr_business_unit_economical == bu.but_idr_business_unit,\n",
    "          how='inner') \\\n",
    "    .join(sapb,\n",
    "          on=bu.but_num_business_unit.cast('string') == \\\n",
    "             F.regexp_replace(sapb.plant_id, '^0*|\\s', ''),\n",
    "          how='inner') \\\n",
    "    .filter(dyd.the_to_type == 'online') \\\n",
    "    .filter(week.wee_id_week >= first_week_id) \\\n",
    "    .filter(week.wee_id_week < current_week_id) \\\n",
    "    .filter(~sku.unv_num_univers.isin([0, 14, 89, 90])) \\\n",
    "    .filter(sku.mdl_num_model_r3.isNotNull()) \\\n",
    "    .filter(sapb.purch_org == purch_org) \\\n",
    "    .filter(sapb.sapsrc == 'PRT') \\\n",
    "    .filter(F.current_timestamp().between(sapb.date_begin, sapb.date_end)) \\\n",
    "    .select(week.wee_id_week.cast('int').alias('week_id'),\n",
    "            week.day_first_day_week.alias('date'),\n",
    "            sku.mdl_num_model_r3.alias('model'),\n",
    "            dyd.f_qty_item)\n",
    "\n",
    "actual_sales = actual_sales_offline.union(actual_sales_online) \\\n",
    "    .groupby(['week_id', 'date', 'model']) \\\n",
    "    .agg(F.sum('f_qty_item').alias('y')) \\\n",
    "    .filter(F.col('y') > 0) \\\n",
    "    .repartition('model')\n",
    "\n",
    "actual_sales.persist(StorageLevel.MEMORY_ONLY)\n",
    "print(\"====> Counting(cache) [actual_sales] took \")\n",
    "start = time.time()\n",
    "actual_sales_count = actual_sales.count()\n",
    "get_timer(starting_time=start)\n",
    "print(\"actual_sales length:\", actual_sales_count)\n",
    "\n",
    "print(\"====> Collecting [max_week_id] took \")\n",
    "start = time.time()\n",
    "max_week_id = actual_sales.select(F.max('week_id')).collect()[0][0]\n",
    "get_timer(starting_time=start)\n",
    "print(\"max week id in actual_sales:\", max_week_id)\n",
    "\n",
    "\n",
    "assert actual_sales_count > 0\n",
    "assert get_next_week_id(max_week_id) == current_week_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sanity check for DLIGHT DATA Ingestion (Do we have some abnormal decrease of sales\n",
    "# for a specific  week ?)\n",
    "print(\">>> Sanity check for DLIGHT DATA Ingestion (Do we have some abnormal decrease of sales for a specific  week ? )\"\n",
    "      \"\\nIt took...\")\n",
    "start = time.time()\n",
    "print(\"*** The threshold percentage of critical decrease is: {}%\".format(percentage_of_critical_decrease))\n",
    "\n",
    "# Defining  'window_partition' variable which will allow us to calculate 'lag' values.\n",
    "sanity_check_df = actual_sales \\\n",
    "    .withColumn('window_partition', F.lit(1)) \\\n",
    "    .select('window_partition', 'week_id', 'y')\n",
    "\n",
    "# Total sales per week.\n",
    "sanity_check_df = sanity_check_df \\\n",
    "    .groupby(['window_partition', 'week_id']) \\\n",
    "    .agg(F.sum('y').alias('y'))\n",
    "\n",
    "# 4 'lag' values for each week.\n",
    "w = Window().partitionBy(\"window_partition\").orderBy(F.asc(\"week_id\"))\n",
    "sanity_check_df = sanity_check_df \\\n",
    "    .withColumn('lag1',\n",
    "                F.lag(sanity_check_df.y, count=1, default=0).over(w)) \\\n",
    "    .withColumn('lag2',\n",
    "                F.lag(sanity_check_df.y, count=2, default=0).over(w)) \\\n",
    "    .withColumn('lag3',\n",
    "                F.lag(sanity_check_df.y, count=3, default=0).over(w)) \\\n",
    "    .withColumn('lag4',\n",
    "                F.lag(sanity_check_df.y, count=4, default=0).over(w))\n",
    "\n",
    "# Keep only weeks with the 4 complete 'lag' values.\n",
    "sanity_check_df = sanity_check_df \\\n",
    "    .filter(sanity_check_df.lag4 > 0)\n",
    "\n",
    "# For each week, compute the mean values of the 4 'lag'.\n",
    "sanity_check_df = sanity_check_df \\\n",
    "    .withColumn('mean_lag',\n",
    "                (F.col(\"lag1\") + F.col(\"lag2\") + F.col(\"lag3\") + F.col(\"lag4\")) / 4)\n",
    "sanity_check_df = sanity_check_df \\\n",
    "    .withColumn('evolution', ((F.col('y') - F.col('mean_lag')) / F.col('mean_lag')) * 100)\n",
    "\n",
    "# Keeping only negative evolution rates.\n",
    "sanity_check_df = sanity_check_df \\\n",
    "    .filter(sanity_check_df.evolution < 0)\n",
    "sanity_check_df.describe(['evolution']).show()\n",
    "\n",
    "# Getting the minimum of the negative evolution rates.\n",
    "min_evolution = sanity_check_df.select(F.min('evolution')).collect()[0][0]\n",
    "\n",
    "# Show the week for which the evolution rates is\n",
    "# the minimum.\n",
    "sanity_check_df.filter(sanity_check_df.evolution == min_evolution).drop('window_partition').show()\n",
    "\n",
    "print(\"*** Writing sanity check table [data_sanity_check]\")\n",
    "write_parquet_s3(sanity_check_df.withColumn(\"execution_day\", F.current_timestamp()), bucket_refine_global,\n",
    "                    'sanity_check_df')\n",
    "get_timer(starting_time=start)\n",
    "\n",
    "if apply_the_sanity_check:\n",
    "    print(\"************* Activate the application of DLIGHT data sanity-check *************\")\n",
    "    assert min_evolution > percentage_of_critical_decrease, \"There is an abnormal decreasing of data !\"\n",
    "else:\n",
    "    print(\"************ Deactivate the application of DLIGHT data sanity-check ************\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create Lifestage_Update\n",
    "lifestage_update = sdm \\\n",
    "    .join(sku,\n",
    "          on=F.regexp_replace(sdm.material_id, '^0*|\\s', '') == \\\n",
    "             sku.mdl_num_model_r3.cast('string'),\n",
    "          how='inner') \\\n",
    "    .filter(sdm.sales_org == sales_org) \\\n",
    "    .filter(sdm.sap_source == 'PRT') \\\n",
    "    .filter(sdm.lifestage != '') \\\n",
    "    .filter(sdm.distrib_channel == '02') \\\n",
    "    .filter(sku.mdl_num_model_r3.isNotNull()) \\\n",
    "    .filter(~sku.unv_num_univers.isin([0, 14, 89, 90])) \\\n",
    "    .filter(F.current_timestamp().between(sku.sku_date_begin, sku.sku_date_end)) \\\n",
    "    .withColumn(\"date_end\",\n",
    "                F.when(sdm.date_end == '2999-12-31',\n",
    "                       F.to_date(F.lit('2100-12-31'), 'yyyy-MM-dd')) \\\n",
    "                .otherwise(sdm.date_end)) \\\n",
    "    .select(sku.mdl_num_model_r3.alias('model'),\n",
    "            sdm.date_begin,\n",
    "            \"date_end\",\n",
    "            sdm.lifestage.cast('int').alias('lifestage')) \\\n",
    "    .drop_duplicates() \\\n",
    "    .repartition('model')\n",
    "\n",
    "lifestage_update.persist(StorageLevel.MEMORY_ONLY)\n",
    "print(\"====> Counting(cache) [lifestage_update] took \")\n",
    "start = time.time()\n",
    "lifestage_update_count = lifestage_update.count()\n",
    "get_timer(starting_time=start)\n",
    "print(\"lifestage_update length:\", lifestage_update_count)\n",
    "assert lifestage_update_count > 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create Model_Info\n",
    "model_info = sku \\\n",
    "    .filter(sku.mdl_num_model_r3.isNotNull()) \\\n",
    "    .filter(~sku.unv_num_univers.isin([0, 14, 89, 90])) \\\n",
    "    .filter(F.current_timestamp().between(sku.sku_date_begin, sku.sku_date_end)) \\\n",
    "    .select(sku.mdl_num_model_r3.alias('model'),\n",
    "            sku.mdl_label.alias('model_label'),\n",
    "            sku.fam_num_family.alias('family'),\n",
    "            sku.family_label.alias('family_label'),\n",
    "            sku.sdp_num_sub_department.alias('sub_department'),\n",
    "            sku.sdp_label.alias('sub_department_label'),\n",
    "            sku.dpt_num_department.alias('department'),\n",
    "            sku.unv_label.alias('department_label'),\n",
    "            sku.unv_num_univers.alias('univers'),\n",
    "            sku.unv_label.alias('univers_label'),\n",
    "            sku.pnt_num_product_nature.alias('product_nature'),\n",
    "            sku.product_nature_label.alias('product_nature_label'),\n",
    "            sku.category_label.alias('category_label')) \\\n",
    "    .drop_duplicates() \\\n",
    "    .repartition('model')\n",
    "\n",
    "model_info.persist(StorageLevel.MEMORY_ONLY)\n",
    "print(\"====> Counting(cache) [model_info] (1st time) took \")\n",
    "start = time.time()\n",
    "model_info_count = model_info.count()\n",
    "get_timer(starting_time=start)\n",
    "print(\"model_info length:\", model_info_count)\n",
    "assert model_info_count > 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep only usefull life stage values: models in actual sales\n",
    "lifestage_update = lifestage_update.join(actual_sales.select('model').drop_duplicates(),\n",
    "                                         on='model', how='inner')\n",
    "\n",
    "# Calculates all possible date/model combinations associated with a life stage update\n",
    "min_date = lifestage_update.select(F.min('date_begin')).collect()[0][0]\n",
    "\n",
    "all_lifestage_date = actual_sales \\\n",
    "    .filter(actual_sales.date >= min_date) \\\n",
    "    .select('date') \\\n",
    "    .drop_duplicates() \\\n",
    "    .orderBy('date')\n",
    "\n",
    "all_lifestage_model = lifestage_update.select('model').drop_duplicates().orderBy('model')\n",
    "\n",
    "date_model = all_lifestage_date.crossJoin(all_lifestage_model)\n",
    "\n",
    "# Calculate lifestage by date\n",
    "model_lifestage = date_model.join(lifestage_update, on='model', how='left')\n",
    "model_lifestage = model_lifestage \\\n",
    "    .filter((model_lifestage.date >= model_lifestage.date_begin) &\n",
    "            (model_lifestage.date <= model_lifestage.date_end)) \\\n",
    "    .drop('date_begin', 'date_end')\n",
    "\n",
    "# The previous filter removes combinations that do not match the update dates.\n",
    "# But sometimes the update dates do not cover all periods, \n",
    "# which causes some dates to disappear, even during the model's activity periods.\n",
    "# To avoid this problem, we must merge again with all combinations to be sure \n",
    "# not to lose anything.\n",
    "model_lifestage = date_model.join(model_lifestage, on=['date', 'model'], how='left')\n",
    "\n",
    "model_lifestage = model_lifestage \\\n",
    "    .groupby(['date', 'model']) \\\n",
    "    .agg(F.min('lifestage').alias('lifestage'))\n",
    "\n",
    "# This is a ffil by group in pyspark\n",
    "window = Window \\\n",
    "    .partitionBy('model') \\\n",
    "    .orderBy('date') \\\n",
    "    .rowsBetween(-sys.maxsize, 0)\n",
    "\n",
    "ffilled_lifestage = F.last(model_lifestage['lifestage'], ignorenulls=True).over(window)\n",
    "\n",
    "model_lifestage = model_lifestage.withColumn('lifestage', ffilled_lifestage)\n",
    "\n",
    "model_lifestage = model_lifestage \\\n",
    "    .withColumn('lifestage_shift',\n",
    "                F.lag(model_lifestage['lifestage']) \\\n",
    "                .over(Window.partitionBy(\"model\").orderBy(F.desc('date'))))\n",
    "\n",
    "model_lifestage = model_lifestage \\\n",
    "    .withColumn('diff_shift', model_lifestage['lifestage'] - \\\n",
    "                model_lifestage['lifestage_shift'])\n",
    "\n",
    "df_cut_date = model_lifestage.filter(model_lifestage.diff_shift > 0)\n",
    "\n",
    "df_cut_date = df_cut_date \\\n",
    "    .groupBy('model') \\\n",
    "    .agg(F.max('date').alias('cut_date'))\n",
    "\n",
    "model_lifestage = model_lifestage.join(df_cut_date, on=['model'], how='left')\n",
    "\n",
    "# if no cut_date, fill by an old one\n",
    "model_lifestage = model_lifestage \\\n",
    "    .withColumn('cut_date', F.when(F.col('cut_date').isNull(),\n",
    "                                   F.to_date(F.lit('1993-04-15'), 'yyyy-MM-dd')) \\\n",
    "                .otherwise(F.col('cut_date')))\n",
    "\n",
    "model_lifestage = model_lifestage \\\n",
    "    .filter(model_lifestage.date >= model_lifestage.cut_date) \\\n",
    "    .select(['date', 'model', 'lifestage'])\n",
    "\n",
    "model_lifestage.persist(StorageLevel.MEMORY_ONLY)\n",
    "print(\"====> Counting(cache) [model_lifestage] took \")\n",
    "start = time.time()\n",
    "model_lifestage_count = model_lifestage.count()\n",
    "get_timer(starting_time=start)\n",
    "print(\"model_lifestage length:\", model_lifestage_count)\n",
    "assert model_lifestage_count > 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculates all possible date/model combinations from actual sales\n",
    "all_sales_model = actual_sales.select('model').orderBy('model').drop_duplicates()\n",
    "all_sales_date = actual_sales.select('date').orderBy('date').drop_duplicates()\n",
    "\n",
    "date_model = all_sales_model.crossJoin(all_sales_date)\n",
    "\n",
    "# Add corresponding week id\n",
    "date_model = date_model.join(actual_sales.select(['date', 'week_id']).drop_duplicates(),\n",
    "                             on=['date'], how='inner')\n",
    "\n",
    "# Add actual sales\n",
    "complete_ts = date_model.join(actual_sales, on=['date', 'model', 'week_id'], how='left')\n",
    "complete_ts = complete_ts.select(actual_sales.columns)\n",
    "\n",
    "# Fill NaN (no sales recorded) by 0\n",
    "complete_ts = complete_ts.fillna(0, subset=['y'])\n",
    "\n",
    "complete_ts = complete_ts.join(model_lifestage, ['date', 'model'], how='left')\n",
    "\n",
    "complete_ts.persist(StorageLevel.MEMORY_ONLY)\n",
    "print(\"====> Counting(cache) [complete_ts] (1st time) took \")\n",
    "start = time.time()\n",
    "complete_ts_count = complete_ts.count()\n",
    "get_timer(starting_time=start)\n",
    "\n",
    "print(\"complete_ts length:\", complete_ts_count)\n",
    "assert complete_ts_count > 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_column_index(df, col_name):\n",
    "    new_schema = StructType(df.schema.fields + [StructField(col_name, LongType(), False), ])\n",
    "    return df.rdd.zipWithIndex().map(lambda row: row[0] + (row[1],)).toDF(schema=new_schema)\n",
    "\n",
    "\n",
    "# find models respecting the first condition\n",
    "w = Window.partitionBy('model').orderBy('date')\n",
    "\n",
    "first_lifestage = complete_ts.filter(complete_ts.lifestage.isNotNull()) \\\n",
    "    .withColumn('rn', F.row_number().over(w))\n",
    "\n",
    "first_lifestage = first_lifestage.filter(first_lifestage.rn == 1).drop('rn')\n",
    "\n",
    "first_lifestage = first_lifestage \\\n",
    "    .filter(first_lifestage.lifestage == 1) \\\n",
    "    .select(first_lifestage.model,\n",
    "            first_lifestage.date.alias('first_lifestage_date'))\n",
    "\n",
    "# Create the mask (rows to be completed) for theses models\n",
    "complete_ts = add_column_index(complete_ts, 'idx')  # save original indexes\n",
    "complete_ts.cache()\n",
    "\n",
    "mask = complete_ts\n",
    "\n",
    "# keep only models respecting the first condition\n",
    "mask = mask.join(first_lifestage, on='model', how='inner')\n",
    "\n",
    "# Look only before the first historized lifestage date\n",
    "mask = mask.filter(mask.date <= mask.first_lifestage_date)\n",
    "\n",
    "w = Window.partitionBy('model').orderBy(F.desc('date'))\n",
    "\n",
    "mask = mask \\\n",
    "    .withColumn('cumsum_y', F.sum('y').over(w)) \\\n",
    "    .withColumn('lag_cumsum_y', F.lag('cumsum_y').over(w)) \\\n",
    "    .fillna(0, subset=['lag_cumsum_y']) \\\n",
    "    .withColumn('is_active', F.col('cumsum_y') > F.col('lag_cumsum_y'))\n",
    "\n",
    "ts_start_date = mask \\\n",
    "    .filter(mask.is_active == False) \\\n",
    "    .withColumn('rn', F.row_number().over(w)) \\\n",
    "    .filter(F.col('rn') == 1) \\\n",
    "    .select('model', F.col('date').alias('start_date'))\n",
    "\n",
    "mask = mask.join(ts_start_date, on='model', how='left')\n",
    "\n",
    "# Case model start date unknown (older than first week recorded here)\n",
    "# ==> fill by an old date\n",
    "mask = mask \\\n",
    "    .withColumn('start_date', F.when(F.col('start_date').isNull(),\n",
    "                                     F.to_date(F.lit('1993-04-15'), 'yyyy-MM-dd')) \\\n",
    "                .otherwise(F.col('start_date'))) \\\n",
    "    .withColumn('is_model_start', F.col('date') > F.col('start_date')) \\\n",
    "    .withColumn('to_fill', F.col('is_active') & \\\n",
    "                F.col('is_model_start') & \\\n",
    "                F.col('lifestage').isNull())\n",
    "\n",
    "mask = mask.filter(mask.to_fill == True).select(['idx', 'to_fill'])\n",
    "\n",
    "# Fill the eligible rows under all conditions\n",
    "complete_ts = complete_ts.join(mask, on='idx', how='left')\n",
    "complete_ts = complete_ts \\\n",
    "    .withColumn('lifestage',\n",
    "                F.when(F.col('to_fill') == True, F.lit(1)).otherwise(F.col('lifestage')))\n",
    "\n",
    "complete_ts = complete_ts.select(['week_id', 'date', 'model', 'y', 'lifestage'])\n",
    "\n",
    "complete_ts.persist(StorageLevel.MEMORY_ONLY)\n",
    "print(\"====> Counting(cache) [complete_ts] (2nd time) took \")\n",
    "start = time.time()\n",
    "complete_ts_count = complete_ts.count()\n",
    "get_timer(starting_time=start)\n",
    "print(\"complete_ts length:\", complete_ts_count)\n",
    "assert complete_ts_count > 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = Window.partitionBy('model').orderBy('date')\n",
    "\n",
    "model_start_date = actual_sales.withColumn('rn', F.row_number().over(w))\n",
    "\n",
    "model_start_date = model_start_date \\\n",
    "    .filter(model_start_date.rn == 1) \\\n",
    "    .drop('rn', 'week_id', 'y') \\\n",
    "    .select(F.col(\"model\"), F.col(\"date\").alias(\"first_date\"))\n",
    "\n",
    "active_sales = complete_ts \\\n",
    "    .filter(complete_ts.lifestage == 1) \\\n",
    "    .join(model_start_date, on='model', how='inner') \\\n",
    "    .filter(complete_ts.date >= model_start_date.first_date) \\\n",
    "    .drop('lifestage', 'first_date') \\\n",
    "    .orderBy(['model', 'week_id'])\n",
    "\n",
    "active_sales.persist(StorageLevel.MEMORY_ONLY)\n",
    "print(\"====> Counting(cache) active_sales took \")\n",
    "start = time.time()\n",
    "active_sales_count = active_sales.count()\n",
    "get_timer(starting_time=start)\n",
    "\n",
    "print(\"active_sales length:\", active_sales_count)\n",
    "assert active_sales_count > 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_info = model_info \\\n",
    "    .withColumn('category_label',\n",
    "                F.when(model_info.category_label == 'SOUS RAYON POUB', F.lit(None)) \\\n",
    "                .otherwise(model_info.category_label)) \\\n",
    "    .fillna('UNKNOWN')\n",
    "\n",
    "# Due to a discrepant seasonal behaviour between LOW SOCKS and HIGH SOCKS, we chose to split\n",
    "# the product nature 'SOCKS' into two different product natures 'LOW SOCKS' and 'HIGH SOCKS'\n",
    "model_info = model_info \\\n",
    "    .withColumn('product_nature_label',\n",
    "                F.when((model_info.product_nature_label == 'SOCKS') & \\\n",
    "                       (model_info.model_label.contains(' LOW')),\n",
    "                       F.lit('LOW SOCKS')) \\\n",
    "                .when((model_info.product_nature_label == 'SOCKS') & \\\n",
    "                      (model_info.model_label.contains(' MID')),\n",
    "                      F.lit('MID SOCKS')) \\\n",
    "                .when((model_info.product_nature_label == 'SOCKS') & \\\n",
    "                      (model_info.model_label.contains(' HIGH')),\n",
    "                      F.lit('HIGH SOCKS')) \\\n",
    "                .otherwise(model_info.product_nature_label)) \\\n",
    "    .drop('product_nature')\n",
    "\n",
    "indexer = StringIndexer(inputCol='product_nature_label', outputCol='product_nature')\n",
    "\n",
    "model_info = indexer \\\n",
    "    .fit(model_info) \\\n",
    "    .transform(model_info) \\\n",
    "    .withColumn('product_nature', F.col('product_nature').cast('integer')) \\\n",
    "    .orderBy('model')\n",
    "\n",
    "model_info.persist(StorageLevel.MEMORY_ONLY)\n",
    "print(\"====> Counting(cache) [model_info] (2nd time) took \")\n",
    "start = time.time()\n",
    "model_info_count = model_info.count()\n",
    "get_timer(starting_time=start)\n",
    "print(\"model_info length:\", model_info_count)\n",
    "assert model_info_count > 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check duplicates rows\n",
    "assert active_sales.groupBy(['date', 'model']).count().select(F.max(\"count\")).collect()[0][0] == 1\n",
    "assert model_info.count() == model_info.select('model').drop_duplicates().count()\n",
    "\n",
    "# Write\n",
    "print(\"writing tables...\")\n",
    "\n",
    "print(\"====> Writing table [model_info]\")\n",
    "start = time.time()\n",
    "write_parquet_s3(model_info, bucket_refine_global, 'model_info')\n",
    "get_timer(starting_time=start)\n",
    "\n",
    "print(\"====> Writing table [actual_sales]\")\n",
    "start = time.time()\n",
    "write_parquet_s3(actual_sales, bucket_refine_global, 'actual_sales')\n",
    "get_timer(starting_time=start)\n",
    "\n",
    "print(\"====> Writing table [active_sales]\")\n",
    "start = time.time()\n",
    "write_parquet_s3(active_sales, bucket_refine_global, 'active_sales')\n",
    "get_timer(starting_time=start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Sparkmagic (PySpark)",
   "language": "",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 3
   },
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
