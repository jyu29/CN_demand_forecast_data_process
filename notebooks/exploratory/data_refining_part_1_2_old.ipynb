{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import SparkSession, Window\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, LongType, DateType \n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "\n",
    "import numpy as np\n",
    "import sys\n",
    "import datetime\n",
    "from datetime import datetime, timedelta\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .appName(\"cleaning\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "spark.version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data from refining part 1_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actual_sales_schema = StructType([\n",
    "    StructField('week_id', IntegerType()),\n",
    "    StructField('date', DateType()),\n",
    "    StructField('model', IntegerType()),\n",
    "    StructField('y', IntegerType())])\n",
    "\n",
    "actual_sales = spark.read.csv('s3://fcst-workspace/qlik/data/raw/actual_sales.csv.gz',\n",
    "                              schema=actual_sales_schema, sep='|', header=True)\n",
    "actual_sales.cache()\n",
    "actual_sales.printSchema()\n",
    "actual_sales.show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lifestage_update_schema = StructType([\n",
    "    StructField('model', IntegerType()),\n",
    "    StructField('sku', IntegerType()),\n",
    "    StructField('date_begin', DateType()),\n",
    "    StructField('date_end', DateType()),\n",
    "    StructField('lifestage', IntegerType())])\n",
    "\n",
    "lifestage_update = spark.read.csv('s3://fcst-workspace/qlik/data/raw/lifestage_update.csv.gz',\n",
    "                                  schema=lifestage_update_schema, sep='|', header=True)\n",
    "lifestage_update.cache()\n",
    "lifestage_update.printSchema()\n",
    "lifestage_update.show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_info = spark.read.csv('s3://fcst-workspace/qlik/data/raw/model_info.csv.gz',\n",
    "                            inferSchema=True, sep='|', header=True)\n",
    "model_info.cache()\n",
    "model_info.printSchema()\n",
    "model_info.show(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Delete incomplete weeks ?\n",
    "- To be sure to have complete weeks (Sunday --> Saturday) regardless raw data extraction date, the first and last week of sales are deleted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# actual_sales.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_max_week_id = actual_sales.select(F.min('week_id').alias('min'), \n",
    "                                      F.max('week_id').alias('max'))\n",
    "\n",
    "actual_sales = actual_sales \\\n",
    "    .join(min_max_week_id, \n",
    "          on=(actual_sales.week_id > min_max_week_id.min) & (actual_sales.week_id < min_max_week_id.max),\n",
    "          how='inner') \\\n",
    "    .select('week_id', 'date', 'model', 'y') \\\n",
    "    .orderBy('model', 'date') \n",
    "\n",
    "actual_sales.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# actual_sales.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Format life stages values\n",
    "/!\\ life stage values are only historized since September 10, 2018\n",
    "\n",
    "##### 1) Keep only usefull life stage values: models in actual sales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lifestage_update.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lifestage_update = lifestage_update.join(actual_sales.select('model').drop_duplicates(), \n",
    "                                         on='model', how='inner')\n",
    "\n",
    "lifestage_update.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lifestage_update.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2) Sku life stage updates ==> sku life stage by week"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculates all possible date/sku combinations associated with a life stage update\n",
    "min_date = lifestage_update.select(F.min('date_begin')).collect()[0][0]\n",
    "\n",
    "all_lifestage_date = actual_sales.filter(actual_sales.date >= min_date)\\\n",
    "                                 .select('date') \\\n",
    "                                 .drop_duplicates() \\\n",
    "                                 .orderBy('date')\n",
    "\n",
    "all_lifestage_sku = lifestage_update.select('sku').drop_duplicates().orderBy('sku')\n",
    "\n",
    "\n",
    "date_sku = all_lifestage_date.crossJoin(all_lifestage_sku)\n",
    "\n",
    "# Add corresponding models\n",
    "date_sku = date_sku.join(lifestage_update.select('sku', 'model').drop_duplicates(), \n",
    "                         on='sku', \n",
    "                         how='inner')\n",
    "\n",
    "# Calculate lifestage by date\n",
    "sku_lifestage = date_sku.join(lifestage_update, on=['model', 'sku'], how='left')\n",
    "sku_lifestage = sku_lifestage \\\n",
    "    .filter((sku_lifestage.date >= sku_lifestage.date_begin) &\n",
    "            (sku_lifestage.date <= sku_lifestage.date_end)) \\\n",
    "    .drop('date_begin', 'date_end')\n",
    "\n",
    "# The previous filter removes combinations that do not match the update dates.\n",
    "# But sometimes the update dates do not cover all periods, \n",
    "# which causes some dates to disappear, even during the model's activity periods.\n",
    "# To avoid this problem, we must merge again with all combinations to be sure \n",
    "# not to lose anything.\n",
    "sku_lifestage = date_sku.join(sku_lifestage, on=['date', 'model', 'sku'], how='left')\n",
    "\n",
    "sku_lifestage.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sku_lifestage.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3) Sku life stage ==> model life stage\n",
    "- In order to aggregate at the model level, we decided to take the minimum life stage value of the SKUs that compose it\n",
    "- If no life stage is filled in, we take the last known value (if exists)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_lifestage = sku_lifestage \\\n",
    "    .groupby(['date', 'model']) \\\n",
    "    .agg(F.min('lifestage').alias('lifestage'))\n",
    "\n",
    "model_lifestage.cache()\n",
    "\n",
    "# This is a ffil by group in pyspark ==> OMG\n",
    "window = Window.partitionBy('model')\\\n",
    "               .orderBy('date')\\\n",
    "               .rowsBetween(-sys.maxsize, 0)\n",
    "\n",
    "ffilled_lifestage = F.last(model_lifestage['lifestage'], ignorenulls=True).over(window)\n",
    "\n",
    "model_lifestage = model_lifestage.withColumn('lifestage', ffilled_lifestage)\n",
    "\n",
    "model_lifestage.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_lifestage.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 4) Deal with zombie models\n",
    "If the life stage changes from active (1) to inactive (2 or more) and then back active, we consider the model is a zombie.  \n",
    "This new life may have a different sales behaviour than the previous one, so it's better to pretend that the latter is the only one that never existed.  \n",
    "\n",
    "For example, if the life stage looks like this: **1 1 1 3 3 3 1 1 1**, we only keep that: **3 1 1 1**.  \n",
    "Note that we still keep the last inactive value (here 3) before life stages 1 in order to avoid that the model life stages are considered incomplete in the following session."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print('NB row before: ', model_lifestage.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_lifestage = model_lifestage \\\n",
    "    .withColumn('lifestage_shift', \n",
    "                F.lag(model_lifestage['lifestage']).over(Window.partitionBy(\"model\").orderBy(F.desc('date'))))\n",
    "\n",
    "model_lifestage = model_lifestage \\\n",
    "    .withColumn('diff_shift', model_lifestage['lifestage'] - model_lifestage['lifestage_shift'])\n",
    "\n",
    "df_cut_date = model_lifestage.filter(model_lifestage.diff_shift > 0)\n",
    "\n",
    "df_cut_date = df_cut_date \\\n",
    "    .groupBy('model') \\\n",
    "    .agg(F.max('date').alias('cut_date'))\n",
    "\n",
    "model_lifestage = model_lifestage.join(df_cut_date, on=['model'], how='left')\n",
    "\n",
    "# if no cut_date, fill by an old one\n",
    "model_lifestage = model_lifestage \\\n",
    "    .withColumn('cut_date', F.when(F.col('cut_date').isNull(),\n",
    "                                   F.to_date(F.lit('1993-04-15'), 'yyyy-MM-dd')) \\\n",
    "                                   .otherwise(F.col('cut_date')))\n",
    "\n",
    "model_lifestage = model_lifestage \\\n",
    "    .filter(model_lifestage.date >= model_lifestage.cut_date) \\\n",
    "    .select(['date', 'model', 'lifestage'])\n",
    "\n",
    "model_lifestage.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print('NB row after: ', model_lifestage.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Match sales and life stages & rebuild incomplete life stages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1) Complete sales\n",
    "- Fill missing quantities by 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculates all possible date/model combinations from actual sales\n",
    "all_sales_model = actual_sales.select('model').orderBy('model').drop_duplicates()\n",
    "all_sales_date = actual_sales.select('date').orderBy('date').drop_duplicates()\n",
    "\n",
    "date_model = all_sales_model.crossJoin(all_sales_date)\n",
    "\n",
    "# Add corresponding week id\n",
    "date_model = date_model.join(actual_sales.select(['date', 'week_id']).drop_duplicates(), on=['date'], how='inner')\n",
    "\n",
    "# Add actual sales\n",
    "complete_ts = date_model.join(actual_sales, on=['date', 'model', 'week_id'], how='left')\n",
    "complete_ts = complete_ts.select(actual_sales.columns)\n",
    "\n",
    "# Fill NaN (no sales recorded) by 0\n",
    "complete_ts = complete_ts.fillna(0, subset=['y'])\n",
    "complete_ts.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# complete_ts.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2) Add model life stage by week"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "complete_ts = complete_ts.join(model_lifestage, ['date', 'model'], how='left')\n",
    "complete_ts.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# complete_ts.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3) Rebuild incomplete life stages\n",
    "/!\\ Reminder: the life stage values are only historized since September 10, 2018\n",
    "- If the life stage value is 1 at the first historized date \n",
    "- And we observe sales in the previous and consecutive weeks\n",
    "- Then we fill the life stage values of these weeks with 1 as well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_column_index(df, col_name): \n",
    "    new_schema = StructType(df.schema.fields + [StructField(col_name, LongType(), False),])\n",
    "    return df.rdd.zipWithIndex().map(lambda row: row[0] + (row[1], )).toDF(schema=new_schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find models respecting the first condition\n",
    "w = Window.partitionBy('model').orderBy(F.col('date').asc())\n",
    "\n",
    "first_lifestage = complete_ts.filter(complete_ts.lifestage.isNotNull()) \\\n",
    "                             .withColumn('rn', F.row_number().over(w))\n",
    "\n",
    "first_lifestage = first_lifestage.where(first_lifestage.rn == 1).drop('rn')\n",
    "\n",
    "\n",
    "first_lifestage = first_lifestage \\\n",
    "    .filter(first_lifestage.lifestage == 1) \\\n",
    "    .select(first_lifestage.model, \n",
    "            first_lifestage.date.alias('first_lifestage_date'))\n",
    "\n",
    "# Create the mask (rows to be completed) for theses models\n",
    "complete_ts = add_column_index(complete_ts, 'idx') # save original indexes\n",
    "complete_ts.cache()\n",
    "\n",
    "mask = complete_ts\n",
    "\n",
    "# keep only models respecting the first condition\n",
    "mask = mask.join(first_lifestage, on='model', how='inner')\n",
    "\n",
    "# Look only before the first historized lifestage date\n",
    "mask = mask.filter(mask.date <= mask.first_lifestage_date)\n",
    "\n",
    "mask = mask \\\n",
    "    .withColumn('cumsum_y', F.sum('y').over(Window.partitionBy('model').orderBy(F.col('date').desc()))) \\\n",
    "    .withColumn('lag_cumsum_y', F.lag('cumsum_y').over(Window.partitionBy('model').orderBy(F.col('date').desc()))) \\\n",
    "    .fillna(0, subset=['lag_cumsum_y']) \\\n",
    "    .withColumn('is_active', F.col('cumsum_y') > F.col('lag_cumsum_y'))\n",
    "\n",
    "ts_start_date = mask \\\n",
    "    .filter(mask.is_active == False) \\\n",
    "    .groupBy('model') \\\n",
    "    .agg(F.first('date').alias('start_date'))\n",
    "\n",
    "mask = mask.join(ts_start_date, on='model', how='left')\n",
    "\n",
    "# Case model start date unknown (older than first week recorded here)\n",
    "# ==> fill by an old date\n",
    "mask = mask \\\n",
    "    .withColumn('start_date', F.when(F.col('start_date').isNull(),\n",
    "                                     F.to_date(F.lit('1993-04-15'), 'yyyy-MM-dd')) \\\n",
    "                                     .otherwise(F.col('start_date'))) \\\n",
    "    .withColumn('is_model_start', F.col('date') > F.col('start_date')) \\\n",
    "    .withColumn('to_fill', F.col('is_active') & F.col('is_model_start') & F.col('lifestage').isNull())\n",
    "\n",
    "\n",
    "mask = mask.filter(mask.to_fill == True).select(['idx', 'to_fill'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill the eligible rows under all conditions\n",
    "complete_ts = complete_ts.join(mask, on='idx', how='left')\n",
    "complete_ts = complete_ts.withColumn('lifestage', \n",
    "                                     F.when(F.col('to_fill') == True, F.lit(1)).otherwise(F.col('lifestage')))\n",
    "complete_ts.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "complete_ts = complete_ts.select([complete_ts.week_id, complete_ts.date, complete_ts.model, complete_ts.y, complete_ts.lifestage])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# complete_ts.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lifestage1 : wtf weahhhhhhh!!!!\n",
    "# python: 2778965\n",
    "# spark:  2778965"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## -----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create active sales data set\n",
    "\n",
    "##### 1) Keep in memory first sales dates by model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_start_date = actual_sales.groupBy('model').agg(F.first(actual_sales['date']).alias('first_date'))\n",
    "                                       \n",
    "model_start_date.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = Window.partitionBy('model').orderBy(F.col('date').asc())\n",
    "\n",
    "model_start_date = actual_sales.withColumn('rn', F.row_number().over(w))\n",
    "\n",
    "model_start_date = model_start_date.filter(model_start_date.rn == 1).drop('rn', 'week_id', 'y')\n",
    "\n",
    "model_start_date = model_start_date.selectExpr('date as first_date', 'model as model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_start_date.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2) Construct active sales\n",
    "- Filtered on active life stage \n",
    "- After the first actual sales date\n",
    "- Padded with zeros (already done in complete sales)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Nb rows before:', complete_ts.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "active_sales = complete_ts.filter(complete_ts.lifestage == 1)\n",
    "\n",
    "active_sales = active_sales.join(model_start_date,'model' , how='inner')\\\n",
    "                           \n",
    "active_sales = active_sales.filter(active_sales.date >= active_sales.first_date)\\\n",
    "                           .drop('lifestage', 'first_date', 'to_fill')\n",
    "\n",
    "active_sales.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Nb rows after: ', active_sales.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean model info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_info = model_info.withColumn('category_label', F.when(model_info.category_label == 'SOUS RAYON POUB', F.lit(None)).otherwise(model_info.category_label))\n",
    "model_info = model_info.fillna('UNKNOWN')\n",
    "model_info.select(model_info.category_label).distinct().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_info = model_info.withColumn('product_nature_label', F.when((model_info.product_nature_label == 'SOCKS') & (model_info.model_label.contains('% LOW')), F.lit('LOW SOCKS'))\n",
    "                                                            .when((model_info.product_nature_label == 'SOCKS') & (model_info.model_label.contains('% MID')), F.lit('MID SOCKS'))\n",
    "                                                            .when((model_info.product_nature_label == 'SOCKS') & (model_info.model_label.contains('% HIGH')), F.lit('HIGH SOCKS'))\n",
    "                                                            .otherwise(model_info.product_nature_label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_info = model_info.drop('product_nature')\n",
    "indexer = StringIndexer(inputCol='product_nature_label', outputCol='product_nature')\n",
    "model_info = indexer.fit(model_info).transform(model_info)\n",
    "\n",
    "model_info = model_info.withColumn('product_nature', F.expr('product_nature+1').cast('integer'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_info.write.parquet('s3://fcst-refined-demand-forecast-dev/part_1_2/model_info', mode=\"overwrite\")\n",
    "actual_sales.write.parquet('s3://fcst-refined-demand-forecast-dev/part_1_2/actual_sales', mode=\"overwrite\")\n",
    "active_sales.write.parquet('s3://fcst-refined-demand-forecast-dev/part_1_2/active_sales', mode=\"overwrite\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = spark.read.csv('s3://fcst-workspace/qlik/data/clean/actual_sales.csv',\n",
    "                              schema=actual_sales_schema, sep='|', header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Sparkmagic (PySpark)",
   "language": "",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 2
   },
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
