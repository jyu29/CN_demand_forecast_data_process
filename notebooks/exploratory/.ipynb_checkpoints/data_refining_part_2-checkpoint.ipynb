{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import SparkSession, Window\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DateType \n",
    "import pyspark.sql.functions as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .appName(\"data_refining_part_2\") \\\n",
    "    .getOrCreate()\n",
    "spark.version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load raw data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actual_sales_schema = StructType([\n",
    "    StructField('week_id', IntegerType()),\n",
    "    StructField('date', DateType()),\n",
    "    StructField('model', IntegerType()),\n",
    "    StructField('y', IntegerType())])\n",
    "\n",
    "actual_sales = spark.read.csv('s3://fcst-workspace/qlik/data/raw/actual_sales.csv.gz',\n",
    "                              schema=actual_sales_schema, sep='|', header=True)\n",
    "actual_sales.cache()\n",
    "actual_sales.printSchema()\n",
    "actual_sales.show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lifestage_update_schema = StructType([\n",
    "    StructField('model', IntegerType()),\n",
    "    StructField('sku', IntegerType()),\n",
    "    StructField('date_begin', DateType()),\n",
    "    StructField('date_end', DateType()),\n",
    "    StructField('lifestage', IntegerType())])\n",
    "\n",
    "lifestage_update = spark.read.csv('s3://fcst-workspace/qlik/data/raw/lifestage_update.csv.gz',\n",
    "                                  schema=lifestage_update_schema, sep='|', header=True)\n",
    "lifestage_update.cache()\n",
    "lifestage_update.printSchema()\n",
    "lifestage_update.show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_info = spark.read.csv('s3://fcst-workspace/qlik/data/raw/model_info.csv.gz',\n",
    "                            inferSchema=True, sep='|', header=True)\n",
    "model_info.cache()\n",
    "model_info.printSchema()\n",
    "model_info.show(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Delete incomplete weeks ==> A gÃ©rer dans la partie 1 ?\n",
    "- To be sure to have complete weeks (Sunday --> Saturday) regardless raw data extraction date, the first and last week of sales are deleted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_max_week_id = actual_sales.select(F.min('week_id').alias('min'), \n",
    "                                      F.max('week_id').alias('max'))\n",
    "\n",
    "actual_sales = actual_sales \\\n",
    "    .join(min_max_week_id, \n",
    "          on=(actual_sales.week_id > min_max_week_id.min) & (actual_sales.week_id < min_max_week_id.max),\n",
    "          how='inner') \\\n",
    "    .select('week_id', 'date', 'model', 'y') \\\n",
    "    .orderBy('model', 'date')\n",
    "\n",
    "actual_sales.cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Format life stages values\n",
    "/!\\ life stage values are only historized since September 10, 2018\n",
    "\n",
    "##### 1) Keep only usefull life stage values: models in actual sales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lifestage_update = lifestage_update.join(actual_sales.select('model').drop_duplicates(), \n",
    "                                         on='model', how='inner')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2) Sku life stage updates ==> sku life stage by week"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculates all possible date/sku combinations associated with a life stage update\n",
    "first_lifestage_date = lifestage_update.select(F.min('date_begin').alias('first_date'))\n",
    "\n",
    "all_lifestage_date = actual_sales \\\n",
    "    .join(first_lifestage_date,\n",
    "          on=actual_sales.date >= first_lifestage_date.first_date,\n",
    "          how='inner') \\\n",
    "    .select('date') \\\n",
    "    .drop_duplicates() \\\n",
    "    .orderBy('date')\n",
    "\n",
    "all_lifestage_sku = lifestage_update.select('sku').drop_duplicates().orderBy('sku')\n",
    "\n",
    "date_sku = all_lifestage_date.crossJoin(all_lifestage_sku)\n",
    "\n",
    "# Add corresponding models\n",
    "date_sku = date_sku.join(lifestage_update.select('sku', 'model').drop_duplicates(), \n",
    "                         on='sku', \n",
    "                         how='inner')\n",
    "\n",
    "# Calculate lifestage by date\n",
    "sku_lifestage = date_sku.join(lifestage_update, on=['model', 'sku'], how='left')\n",
    "sku_lifestage = sku_lifestage \\\n",
    "    .filter((sku_lifestage.date >= sku_lifestage.date_begin) &\n",
    "            (sku_lifestage.date <= sku_lifestage.date_end)) \\\n",
    "    .drop('date_begin', 'date_end')\n",
    "\n",
    "# The previous filter removes combinations that do not match the update dates.\n",
    "# But sometimes the update dates do not cover all periods, \n",
    "# which causes some dates to disappear, even during the model's activity periods.\n",
    "# To avoid this problem, we must merge again with all combinations to be sure \n",
    "# not to lose anything.\n",
    "sku_lifestage = date_sku.join(sku_lifestage, on=['date', 'model', 'sku'], how='left')\n",
    "\n",
    "sku_lifestage.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3) Sku life stage ==> model life stage\n",
    "- In order to aggregate at the model level, we decided to take the minimum life stage value of the SKUs that compose it\n",
    "- If no life stage is filled in, we take the last known value (if exists)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_lifestage = sku_lifestage \\\n",
    "    .groupby(['date', 'model']) \\\n",
    "    .agg(F.min('lifestage').alias('lifestage'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print('NB Null before:', model_lifestage.count() - model_lifestage.na.drop().count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is a ffil by group in pyspark ==> OMG\n",
    "window = Window.partitionBy('model')\\\n",
    "               .orderBy('date')\\\n",
    "               .rowsBetween(-sys.maxsize, 0)\n",
    "\n",
    "ffilled_lifestage = F.last(model_lifestage['lifestage'], ignorenulls=True).over(window)\n",
    "\n",
    "model_lifestage = model_lifestage.withColumn('lifestage', ffilled_lifestage)\n",
    "\n",
    "model_lifestage.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print('NB Null after:', model_lifestage.count() - model_lifestage.na.drop().count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 4) Deal with zombie models ==> TO DO\n",
    "If the life stage changes from active (1) to inactive (2 or more) and then back active, we consider the model is a zombie.  \n",
    "This new life may have a different sales behaviour than the previous one, so it's better to pretend that the latter is the only one that never existed.  \n",
    "\n",
    "For example, if the life stage looks like this: **1 1 1 3 3 3 1 1 1**, we only keep that: **3 1 1 1**.  \n",
    "Note that we still keep the last inactive value (here 3) before life stages 1 in order to avoid that the model life stages are considered incomplete in the following session."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('NB row before: ', model_lifestage.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_lifestage = model_lifestage.withColumn('lifestage_shift', F.lag(model_lifestage['lifestage']).over(Window.partitionBy(\"model\").orderBy(F.desc('date'))))\n",
    "\n",
    "model_lifestage = model_lifestage.withColumn('diff_shift', model_lifestage['lifestage'] - model_lifestage['lifestage_shift'])\n",
    "\n",
    "df_cut_date = model_lifestage.filter(F.col('diff_shift')>0)\n",
    "\n",
    "df_cut_date = df_cut_date.groupBy('model')\\\n",
    "                         .agg(F.max('date').alias('cut_date'))\n",
    "\n",
    "model_lifestage = model_lifestage.join(df_cut_date, on=['model'], how='left')\n",
    "\n",
    "ml = ml.withColumn('cut_date',F.when(F.col('cut_date').isNull(),F.to_date(F.lit('1993-04-15'),'yyyy-MM-dd')).otherwise(F.col('cut_date')))\n",
    "\n",
    "model_lifestage = ml.where(ml.date >= ml.cut_date)\\\n",
    "                    .select(['date', 'model', 'lifestage'])\n",
    "\n",
    "# model_lifestage.orderBy(['model', 'date', 'lifestage'], ascending=True).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('NB row after: ', model_lifestage.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Sparkmagic (PySpark)",
   "language": "",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 2
   },
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
