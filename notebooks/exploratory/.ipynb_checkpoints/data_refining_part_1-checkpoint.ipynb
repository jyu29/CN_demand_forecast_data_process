{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import boto3\n",
    "\n",
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DateType \n",
    "from pyspark.sql.functions import col\n",
    "from datetime import date, datetime, timedelta\n",
    "from pyspark.sql.functions import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .appName(\"sql\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read(app, path_s3):\n",
    "    \n",
    "    # Function's signature for printing purpose\n",
    "    func_signature = \"\\n------> Using function: read_parquet\"\n",
    "    print(\"{:<32}\".format(func_signature) + '\\n')    \n",
    "    \n",
    "    df = app.read.parquet(path_s3)\n",
    "    path_sinature = \"\\n------> Read from: \" + path_s3\n",
    "    print(\"{:<32}\".format(path_sinature) + '\\n')    \n",
    "    \n",
    "    # df.limit(1).show()\n",
    "    # df.printSchema()\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "def export_parquet(spark_df, bucket_name, table_path):\n",
    "    \"\"\"Writing spark Dataframe into s3 as a parquet file.\n",
    "    \n",
    "    Parameters:\n",
    "    df (pyspark.sql.dataframe): the spark Dataframe.\n",
    "    bucket_name (string): the s3 bucket name.\n",
    "    table_path (string) : the table name or directory.\n",
    "    \n",
    "    Returns:\n",
    "    \"\"\"\n",
    "    # Function's signature for printing purpose\n",
    "    func_signature = \"\\n------> Using function: export_parquet\"\n",
    "    print(\"{:<32}\".format(func_signature) + '\\n')\n",
    "    \n",
    "    s3_writing_path = 's3://{}/{}'.format(bucket_name, table_path)\n",
    "    print(\"\\t| Writing parquet to path: {}\".format(s3_writing_path))\n",
    "    \n",
    "    spark_df.write.parquet(s3_writing_path, mode=\"overwrite\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_f_transaction_detail = read(spark, 's3://fcst-clean-dev/f_transaction_detail/201912/')\n",
    "df_f_delivery_detail = read(spark, 's3://fcst-clean-dev/f_delivery_detail/201912/')\n",
    "\n",
    "df_d_sku = read(spark, 's3://fcst-clean-dev/d_sku/')\n",
    "df_d_business_unit = read(spark, 's3://fcst-clean-dev/d_business_unit/')\n",
    "df_d_day = read(spark, 's3://fcst-clean-dev/d_day/')\n",
    "site = read(spark, 's3://fcst-clean-dev/sites_attribut_0plant_branches_h/')\n",
    "sales_data_material = read(spark, 's3://fcst-clean-dev/d_sales_data_material_h/')\n",
    "df_d_week = read(spark, 's3://fcst-clean-dev/d_week/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Actual_sales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "site = site.withColumn('but_num', site['plant_id'].cast('int'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bu_europe = df_d_business_unit.join(site, [(site.but_num == df_d_business_unit.but_num_business_unit)], how='inner')\\\n",
    "                       .where(df_d_business_unit.but_num_typ_but == 7) \\\n",
    "                       .where(site.purch_org == 'Z001')\\\n",
    "                       .where(site.sapsrc == 'PRT')\\\n",
    "                       .select(df_d_business_unit.but_idr_business_unit, df_d_business_unit.but_num_business_unit, site.but_num)\\\n",
    "                       .distinct()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sku = df_d_sku.filter(~col('unv_num_univers').isin([0, 14, 89, 90])) \\\n",
    "              .select(['mdl_num_model_r3', 'sku_idr_sku']) \\\n",
    "              .distinct()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "offline_turnover_detail = df_f_transaction_detail.join(sku, [(sku.sku_idr_sku == df_f_transaction_detail.sku_idr_sku) ], how='inner') \\\n",
    "                                                 .join(bu_europe, [(bu_europe.but_idr_business_unit == df_f_transaction_detail.but_idr_business_unit) ], how='inner')  \\\n",
    "                                                 .join(df_d_day, [(to_date(df_d_day.day_id_day,'yyyy-MM-dd') == to_date(df_f_transaction_detail.tdt_date_to_ordered,'yyyy-MM-dd')) ], how='inner') \\\n",
    "                                                 .where(df_f_transaction_detail.the_to_type == 'offline')\\\n",
    "                                                 .select(df_d_day.wee_id_week.alias('week_id'), sku.mdl_num_model_r3.alias('model'), df_f_transaction_detail.f_qty_item.cast('int').alias('f_qty_item_off'))\\\n",
    "                                                 .where(df_d_day.wee_id_week >= '201911') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "offline_turnover_detail.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# agg offline\n",
    "\n",
    "agg_offline = offline_turnover_detail.groupBy(['week_id', 'model'])\\\n",
    "                                     .agg(sum('f_qty_item_off').alias('y_off'))\n",
    "\n",
    "agg_offline.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# online_turnover need delevery detail\n",
    "\n",
    "online_turnover_detail = df_f_delivery_detail.join(sku, [(sku.sku_idr_sku == df_f_delivery_detail.sku_idr_sku) ], how='inner') \\\n",
    "                                             .join(bu_europe, [(bu_europe.but_idr_business_unit == df_f_delivery_detail.but_idr_business_unit_economical) ], how='inner')  \\\n",
    "                                             .join(df_d_day, [(to_date(df_d_day.day_id_day,'yyyy-MM-dd') == to_date(df_f_delivery_detail.tdt_date_to_ordered,'yyyy-MM-dd')) ], how='inner') \\\n",
    "                                             .where(df_f_delivery_detail.the_to_type == 'online')\\\n",
    "                                             .select(df_d_day.wee_id_week.alias('week_id'), sku.mdl_num_model_r3.alias('model'), df_f_delivery_detail.f_qty_item.cast('int').alias('f_qty_item_on'))\\\n",
    "                                             .where(df_d_day.wee_id_week >= '201911')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# agg online\n",
    "\n",
    "agg_online = online_turnover_detail.groupBy(['week_id', 'model'])\\\n",
    "                                   .agg(sum('f_qty_item_on').alias('y_on'))\n",
    "\n",
    "\n",
    "agg_online.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# agg offline + agg online\n",
    "\n",
    "temp_actual_sales = agg_online.join(agg_offline, [(agg_offline.week_id == agg_online.week_id) & (agg_offline.model == agg_online.model)], how='full')\\\n",
    "                              .select(coalesce(agg_offline.week_id, agg_online.week_id).alias('week_id'),\\\n",
    "                                      coalesce(agg_offline.model, agg_online.model).alias('model'),\\\n",
    "                                      coalesce(agg_offline.y_off, lit(0)).alias('y_off'), \\\n",
    "                                      coalesce(agg_online.y_on,lit(0)).alias('y_on'))\n",
    "              \n",
    "temp_actual_sales = temp_actual_sales.withColumn('y', temp_actual_sales.y_off+temp_actual_sales.y_on)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actual_sales = temp_actual_sales.select(temp_actual_sales.week_id, temp_actual_sales.model, temp_actual_sales.y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actual_sales = actual_sales.join(df_d_week, [(df_d_week.wee_id_week == actual_sales.week_id)], how='inner')\\\n",
    "                           .select(actual_sales.week_id, df_d_week.day_first_day_week.alias('date'), actual_sales.model, actual_sales.y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actual_sales.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lifesatge_update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales_data_material = sales_data_material.where(sales_data_material.sales_org == 'Z001')\\\n",
    "                                         .where(sales_data_material.sap_source == 'PRT')\\\n",
    "                                         .where(sales_data_material.lifestage != '')\\\n",
    "                                         .where(sales_data_material.distrib_channel == '02')\\\n",
    "                                         .where(sales_data_material.date_end >= '2015-01-01')\\\n",
    "                                         .select(sales_data_material.material_id, sales_data_material.lifestage, sales_data_material.date_begin, sales_data_material.date_end)\\\n",
    "                                         .withColumn('sku_num', sales_data_material['material_id'].cast('int'))\\\n",
    "                                         .withColumn(\"d_end\",when(sales_data_material.date_end == '2999-12-31', '2100-12-31').otherwise(sales_data_material.date_end))\\\n",
    "                                         .distinct()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales_data_material.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sku_ls = df_d_sku.where(df_d_sku.sku_date_end == '2999-12-31 23:59:59') \\\n",
    "                 .filter(~col('unv_num_univers').isin([0, 14, 89, 90])) \\\n",
    "                 .where(df_d_sku.sku_date_begin <= date.today()) \\\n",
    "                 .where(df_d_sku.sku_date_end >= date.today()) \\\n",
    "                 .select(['sku_num_sku_r3', 'mdl_num_model_r3']) \\\n",
    "                 .distinct()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lifestage_update = sales_data_material.join(sku_ls, [(sku_ls.sku_num_sku_r3 == sales_data_material.sku_num)])\\\n",
    "                                      .select(sku_ls.mdl_num_model_r3.alias('model'), sku_ls.sku_num_sku_r3.alias('sku'), sales_data_material.date_begin, sales_data_material.d_end.alias('date_end'), sales_data_material.lifestage)\\\n",
    "                                      .distinct()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lifestage_update.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "export_parquet(lifestage_update, 'fcst-refined-demand-forecast-dev', 'part_1/lifestage_update')\n",
    "export_parquet(actual_sales, 'fcst-refined-demand-forecast-dev', 'part_1/actual_sales')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Sparkmagic (PySpark)",
   "language": "",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 2
   },
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
