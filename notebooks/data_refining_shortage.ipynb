{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%configure -f\n",
    "{\n",
    " \"conf\" :\n",
    "    {\n",
    "        \"spark.serializer\" : \"org.apache.spark.serializer.KryoSerializer\",\n",
    "        \"spark.sql.legacy.parquet.int96RebaseModeInRead\" : \"CORRECTED\",\n",
    "        \"spark.sql.legacy.parquet.datetimeRebaseModeInWrite\" : \"CORRECTED\",\n",
    "        \"spark.sql.legacy.parquet.datetimeRebaseModeInRead\" : \"CORRECTED\",\n",
    "        \"spark.sql.legacy.timeParserPolicy\" : \"LEGACY\"\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext, SparkConf, StorageLevel\n",
    "from pyspark.sql import Window, DataFrame\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "import time\n",
    "from functools import reduce\n",
    "from datetime import datetime, timedelta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_uri(bucket, key):\n",
    "    \"\"\"\n",
    "    List all files under a S3 bucket\n",
    "    :param bucket: (string) name of the S3 bucket\n",
    "    :param key: (string) S3 key\n",
    "    :return: (string) URI format\n",
    "    \"\"\"\n",
    "    return 's3://{}/{}'.format(bucket, key)\n",
    "\n",
    "\n",
    "def read_parquet_s3(app, bucket, key):\n",
    "    \"\"\"\n",
    "    Read parquet files on s3 and return a spark dataframe\n",
    "    :app: (SparkSession) spark app\n",
    "    :param bucket: (string) name of the S3 bucket\n",
    "    :param key: (string) S3 key\n",
    "    :return: (SparkDataframe)\n",
    "    \"\"\"\n",
    "    df = app.read.parquet(to_uri(bucket, key))\n",
    "    return df\n",
    "\n",
    "\n",
    "def write_parquet_s3(df, bucket, key, mode='overwrite'):\n",
    "    \"\"\"\n",
    "    Write a SparkDataframe to parquet files on a S3 bucket\n",
    "    :df: (SparkDataframe)\n",
    "    :param bucket: (string) name of the S3 bucket\n",
    "    :param key: (string) S3 key\n",
    "    \"\"\"\n",
    "    df.write.parquet(to_uri(bucket, key), mode=mode)\n",
    "\n",
    "\n",
    "def get_current_week():\n",
    "    \"\"\"\n",
    "    Return current week (international standard ISO 8601 - first day of week\n",
    "    is Sunday, with format 'YYYYWW'\n",
    "    :return current week (international standard ISO 8601) with format 'YYYYWW'\n",
    "    \"\"\"\n",
    "    shifted_date = datetime.today() + timedelta(days=1)\n",
    "    current_week_id = int(str(shifted_date.isocalendar()[0]) + str(shifted_date.isocalendar()[1]).zfill(2))\n",
    "    return current_week_id\n",
    "\n",
    "\n",
    "def get_timer(starting_time):\n",
    "    \"\"\"\n",
    "    Displays the time that has elapsed between the input timer and the current time.\n",
    "    :param starting_time: (timecode) timecode from Python 'time' package\n",
    "    \"\"\"\n",
    "    \n",
    "    end_time = time.time()\n",
    "    minutes, seconds = divmod(int(end_time - starting_time), 60)\n",
    "    print(\"{} minute(s) {} second(s)\".format(int(minutes), seconds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bucket_clean = 'fcst-clean-prod'\n",
    "bucket_refined = 'fcst-refined-demand-forecast-dev'\n",
    "\n",
    "path_clean_datalake = 'datalake/'\n",
    "path_refined_global = 'global/'\n",
    "\n",
    "first_historical_week = 201601\n",
    "first_backtesting_cutoff = 201925\n",
    "current_week = get_current_week()\n",
    "\n",
    "list_purch_org = ['Z001', 'Z008', 'Z042', 'Z066', 'Z078',  'Z098', 'Z107']\n",
    "\n",
    "shortage_history_update = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load all needed clean data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spr = spark.table('fcst_clean_prod.f_stock_picture')\n",
    "lga = read_parquet_s3(spark, bucket_clean, path_clean_datalake + 'd_listing_assortment/')\n",
    "sku = read_parquet_s3(spark, bucket_clean, path_clean_datalake + 'd_sku/')\n",
    "but = read_parquet_s3(spark, bucket_clean, path_clean_datalake + 'd_business_unit/')\n",
    "sapb = read_parquet_s3(spark, bucket_clean, path_clean_datalake + 'sites_attribut_0plant_branches_h/')\n",
    "day = read_parquet_s3(spark, bucket_clean, path_clean_datalake + 'd_day/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apply generic filters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sku = sku \\\n",
    "    .filter(~sku['unv_num_univers'].isin([0, 14, 89, 90])) \\\n",
    "    .filter(sku['mdl_num_model_r3'].isNotNull())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep only open stores\n",
    "but = but \\\n",
    "    .filter(but['but_closed'] != 1) \\\n",
    "    .filter(but['but_num_typ_but'] == 7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Temporary fix with drop duplicates waiting Otman fix\n",
    "sapb = sapb \\\n",
    "    .filter(sapb['sapsrc'] == 'PRT') \\\n",
    "    .filter(sapb['purch_org'].isin(list_purch_org)) \\\n",
    "    .filter(current_timestamp().between(sapb['date_begin'], sapb['date_end'])) \\\n",
    "    .select(sapb['purch_org'], sapb['sales_org'], sapb['plant_id']) \\\n",
    "    .drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "day = day \\\n",
    "    .filter(day['wee_id_week'] >= first_historical_week) \\\n",
    "    .filter(day['wee_id_week'] < current_week)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stock de ventes (out stock en transit, exposition et réservés)\n",
    "spr = spr \\\n",
    "    .filter(spr['stt_idr_stock_type'] == 67)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lga = lga.filter(lga['sap_source'] == 'PRT')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get stock pictures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stock_picture = spr \\\n",
    "    .join(broadcast(sku),\n",
    "          on=spr['sku_idr_sku'] == sku['sku_idr_sku'],\n",
    "          how='inner') \\\n",
    "    .join(broadcast(day),\n",
    "          on=to_date(spr['spr_date_stock'], 'yyyy-MM-dd') == day['day_id_day'],\n",
    "          how='inner') \\\n",
    "    .join(broadcast(but),\n",
    "          on=spr['but_idr_business_unit'] == but['but_idr_business_unit'],\n",
    "          how='inner') \\\n",
    "    .join(broadcast(sapb),\n",
    "          on=but['but_num_business_unit'].cast('string') == regexp_replace(sapb['plant_id'], '^0*|\\s', ''),\n",
    "          how='inner') \\\n",
    "    .select(day['mon_id_month'].alias('month_id'),\n",
    "            day['wee_id_week'].alias('week_id'),\n",
    "            day['day_id_day'].alias('date'),\n",
    "            sapb['purch_org'],\n",
    "            sapb['sales_org'],\n",
    "            but['but_num_business_unit'].alias('but_id'),\n",
    "            sku['mdl_num_model_r3'].alias('model_id'),\n",
    "            sku['sku_idr_sku'].alias('sku_idr'),\n",
    "            spr['f_quantity'].cast('int').alias('stock_quantity')) \\\n",
    "    .persist(StorageLevel.MEMORY_ONLY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"====> counting(cache) [stock_picture] took \")\n",
    "start = time.time()\n",
    "stock_picture_count = stock_picture.count()\n",
    "get_timer(starting_time=start)\n",
    "print(\"[stock_picture] length:\", stock_picture_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get & clean store picking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get store_picking\n",
    "store_picking = lga \\\n",
    "    .join(broadcast(sku),\n",
    "          on=sku['sku_num_sku_r3'] == lga['material_id'].cast('int'),\n",
    "          how='inner') \\\n",
    "    .join(broadcast(sapb),\n",
    "          on=regexp_replace(sapb['plant_id'], '^0*|\\s', '') == regexp_replace(lga['plant_id'], '^0*|\\s', ''),\n",
    "          how='inner') \\\n",
    "    .join(broadcast(but),\n",
    "          on=but['but_num_business_unit'].cast('string') == regexp_replace(lga['plant_id'], '^0*|\\s', ''),\n",
    "          how='inner') \\\n",
    "    .select(but['but_num_business_unit'].alias('but_id'),\n",
    "            sku['sku_idr_sku'].alias('sku_idr'),\n",
    "            to_date(lga['date_valid_from'], 'yyyy-MM-dd').alias('date_valid_from'),\n",
    "            to_date(lga['date_valid_to'], 'yyyy-MM-dd').alias('date_valid_to'),\n",
    "            lga['date_last_change'])\n",
    "\n",
    "# Clean duplicates & date overlaps\n",
    "def overlap_period(list_periods):\n",
    "    res = [list_periods[0]]\n",
    "    for a in list_periods[1:]:\n",
    "        overlap = False\n",
    "        for b in res:\n",
    "            if a[0] <= b[1] and b[0] <= a[1]:\n",
    "                overlap = True\n",
    "                res[res.index(b)] = (a[0] if a[0] < b[0] else b[0], a[1] if a[1] > b[1] else b[1])\n",
    "        if not overlap:\n",
    "            res.append(a)\n",
    "    return res\n",
    "\n",
    "def recursive_overlap(x):\n",
    "    y = overlap_period(x)\n",
    "    if x == y:\n",
    "        return x\n",
    "    else:\n",
    "        return recursive_overlap(y)\n",
    "    \n",
    "w_valid_to = Window().partitionBy('but_id', 'sku_idr', 'date_valid_to').orderBy(col('date_last_change').desc())\n",
    "\n",
    "store_picking = store_picking \\\n",
    "    .withColumn('rn', row_number().over(w_valid_to)) \\\n",
    "    .filter(col('rn') == 1)\n",
    "    \n",
    "w_valid_from = Window().partitionBy(\"but_id\", \"sku_idr\", \"date_valid_from\").orderBy(col(\"date_last_change\").desc())\n",
    "\n",
    "store_picking = store_picking\\\n",
    "    .withColumn(\"rn\", row_number().over(w_valid_from)) \\\n",
    "    .where(col(\"rn\") == 1)\n",
    "\n",
    "overlap_period_udf = udf(lambda list_periods: recursive_overlap(list_periods), ArrayType(ArrayType(DateType())))\n",
    "\n",
    "# Note: you don't have to understand, but it works, trust me\n",
    "store_picking = store_picking \\\n",
    "    .withColumn('dates_from_to', array(store_picking['date_valid_from'], store_picking['date_valid_to'])) \\\n",
    "    .groupBy(\"but_id\", \"sku_idr\") \\\n",
    "    .agg(collect_list(col(\"dates_from_to\")).alias(\"dates_from_to\")) \\\n",
    "    .withColumn(\"all_periods\", overlap_period_udf(col(\"dates_from_to\"))) \\\n",
    "    .withColumn(\"period\", explode(col(\"all_periods\"))) \\\n",
    "    .select(col(\"but_id\"),\n",
    "            col(\"sku_idr\"),\n",
    "            col('period')[0].alias(\"date_valid_from\"),\n",
    "            col('period')[1].alias(\"date_valid_to\")) \\\n",
    "    .persist(StorageLevel.MEMORY_ONLY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"====> counting(cache) [store_picking] took \")\n",
    "start = time.time()\n",
    "store_picking_count = store_picking.count()\n",
    "get_timer(starting_time=start)\n",
    "print(\"[store_picking] length:\", store_picking_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate shortage rate per month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l_month_id = day.select('mon_id_month').drop_duplicates().orderBy('mon_id_month').collect()\n",
    "l_month_id = [int(m[0]) for m in l_month_id]\n",
    "\n",
    "if not shortage_history_update:\n",
    "    l_month_id = l_month_id[-2:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "\n",
    "for month_id in l_month_id:\n",
    "\n",
    "    # calculate all daily_stock combinations\n",
    "    all_week_date = day \\\n",
    "        .filter(day['mon_id_month'] == month_id) \\\n",
    "        .select(day['mon_id_month'].alias('month_id'),\n",
    "                day['wee_id_week'].alias('week_id'), \n",
    "                day['day_id_day'].alias('date'))\n",
    "\n",
    "    all_stock_picture_key = stock_picture \\\n",
    "        .filter(day['mon_id_month'] == month_id) \\\n",
    "        .select(['purch_org', 'sales_org', 'but_id', 'model_id', 'sku_idr']) \\\n",
    "        .drop_duplicates()\n",
    "\n",
    "    all_daily_stock_comb = all_week_date.crossJoin(all_stock_picture_key)\n",
    "\n",
    "    # Left join with stock_picture\n",
    "    daily_stock = all_daily_stock_comb \\\n",
    "        .join(stock_picture, \n",
    "              how='left',\n",
    "              on=['month_id', 'week_id', 'date', 'purch_org', 'sales_org', 'but_id', 'model_id', 'sku_idr'])\n",
    "\n",
    "    # ffill (\"from delta stock check to stock by day\")\n",
    "    window = Window.partitionBy(['but_id', 'sku_idr']) \\\n",
    "        .orderBy('date') \\\n",
    "        .rowsBetween(-sys.maxsize, 0)\n",
    "\n",
    "    daily_stock = daily_stock \\\n",
    "        .withColumn('stock_quantity', last(daily_stock['stock_quantity'], ignorenulls=True).over(window)) \\\n",
    "        .dropna()\n",
    "\n",
    "    # Keep only stock combinations matching with stores picking\n",
    "    daily_stock = daily_stock \\\n",
    "        .join(store_picking,\n",
    "              on=(daily_stock['but_id'] == store_picking['but_id']) &\n",
    "                 (daily_stock['sku_idr'] == store_picking['sku_idr']) &\n",
    "                 (daily_stock['date'].between(store_picking['date_valid_from'], store_picking['date_valid_to'])),\n",
    "              how='inner'\n",
    "             ) \\\n",
    "        .select(daily_stock['*'])\n",
    "\n",
    "    ## Calculate shortage rate\n",
    "    # Agg day to week\n",
    "    shortage_rate = daily_stock \\\n",
    "        .groupBy(['month_id', 'week_id', 'but_id', 'model_id', 'sku_idr']) \\\n",
    "        .agg(count(when(daily_stock['stock_quantity'] == 0, 'stock_quantity')).alias('nb_day_zero'),\n",
    "             count(daily_stock['stock_quantity'] == 0).alias('nb_day_follow'))\n",
    "\n",
    "    # Agg sku to model\n",
    "    shortage_rate = shortage_rate \\\n",
    "        .groupBy(['month_id', 'week_id', 'but_id', 'model_id']) \\\n",
    "        .agg(sum('nb_day_zero').alias('nb_day_zero'),\n",
    "             sum('nb_day_follow').alias('nb_day_follow'))\n",
    "\n",
    "    # Agg bu to zd\n",
    "    shortage_rate = shortage_rate \\\n",
    "        .groupBy(['month_id', 'week_id', 'model_id']) \\\n",
    "        .agg((sum('nb_day_zero') / sum('nb_day_follow')).alias('shortage_rate'))\n",
    "\n",
    "    # Write parquet by month in batch\n",
    "    write_parquet_s3(shortage_rate \\\n",
    "                         .filter(shortage_rate['month_id'] == month_id) \\\n",
    "                         .select('model_id', 'week_id', 'shortage_rate'),\n",
    "                     bucket_refined,\n",
    "                     path_refined_global + 'model_week_shortage_per_month/' + str(month_id))\n",
    "\n",
    "get_timer(starting_time=start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read/Write model week shortage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_week_shortage = read_parquet_s3(spark, \n",
    "                                      bucket_refined, \n",
    "                                      path_refined_global + 'model_week_shortage_per_month/*/')\n",
    "\n",
    "write_parquet_s3(model_week_shortage.orderBy('model_id', 'week_id'), \n",
    "                 bucket_refined,\n",
    "                 path_refined_global + 'model_week_shortage')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Sparkmagic (PySpark)",
   "language": "python",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
