{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Spark application\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>User</th><th>Current session?</th></tr><tr><td>8</td><td>application_1634288852134_0011</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-10-226-39-201.ie.aws.dktinfra.cloud:20888/proxy/application_1634288852134_0011/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-10-226-39-20.ie.aws.dktinfra.cloud:8042/node/containerlogs/container_1634288852134_0011_01_000001/livy\">Link</a></td><td>None</td><td>✔</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "28a7d57752fd4ff6bc6070c54fd4ca9d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession available as 'spark'.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "61aa78ac62974662a6d1a77223de9b57",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Spark application\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>User</th><th>Current session?</th></tr><tr><td>9</td><td>application_1634288852134_0012</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-10-226-39-201.ie.aws.dktinfra.cloud:20888/proxy/application_1634288852134_0012/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-10-226-39-20.ie.aws.dktinfra.cloud:8042/node/containerlogs/container_1634288852134_0012_01_000001/livy\">Link</a></td><td>None</td><td>✔</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f77c12c1b0634f38b440061c0bc48552",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession available as 'spark'.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Current session configs: <tt>{'conf': {'spark.serializer': 'org.apache.spark.serializer.KryoSerializer', 'spark.sql.legacy.parquet.int96RebaseModeInRead': 'CORRECTED', 'spark.sql.legacy.parquet.datetimeRebaseModeInWrite': 'CORRECTED', 'spark.sql.legacy.parquet.datetimeRebaseModeInRead': 'CORRECTED', 'spark.sql.legacy.timeParserPolicy': 'LEGACY'}, 'kind': 'pyspark'}</tt><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>User</th><th>Current session?</th></tr><tr><td>9</td><td>application_1634288852134_0012</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-10-226-39-201.ie.aws.dktinfra.cloud:20888/proxy/application_1634288852134_0012/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-10-226-39-20.ie.aws.dktinfra.cloud:8042/node/containerlogs/container_1634288852134_0012_01_000001/livy\">Link</a></td><td>None</td><td>✔</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%configure -f\n",
    "{\n",
    " \"conf\" :\n",
    "    {\n",
    "        \"spark.serializer\" : \"org.apache.spark.serializer.KryoSerializer\",\n",
    "        \"spark.sql.legacy.parquet.int96RebaseModeInRead\" : \"CORRECTED\",\n",
    "        \"spark.sql.legacy.parquet.datetimeRebaseModeInWrite\" : \"CORRECTED\",\n",
    "        \"spark.sql.legacy.parquet.datetimeRebaseModeInRead\" : \"CORRECTED\",\n",
    "        \"spark.sql.legacy.timeParserPolicy\" : \"LEGACY\"\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5f1199d0e10f499cb56c7328ae65fdf8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pyspark import SparkContext, SparkConf, StorageLevel\n",
    "from pyspark.sql import Window, DataFrame\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "import time\n",
    "from functools import reduce\n",
    "from datetime import datetime, timedelta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f8a62857f3114557ad24f76491f2bb85",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def to_uri(bucket, key):\n",
    "    \"\"\"\n",
    "    List all files under a S3 bucket\n",
    "    :param bucket: (string) name of the S3 bucket\n",
    "    :param key: (string) S3 key\n",
    "    :return: (string) URI format\n",
    "    \"\"\"\n",
    "    return 's3://{}/{}'.format(bucket, key)\n",
    "\n",
    "\n",
    "def read_parquet_s3(app, bucket, key):\n",
    "    \"\"\"\n",
    "    Read parquet files on s3 and return a spark dataframe\n",
    "    :app: (SparkSession) spark app\n",
    "    :param bucket: (string) name of the S3 bucket\n",
    "    :param key: (string) S3 key\n",
    "    :return: (SparkDataframe)\n",
    "    \"\"\"\n",
    "    df = app.read.parquet(to_uri(bucket, key))\n",
    "    return df\n",
    "\n",
    "\n",
    "def write_parquet_s3(df, bucket, key, mode='overwrite'):\n",
    "    \"\"\"\n",
    "    Write a SparkDataframe to parquet files on a S3 bucket\n",
    "    :df: (SparkDataframe)\n",
    "    :param bucket: (string) name of the S3 bucket\n",
    "    :param key: (string) S3 key\n",
    "    \"\"\"\n",
    "    df.write.parquet(to_uri(bucket, key), mode=mode)\n",
    "\n",
    "\n",
    "def get_current_week():\n",
    "    \"\"\"\n",
    "    Return current week (international standard ISO 8601 - first day of week\n",
    "    is Sunday, with format 'YYYYWW'\n",
    "    :return current week (international standard ISO 8601) with format 'YYYYWW'\n",
    "    \"\"\"\n",
    "    shifted_date = datetime.today() + timedelta(days=1)\n",
    "    current_week_id = int(str(shifted_date.isocalendar()[0]) + str(shifted_date.isocalendar()[1]).zfill(2))\n",
    "    return current_week_id\n",
    "\n",
    "\n",
    "def get_timer(starting_time):\n",
    "    \"\"\"\n",
    "    Displays the time that has elapsed between the input timer and the current time.\n",
    "    :param starting_time: (timecode) timecode from Python 'time' package\n",
    "    \"\"\"\n",
    "    \n",
    "    end_time = time.time()\n",
    "    minutes, seconds = divmod(int(end_time - starting_time), 60)\n",
    "    print(\"{} minute(s) {} second(s)\".format(int(minutes), seconds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b91a4900660d418f8a6d2f93c920f602",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "bucket_clean = 'fcst-clean-prod'\n",
    "bucket_refined = 'fcst-refined-demand-forecast-dev'\n",
    "\n",
    "path_clean_datalake = 'datalake/'\n",
    "path_refined_global = 'global/'\n",
    "\n",
    "first_historical_week = 201601\n",
    "first_backtesting_cutoff = 201925\n",
    "current_week = get_current_week()\n",
    "\n",
    "list_purch_org = ['Z001', 'Z008', 'Z042', 'Z066', 'Z078',  'Z098', 'Z107']\n",
    "\n",
    "shortage_history_update = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load all needed clean data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "404d4a04606d4fcd9d0a82d4e9f25407",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "spr = spark.table('fcst_clean_prod.f_stock_picture')\n",
    "lga = read_parquet_s3(spark, bucket_clean, path_clean_datalake + 'd_listing_assortment/')\n",
    "sku = read_parquet_s3(spark, bucket_clean, path_clean_datalake + 'd_sku/')\n",
    "but = read_parquet_s3(spark, bucket_clean, path_clean_datalake + 'd_business_unit/')\n",
    "sapb = read_parquet_s3(spark, bucket_clean, path_clean_datalake + 'sites_attribut_0plant_branches_h/')\n",
    "day = read_parquet_s3(spark, bucket_clean, path_clean_datalake + 'd_day/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apply generic filters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e87b1dbd8c0b44dbaf5900731550cbad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sku = sku \\\n",
    "    .filter(~sku['unv_num_univers'].isin([0, 14, 89, 90])) \\\n",
    "    .filter(sku['mdl_num_model_r3'].isNotNull())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f1a969e0aee64f959f87f05f8cf076dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Keep only open stores\n",
    "but = but \\\n",
    "    .filter(but['but_closed'] != 1) \\\n",
    "    .filter(but['but_num_typ_but'] == 7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cfc064f15c7c42c1b378a7a8e95d7e3c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Temporary fix with drop duplicates waiting Otman fix\n",
    "sapb = sapb \\\n",
    "    .filter(sapb['sapsrc'] == 'PRT') \\\n",
    "    .filter(sapb['purch_org'].isin(list_purch_org)) \\\n",
    "    .filter(current_timestamp().between(sapb['date_begin'], sapb['date_end'])) \\\n",
    "    .select(sapb['purch_org'], sapb['sales_org'], sapb['plant_id']) \\\n",
    "    .drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7003a67d23e24159a6194a28f4b18520",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "day = day \\\n",
    "    .filter(day['wee_id_week'] >= first_historical_week) \\\n",
    "    .filter(day['wee_id_week'] < current_week)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dbe0dbd76c6b4498b982e466018c94c2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# stock de ventes (out stock en transit, exposition et réservés)\n",
    "spr = spr \\\n",
    "    .filter(spr['stt_idr_stock_type'] == 67)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f3b54840a50149aeb0b1c6d41bb2ba25",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "lga = lga.filter(lga['sap_source'] == 'PRT')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get stock pictures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "48b772ccb1c24dc982d3665bebda6b8d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "stock_picture = spr \\\n",
    "    .join(broadcast(sku),\n",
    "          on=spr['sku_idr_sku'] == sku['sku_idr_sku'],\n",
    "          how='inner') \\\n",
    "    .join(broadcast(day),\n",
    "          on=to_date(spr['spr_date_stock'], 'yyyy-MM-dd') == day['day_id_day'],\n",
    "          how='inner') \\\n",
    "    .join(broadcast(but),\n",
    "          on=spr['but_idr_business_unit'] == but['but_idr_business_unit'],\n",
    "          how='inner') \\\n",
    "    .join(broadcast(sapb),\n",
    "          on=but['but_num_business_unit'].cast('string') == regexp_replace(sapb['plant_id'], '^0*|\\s', ''),\n",
    "          how='inner') \\\n",
    "    .select(day['wee_id_week'].alias('week_id'),\n",
    "            day['day_id_day'].alias('date'),\n",
    "            sapb['purch_org'],\n",
    "            sapb['sales_org'],\n",
    "            but['but_num_business_unit'].alias('but_id'),\n",
    "            sku['mdl_num_model_r3'].alias('model_id'),\n",
    "            sku['sku_idr_sku'].alias('sku_idr'),\n",
    "            spr['f_quantity'].cast('int').alias('stock_quantity')) \\\n",
    "    .persist(StorageLevel.MEMORY_ONLY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "99f9dfa80b5f4e87bf4d01a5d8190eeb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====> counting(cache) [stock_picture] took \n",
      "5 minute(s) 20 second(s)\n",
      "[stock_picture] length: 9717511852"
     ]
    }
   ],
   "source": [
    "print(\"====> counting(cache) [stock_picture] took \")\n",
    "start = time.time()\n",
    "stock_picture_count = stock_picture.count()\n",
    "get_timer(starting_time=start)\n",
    "print(\"[stock_picture] length:\", stock_picture_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get & clean store picking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2d4beb6de78c4f7c8b8448e5e41177a4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Get store_picking\n",
    "store_picking = lga \\\n",
    "    .join(broadcast(sku),\n",
    "          on=sku['sku_num_sku_r3'] == lga['material_id'].cast('int'),\n",
    "          how='inner') \\\n",
    "    .join(broadcast(sapb),\n",
    "          on=regexp_replace(sapb['plant_id'], '^0*|\\s', '') == regexp_replace(lga['plant_id'], '^0*|\\s', ''),\n",
    "          how='inner') \\\n",
    "    .join(broadcast(but),\n",
    "          on=but['but_num_business_unit'].cast('string') == regexp_replace(lga['plant_id'], '^0*|\\s', ''),\n",
    "          how='inner') \\\n",
    "    .select(but['but_num_business_unit'].alias('but_id'),\n",
    "            sku['sku_idr_sku'].alias('sku_idr'),\n",
    "            to_date(lga['date_valid_from'], 'yyyy-MM-dd').alias('date_valid_from'),\n",
    "            to_date(lga['date_valid_to'], 'yyyy-MM-dd').alias('date_valid_to'),\n",
    "            to_date(lga['date_last_change'], 'yyyy-MM-dd').alias('date_last_change')) \\"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8b0c7083f9a54517974c086242a60511",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Clean duplicates & date overlaps\n",
    "def overlap_period(list_periods):\n",
    "    list_periods = sorted(list_periods, key=sorter, reverse = True)\n",
    "    res = [list_periods[0]]\n",
    "    date_from = 0\n",
    "    date_to = 1\n",
    "    date_update = 2\n",
    "    for a in list_periods[1:]:\n",
    "        overlap = False\n",
    "        for b in res:\n",
    "            if a[date_update] < b[date_update] and a[date_from] <= b[date_to] and b[date_from] <= a[date_to]:\n",
    "                overlap = True\n",
    "        if not overlap:\n",
    "            res.append(a)\n",
    "    return res\n",
    "\n",
    "\n",
    "overlap_period_udf = udf(lambda list_periods: overlap_period(list_periods), ArrayType(ArrayType(DateType())))\n",
    "sorter = lambda x: (x[2], x[0], x[1])\n",
    "\n",
    "w_valid_to = Window().partitionBy('but_id', 'sku_idr', 'date_valid_to').orderBy(col('date_last_change').desc())\n",
    "\n",
    "store_picking = store_picking \\\n",
    "    .withColumn('rn', row_number().over(w_valid_to)) \\\n",
    "    .filter(col('rn') == 1)\n",
    "    \n",
    "w_valid_from = Window().partitionBy(\"but_id\", \"sku_idr\", \"date_valid_from\").orderBy(col(\"date_last_change\").desc())\n",
    "\n",
    "store_picking = store_picking\\\n",
    "    .withColumn(\"rn\", row_number().over(w_valid_from)) \\\n",
    "    .where(col(\"rn\") == 1)\n",
    "\n",
    "# Note: you don't have to understand, but it works, trust me\n",
    "store_picking = store_picking \\\n",
    "    .withColumn('dates_from_to', array(store_picking['date_valid_from'], store_picking['date_valid_to'],store_picking['date_last_change'])) \\\n",
    "    .groupBy(\"but_id\", \"sku_idr\") \\\n",
    "    .agg(collect_list(col(\"dates_from_to\")).alias(\"dates_from_to\")) \\\n",
    "    .withColumn(\"all_periods\", overlap_period_udf(col(\"dates_from_to\"))) \\\n",
    "    .withColumn(\"period\", explode(col(\"all_periods\"))) \\\n",
    "    .select(col(\"but_id\"),\n",
    "            col(\"sku_idr\"),\n",
    "            col('period'),\n",
    "            col('period')[0].alias(\"date_valid_from\"),\n",
    "            col('period')[1].alias(\"date_valid_to\")) \\\n",
    "    .persist(StorageLevel.MEMORY_ONLY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7a8e48c08cce4203a14543f2d1d2196d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====> counting(cache) [store_picking] took \n",
      "4 minute(s) 13 second(s)\n",
      "[store_picking] length: 445979432"
     ]
    }
   ],
   "source": [
    "print(\"====> counting(cache) [store_picking] took \")\n",
    "start = time.time()\n",
    "store_picking_count = store_picking.count()\n",
    "get_timer(starting_time=start)\n",
    "print(\"[store_picking] length:\", store_picking_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate shortage rate per week"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "08c5c0bedba04d9d839c76345e888568",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "l_week_id = day.select('wee_id_week').drop_duplicates().orderBy('wee_id_week').collect()\n",
    "l_week_id = [int(w[0]) for w in l_week_id]\n",
    "\n",
    "if not shortage_history_update:\n",
    "    l_week_id = l_week_id[-8:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f623b36b0b614d73b20a3796f97fd8c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 minute(s) 23 second(s)"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "#l_week_id = [202110,202109,202108,202107,202106,202105,202104,202103,202102,202101]\n",
    "l_week_id = [202110,202109]\n",
    "for week_id in l_week_id:\n",
    "\n",
    "    # calculate all daily_stock combinations\n",
    "    all_week_date = day \\\n",
    "        .filter(day['wee_id_week'] == week_id) \\\n",
    "        .select(day['wee_id_week'].alias('week_id'), \n",
    "                day['day_id_day'].alias('date'))\n",
    "\n",
    "    all_stock_picture_key = stock_picture \\\n",
    "        .filter(day['week_id'] == week_id) \\\n",
    "        .select(['purch_org', 'sales_org', 'but_id', 'model_id', 'sku_idr']) \\\n",
    "        .drop_duplicates()\n",
    "\n",
    "    all_daily_stock_comb = all_week_date.crossJoin(all_stock_picture_key)\n",
    "\n",
    "    # Left join with stock_picture\n",
    "    daily_stock = all_daily_stock_comb \\\n",
    "        .join(stock_picture, \n",
    "              how='left',\n",
    "              on=['week_id', 'date', 'purch_org', 'sales_org', 'but_id', 'model_id', 'sku_idr'])\n",
    "\n",
    "    # ffill (\"from delta stock check to stock by day\")\n",
    "    window = Window.partitionBy(['but_id', 'sku_idr']) \\\n",
    "        .orderBy('date') \\\n",
    "        .rowsBetween(-sys.maxsize, 0)\n",
    "\n",
    "    daily_stock = daily_stock \\\n",
    "        .withColumn('stock_quantity', last(daily_stock['stock_quantity'], ignorenulls=True).over(window)) \\\n",
    "        .dropna()\n",
    "\n",
    "    # Keep only stock combinations matching with stores picking\n",
    "    daily_stock = daily_stock \\\n",
    "        .join(store_picking,\n",
    "              on=(daily_stock['but_id'] == store_picking['but_id']) &\n",
    "                 (daily_stock['sku_idr'] == store_picking['sku_idr']) &\n",
    "                 (daily_stock['date'].between(store_picking['date_valid_from'], store_picking['date_valid_to'])),\n",
    "              how='inner'\n",
    "             ) \\\n",
    "        .select(daily_stock['*'])\n",
    "\n",
    "    ## Calculate shortage rate\n",
    "    # Agg day to week\n",
    "    shortage_rate = daily_stock \\\n",
    "        .groupBy(['week_id', 'but_id', 'model_id', 'sku_idr']) \\\n",
    "        .agg(count(when(daily_stock['stock_quantity'] == 0, 'stock_quantity')).alias('nb_day_zero'),\n",
    "             count(daily_stock['stock_quantity'] == 0).alias('nb_day_follow'))\n",
    "\n",
    "    # Agg sku to model\n",
    "    shortage_rate = shortage_rate \\\n",
    "        .groupBy(['week_id', 'but_id', 'model_id']) \\\n",
    "        .agg(sum('nb_day_zero').alias('nb_day_zero'),\n",
    "             sum('nb_day_follow').alias('nb_day_follow'))\n",
    "\n",
    "    # Agg bu to zd\n",
    "    shortage_rate = shortage_rate \\\n",
    "        .groupBy(['week_id', 'model_id']) \\\n",
    "        .agg((sum('nb_day_zero') / sum('nb_day_follow')).alias('shortage_rate'))\n",
    "\n",
    "    # Write parquet by week in batch\n",
    "    write_parquet_s3(shortage_rate \\\n",
    "                         .filter(shortage_rate['week_id'] == week_id) \\\n",
    "                         .select('model_id', 'week_id', 'shortage_rate'),\n",
    "                     bucket_refined,\n",
    "                     path_refined_global + 'model_week_shortage_per_week/' + str(week_id))\n",
    "\n",
    "get_timer(starting_time=start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read/Write model week shortage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_week_shortage = read_parquet_s3(spark, \n",
    "                                      bucket_refined, \n",
    "                                      path_refined_global + 'model_week_shortage_per_week/*/')\n",
    "\n",
    "write_parquet_s3(model_week_shortage.orderBy('model_id', 'week_id'), \n",
    "                 bucket_refined,\n",
    "                 path_refined_global + 'model_week_shortage')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Sparkmagic (PySpark)",
   "language": "python",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
