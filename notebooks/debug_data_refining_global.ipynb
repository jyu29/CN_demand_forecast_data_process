{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%configure -f\n",
    "{\n",
    " \"conf\" :\n",
    "    {\n",
    "        \"spark.serializer\" : \"org.apache.spark.serializer.KryoSerializer\",\n",
    "        \"spark.sql.legacy.parquet.int96RebaseModeInRead\" : \"CORRECTED\",\n",
    "        \"spark.sql.legacy.parquet.datetimeRebaseModeInWrite\" : \"CORRECTED\",\n",
    "        \"spark.sql.legacy.parquet.datetimeRebaseModeInRead\" : \"CORRECTED\",\n",
    "        \"spark.sql.legacy.timeParserPolicy\" : \"LEGACY\"\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "from datetime import datetime, timedelta\n",
    "from functools import reduce\n",
    "\n",
    "from pyspark import SparkConf, StorageLevel\n",
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.functions as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_uri(bucket, key):\n",
    "    \"\"\"\n",
    "    Transforms bucket & key strings into S3 URI\n",
    "\n",
    "    Args:\n",
    "        bucket (string): name of the S3 bucket\n",
    "        key (string): S3 key\n",
    "\n",
    "    Returns:\n",
    "        object (string): URI format\n",
    "    \"\"\"\n",
    "    return 's3://{}/{}'.format(bucket, key)\n",
    "\n",
    "\n",
    "def spark_read_parquet_s3(spark, bucket, path):\n",
    "    \"\"\"\n",
    "    Read parquet file(s) hosted on a S3 bucket, load and return as spark dataframe\n",
    "\n",
    "    Args:\n",
    "        spark (SparkSession): spark app\n",
    "        bucket (string): S3 bucket\n",
    "        path (string): full path to the parquet directory or file within the S3 bucket\n",
    "\n",
    "    Returns:\n",
    "        (SparkDataframe): data loaded\n",
    "    \"\"\"\n",
    "    return spark.read.parquet(to_uri(bucket, path))\n",
    "\n",
    "\n",
    "def spark_write_parquet_s3(df, bucket, dir_path, repartition=10, mode='overwrite'):\n",
    "    \"\"\"\n",
    "    Write a in-memory SparkDataframe to parquet files on a S3 bucket\n",
    "\n",
    "    Args:\n",
    "        df (SparkDataframe): the data to save\n",
    "        bucket (string): S3 bucket\n",
    "        dir_path (string): full path to the parquet directory within the S3 bucket\n",
    "        repartition (int): number of partitions files to write\n",
    "        mode (string): writing mode\n",
    "    \"\"\"\n",
    "    df.repartition(repartition).write.parquet(to_uri(bucket, dir_path), mode=mode)\n",
    "    \n",
    "\n",
    "def get_timer(starting_time):\n",
    "    \"\"\"\n",
    "    Displays the time that has elapsed between the input timer and the current time.\n",
    "\n",
    "    Args:\n",
    "        starting_time (timecode): timecode from Python 'time' package\n",
    "    \"\"\"\n",
    "    end_time = time.time()\n",
    "    minutes, seconds = divmod(int(end_time - starting_time), 60)\n",
    "    print(\"{} minute(s) {} second(s)\".format(int(minutes), seconds))\n",
    "\n",
    "\n",
    "def union_all(l_df):\n",
    "    \"\"\"\n",
    "    Apply union function on all spark dataframes in l_df\n",
    "\n",
    "    \"\"\"\n",
    "    return reduce(lambda df1, df2: df1.union(df2.select(df1.columns)), l_df)\n",
    "\n",
    "\n",
    "def date_to_week_id(date):\n",
    "    \"\"\"\n",
    "    Turn a date to Decathlon week id\n",
    "    Args:\n",
    "        date (str, pd.Timestamp or pd.Series): the date or pandas column of dates\n",
    "    Returns:\n",
    "        (int): the week id\n",
    "\n",
    "    \"\"\"\n",
    "    day_of_week = date.strftime(\"%w\")\n",
    "    date = date if (day_of_week != '0') else date + timedelta(days=1)\n",
    "    return int(str(date.isocalendar()[0]) + str(date.isocalendar()[1]).zfill(2))\n",
    "\n",
    "\n",
    "def get_current_week_id():\n",
    "    \"\"\"\n",
    "    Return current week id (international standard ISO 8601 - first day of week\n",
    "    is Sunday, with format 'YYYYWW', as integer\n",
    "\n",
    "    \"\"\"\n",
    "    return date_to_week_id(datetime.today())\n",
    "\n",
    "\n",
    "def get_shift_n_week(week_id, nb_weeks):\n",
    "    \"\"\"\n",
    "    Return input week_id shifted by nb_weeks (could be negative)\n",
    "\n",
    "    \"\"\"\n",
    "    shifted_date = datetime.strptime(str(week_id) + '1', '%G%V%u') + timedelta(weeks=nb_weeks)\n",
    "    ret_week_id = date_to_week_id(shifted_date)\n",
    "    return ret_week_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bucket_clean = 'fcst-clean-prod'\n",
    "bucket_refined = 'fcst-refined-demand-forecast-dev'\n",
    "\n",
    "path_clean_datalake = 'datalake/'\n",
    "path_refined_global = 'global/'\n",
    "\n",
    "first_historical_week = 201601\n",
    "first_backtesting_cutoff = 201925\n",
    "current_week = get_current_week_id()\n",
    "\n",
    "list_purch_org = ['Z001', 'Z002', 'Z003', 'Z004', 'Z005', 'Z006', 'Z008', 'Z011', 'Z012', 'Z013', 'Z017', \n",
    "                  'Z019', 'Z022', 'Z025', 'Z026', 'Z027', 'Z028', 'Z042', 'Z060', 'Z061', 'Z065', 'Z066', \n",
    "                  'Z078', 'Z091', 'Z093', 'Z094', 'Z095', 'Z096', 'Z098', 'Z101', 'Z102', 'Z104', 'Z105', \n",
    "                  'Z106', 'Z107', 'Z112', 'Z115']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load all needed clean data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tdt = spark_read_parquet_s3(spark, bucket_clean, path_clean_datalake + 'f_transaction_detail/')\n",
    "dyd = spark_read_parquet_s3(spark, bucket_clean, path_clean_datalake + 'f_delivery_detail/')\n",
    "cex = spark_read_parquet_s3(spark, bucket_clean, path_clean_datalake + 'f_currency_exchange/')\n",
    "sku = spark_read_parquet_s3(spark, bucket_clean, path_clean_datalake + 'd_sku/')\n",
    "sku_h = spark_read_parquet_s3(spark, bucket_clean, path_clean_datalake + 'd_sku_h/')\n",
    "but = spark_read_parquet_s3(spark, bucket_clean, path_clean_datalake + 'd_business_unit/')\n",
    "sapb = spark_read_parquet_s3(spark, bucket_clean, path_clean_datalake + 'sites_attribut_0plant_branches_h/')\n",
    "gdw = spark_read_parquet_s3(spark, bucket_clean, path_clean_datalake + 'd_general_data_warehouse_h/')\n",
    "gdc = spark_read_parquet_s3(spark, bucket_clean, path_clean_datalake + 'd_general_data_customer/')\n",
    "day = spark_read_parquet_s3(spark, bucket_clean, path_clean_datalake + 'd_day/')\n",
    "week = spark_read_parquet_s3(spark, bucket_clean, path_clean_datalake + 'd_week/')\n",
    "sms = spark_read_parquet_s3(spark, bucket_clean, path_clean_datalake + 'apo_sku_mrp_status_h/')\n",
    "zep = spark_read_parquet_s3(spark, bucket_clean, path_clean_datalake + 'ecc_zaa_extplan/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apply generic filters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cex = cex \\\n",
    "    .filter(cex['cpt_idr_cur_price'] == 6) \\\n",
    "    .filter(cex['cur_idr_currency_restit'] == 32) \\\n",
    "    .filter(F.current_timestamp().between(cex['hde_effect_date'], cex['hde_end_date'])) \\\n",
    "    .select(cex['cur_idr_currency_base'].alias('cur_idr_currency'),\n",
    "            cex['hde_share_price']) \\\n",
    "    .groupby('cur_idr_currency') \\\n",
    "    .agg(F.mean(cex['hde_share_price']).alias('exchange_rate'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sku = sku \\\n",
    "    .filter(~sku['unv_num_univers'].isin([0, 14, 89, 90])) \\\n",
    "    .filter(sku['mdl_num_model_r3'].isNotNull()) \\\n",
    "    .filter(sku['sku_num_sku_r3'].isNotNull()) \\\n",
    "    .filter(sku['fam_num_family'].isNotNull()) \\\n",
    "    .filter(sku['sdp_num_sub_department'].isNotNull()) \\\n",
    "    .filter(sku['dpt_num_department'].isNotNull()) \\\n",
    "    .filter(sku['unv_num_univers'].isNotNull()) \\\n",
    "    .filter(sku['pnt_num_product_nature'].isNotNull())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sku_h = sku_h \\\n",
    "    .filter(~sku_h['unv_num_univers'].isin([0, 14, 89, 90])) \\\n",
    "    .filter(sku_h['mdl_num_model_r3'].isNotNull()) \\\n",
    "    .filter(sku_h['sku_num_sku_r3'].isNotNull()) \\\n",
    "    .filter(sku_h['fam_num_family'].isNotNull()) \\\n",
    "    .filter(sku_h['sdp_num_sub_department'].isNotNull()) \\\n",
    "    .filter(sku_h['dpt_num_department'].isNotNull()) \\\n",
    "    .filter(sku_h['unv_num_univers'].isNotNull()) \\\n",
    "    .filter(sku_h['pnt_num_product_nature'].isNotNull())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "day = day \\\n",
    "    .filter(day['wee_id_week'] >= first_historical_week) \\\n",
    "    .filter(day['wee_id_week'] < current_week)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "week = week \\\n",
    "    .filter(week['wee_id_week'] >= first_historical_week) \\\n",
    "    .filter(week['wee_id_week'] < current_week)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sapb = sapb \\\n",
    "    .filter(sapb['sapsrc'] == 'PRT') \\\n",
    "    .filter(sapb['purch_org'].isin(list_purch_org)) \\\n",
    "    .filter(F.current_timestamp().between(sapb['date_begin'], sapb['date_end']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdw = gdw \\\n",
    "    .filter(gdw['sdw_sap_source'] == 'PRT') \\\n",
    "    .filter(gdw['sdw_material_mrp'] != '    ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create model_week_sales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get offline sales\n",
    "offline_sales = tdt \\\n",
    "    .join(F.broadcast(day),\n",
    "          on=F.to_date(tdt['tdt_date_to_ordered'], 'yyyy-MM-dd') == day['day_id_day'],\n",
    "          how='inner') \\\n",
    "    .join(F.broadcast(week),\n",
    "          on='wee_id_week',\n",
    "          how='inner') \\\n",
    "    .join(sku,\n",
    "          on='sku_idr_sku',\n",
    "          how='inner') \\\n",
    "    .join(F.broadcast(but.filter(but['but_num_typ_but'] == 7)),\n",
    "          on='but_idr_business_unit',\n",
    "          how='inner') \\\n",
    "    .join(F.broadcast(cex),\n",
    "          on=tdt['cur_idr_currency'] == cex['cur_idr_currency'],\n",
    "          how='inner') \\\n",
    "    .join(F.broadcast(sapb),\n",
    "          on=but['but_num_business_unit'].cast('string') == F.regexp_replace(sapb['plant_id'], '^0*|\\s', ''),\n",
    "          how='inner') \\\n",
    "    .filter(F.lower(tdt['the_to_type']) == 'offline') \\\n",
    "    .select(sku['mdl_num_model_r3'].alias('model_id'),\n",
    "            day['wee_id_week'].cast('int').alias('week_id'),\n",
    "            week['day_first_day_week'].alias('date'),\n",
    "            tdt['f_qty_item'],\n",
    "            tdt['f_pri_regular_sales_unit'],\n",
    "            tdt['f_to_tax_in'],\n",
    "            cex['exchange_rate'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get online sales\n",
    "online_sales = dyd \\\n",
    "    .join(F.broadcast(day),\n",
    "          on=F.to_date(dyd['tdt_date_to_ordered'], 'yyyy-MM-dd') == day['day_id_day'],\n",
    "          how='inner') \\\n",
    "    .join(F.broadcast(week),\n",
    "          on='wee_id_week',\n",
    "          how='inner') \\\n",
    "    .join(sku,\n",
    "          on='sku_idr_sku',\n",
    "          how='inner') \\\n",
    "    .join(F.broadcast(but),\n",
    "          on=dyd['but_idr_business_unit_sender'] == but['but_idr_business_unit'],\n",
    "          how='inner') \\\n",
    "    .join(F.broadcast(gdc),\n",
    "          on=but['but_code_international'] == F.concat(gdc['ean_1'], gdc['ean_2'], gdc['ean_3']),\n",
    "          how='inner') \\\n",
    "    .join(F.broadcast(cex),\n",
    "          on='cur_idr_currency',\n",
    "          how='inner') \\\n",
    "    .join(F.broadcast(sapb),\n",
    "          on=gdc['plant_id'] == sapb['plant_id'],\n",
    "          how='inner') \\\n",
    "    .filter(F.lower(dyd['the_to_type']) == 'online') \\\n",
    "    .filter(F.lower(dyd['tdt_type_detail']) == 'sale') \\\n",
    "    .select(sku['mdl_num_model_r3'].alias('model_id'),\n",
    "            day['wee_id_week'].cast('int').alias('week_id'),\n",
    "            week['day_first_day_week'].alias('date'),\n",
    "            dyd['f_qty_item'],\n",
    "            dyd['f_tdt_pri_regular_sales_unit'].alias('f_pri_regular_sales_unit'),\n",
    "            dyd['f_to_tax_in'],\n",
    "            cex['exchange_rate'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Union sales & compute metrics\n",
    "model_week_sales = offline_sales.union(online_sales) \\\n",
    "    .groupby(['model_id', 'week_id', 'date']) \\\n",
    "    .agg(F.sum('f_qty_item').alias('sales_quantity'),\n",
    "         F.mean(F.col('f_pri_regular_sales_unit') * F.col('exchange_rate')).alias('average_price'),\n",
    "         F.sum(F.col('f_to_tax_in') * F.col('exchange_rate')).alias('sum_turnover')) \\\n",
    "    .filter(F.col('sales_quantity') > 0) \\\n",
    "    .filter(F.col('average_price') > 0) \\\n",
    "    .filter(F.col('sum_turnover') > 0) \\\n",
    "    .orderBy('model_id', 'week_id') \\\n",
    "    .persist(StorageLevel.MEMORY_ONLY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('====> counting(cache) [model_week_sales] took ')\n",
    "start = time.time()\n",
    "model_week_sales_count = model_week_sales.count()\n",
    "get_timer(starting_time=start)\n",
    "print('[model_week_sales] length:', model_week_sales_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create model_week_tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_week_tree = sku_h \\\n",
    "    .join(F.broadcast(week),\n",
    "          on=week['day_first_day_week'].between(sku_h['sku_date_begin'], sku_h['sku_date_end']),\n",
    "          how='inner') \\\n",
    "    .groupBy(week['wee_id_week'].cast('int').alias('week_id'),\n",
    "             sku_h['mdl_num_model_r3'].alias('model_id')) \\\n",
    "    .agg(F.max(sku_h['fam_num_family']).alias('family_id'),\n",
    "         F.max(sku_h['sdp_num_sub_department']).alias('sub_department_id'),\n",
    "         F.max(sku_h['dpt_num_department']).alias('department_id'),\n",
    "         F.max(sku_h['unv_num_univers']).alias('univers_id'),\n",
    "         F.max(sku_h['pnt_num_product_nature']).alias('product_nature_id'),\n",
    "         F.max(F.when(sku_h['mdl_label'].isNull(), 'UNKNOWN').otherwise(sku_h['mdl_label'])).alias('model_label'),\n",
    "         F.max(sku_h['family_label']).alias('family_label'),\n",
    "         F.max(sku_h['sdp_label']).alias('sub_department_label'),\n",
    "         F.max(sku_h['dpt_label']).alias('department_label'),\n",
    "         F.max(sku_h['unv_label']).alias('univers_label'),\n",
    "         F.max(F.when(sku_h['product_nature_label'].isNull(), 'UNDEFINED')\n",
    "               .otherwise(sku_h['product_nature_label'])).alias('product_nature_label'),\n",
    "         F.max(sku_h['brd_label_brand']).alias('brand_label'),\n",
    "         F.max(sku_h['brd_type_brand_libelle']).alias('brand_type')) \\\n",
    "    .orderBy('week_id', 'model_id') \\\n",
    "    .persist(StorageLevel.MEMORY_ONLY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('====> counting(cache) [model_week_tree] took ')\n",
    "start = time.time()\n",
    "model_week_tree_count = model_week_tree.count()\n",
    "get_timer(starting_time=start)\n",
    "print('[model_week_tree] length:', model_week_tree_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create model_week_mrp"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "TO DO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reduce tables according to the models found in model_week_sales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l_model_id = model_week_sales.select('model_id').drop_duplicates()\n",
    "model_week_tree = model_week_tree.join(l_model_id, on='model_id', how='inner')\n",
    "#model_week_mrp = model_week_mrp.join(l_model_id, on='model_id', how='inner')\n",
    "\n",
    "print('[model_week_tree] (new) length:', model_week_tree.count())\n",
    "#print('[model_week_mrp] (new) length:', model_week_mrp.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split model_week_sales into 3 tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_week_price = model_week_sales.select(['model_id', 'week_id', 'date', 'average_price'])\n",
    "model_week_turnover = model_week_sales.select(['model_id', 'week_id', 'date', 'sum_turnover'])\n",
    "model_week_sales = model_week_sales.select(['model_id', 'week_id', 'date', 'sales_quantity'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data checks & assertions"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "TO DO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark_write_parquet_s3(model_week_sales, bucket_refined, path_refined_global + 'model_week_sales')\n",
    "spark_write_parquet_s3(model_week_price, bucket_refined, path_refined_global + 'model_week_price')\n",
    "spark_write_parquet_s3(model_week_turnover, bucket_refined, path_refined_global + 'model_week_turnover')\n",
    "spark_write_parquet_s3(model_week_tree, bucket_refined, path_refined_global + 'model_week_tree')\n",
    "#spark_write_parquet_s3(model_week_mrp, bucket_refined, path_refined_global + 'model_week_mrp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Sparkmagic (PySpark)",
   "language": "python",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
