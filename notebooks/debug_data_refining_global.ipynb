{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%configure -f\n",
    "#{\n",
    "# \"conf\" :\n",
    "#    {\n",
    "#     \"spark.yarn.isPython\" : \"true\",\n",
    "#     \"spark.serializer\" : \"org.apache.spark.serializer.KryoSerializer\",\n",
    "#     \"spark.hadoop.mapreduce.fileoutputcommitter.algorithm.version\" : 2,\n",
    "#     \"spark.maximizeResourceAllocation\" : \"true\",\n",
    "#     \"spark.dynamicAllocation.enabled\" : \"true\",\n",
    "#     \"spark.dynamicAllocation.minExecutors\" : 2,\n",
    "#     \"spark.dynamicAllocation.maxExecutors\" : 50\n",
    "#    }\n",
    "#}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%configure -f\n",
    "{\n",
    " \"conf\" :\n",
    "    {\n",
    "    \"spark.yarn.isPython\" : \"true\",\n",
    "     \"spark.serializer\" : \"org.apache.spark.serializer.KryoSerializer\",\n",
    "     \"spark.hadoop.mapreduce.fileoutputcommitter.algorithm.version\" : 2,\n",
    "     \"spark.maximizeResourceAllocation\" : \"false\",\n",
    "     \"spark.dynamicAllocation.enabled\" : \"false\",\n",
    "     \"spark.executor.cores\" : 5,\n",
    "     \"spark.executor.memory\" : \"36g\",\n",
    "     \"spark.executor.memoryOverhead\" : \"4g\",\n",
    "     \"spark.executor.instances\" : 8,\n",
    "     \"spark.default.parallelism\" : 80,\n",
    "     \"spark.sql.shuffle.partitions\" : 80,\n",
    "     \"spark.yarn.am.cores\" : 5,\n",
    "     \"spark.yarn.am.memory\" : \"36g\",\n",
    "     \"spark.yarn.am.memoryOverhead\" : \"4g\"\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "import time\n",
    "from functools import reduce\n",
    "from datetime import datetime, timedelta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_yml(file_path):\n",
    "    \"\"\"\n",
    "    Read a local yaml file and return a python dictionary\n",
    "    :param file_path: (string) full path to the yaml file\n",
    "    :return: (dict) data loaded\n",
    "    \"\"\"\n",
    "\n",
    "    if file_path[:2] == \"s3\":\n",
    "        fs = s3fs.S3FileSystem()\n",
    "        with fs.open(file_path, 'r') as f:\n",
    "            yaml_dict = yaml.safe_load(f)\n",
    "    else:\n",
    "        with open(file_path) as f:\n",
    "            yaml_dict = yaml.safe_load(f)\n",
    "\n",
    "    return yaml_dict\n",
    "\n",
    "\n",
    "def to_uri(bucket, key):\n",
    "    \"\"\"\n",
    "    List all files under a S3 bucket\n",
    "    :param bucket: (string) name of the S3 bucket\n",
    "    :param key: (string) S3 key\n",
    "    :return: (string) URI format\n",
    "    \"\"\"\n",
    "    return 's3://{}/{}'.format(bucket, key)\n",
    "\n",
    "\n",
    "def read_parquet_s3(app, bucket, key):\n",
    "    \"\"\"\n",
    "    Read parquet files on s3 and return a spark dataframe\n",
    "    :app: (SparkSession) spark app\n",
    "    :param bucket: (string) name of the S3 bucket\n",
    "    :param key: (string) S3 key\n",
    "    :return: (SparkDataframe)\n",
    "    \"\"\"\n",
    "    df = app.read.parquet(to_uri(bucket, key))\n",
    "    return df\n",
    "\n",
    "\n",
    "def write_parquet_s3(df, bucket, key, mode='overwrite'):\n",
    "    \"\"\"\n",
    "    Write a SparkDataframe to parquet files on a S3 bucket\n",
    "    :df: (SparkDataframe)\n",
    "    :param bucket: (string) name of the S3 bucket\n",
    "    :param key: (string) S3 key\n",
    "    \"\"\"\n",
    "    df.write.parquet(to_uri(bucket, key), mode=mode)\n",
    "\n",
    "\n",
    "def pretty_print_dict(dict_to_print):\n",
    "    \"\"\"\n",
    "    Pretty prints a dictionary\n",
    "    :param dict_to_print: python dictionary\n",
    "    \"\"\"\n",
    "\n",
    "    pprint.pprint(dict_to_print)\n",
    "\n",
    "\n",
    "def get_current_week():\n",
    "    \"\"\"\n",
    "    Return current week (international standard ISO 8601 - first day of week\n",
    "    is Sunday, with format 'YYYYWW'\n",
    "    :return current week (international standard ISO 8601) with format 'YYYYWW'\n",
    "    \"\"\"\n",
    "    shifted_date = datetime.today() + timedelta(days=1)\n",
    "    current_week_id = int(str(shifted_date.isocalendar()[0]) + str(shifted_date.isocalendar()[1]).zfill(2))\n",
    "    return current_week_id\n",
    "\n",
    "\n",
    "def get_timer(starting_time):\n",
    "    \"\"\"\n",
    "    Displays the time that has elapsed between the input timer and the current time.\n",
    "    :param starting_time: (timecode) timecode from Python 'time' package\n",
    "    \"\"\"\n",
    "    \n",
    "    end_time = time.time()\n",
    "    minutes, seconds = divmod(int(end_time - starting_time), 60)\n",
    "    print(\"{} minute(s) {} second(s)\".format(int(minutes), seconds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bucket_clean = 'fcst-clean-dev'\n",
    "bucket_refined = 'fcst-refined-demand-forecast-dev'\n",
    "\n",
    "path_clean_datalake = 'datalake/'\n",
    "path_refined_global = 'global/'\n",
    "\n",
    "first_historical_week = 201507\n",
    "first_backtesting_cutoff = 201924\n",
    "current_week = get_current_week()\n",
    "    \n",
    "list_puch_org = ['Z001', 'Z002', 'Z003', 'Z004', 'Z005', 'Z006', 'Z011', 'Z012', 'Z013', 'Z017',\n",
    "                 'Z019', 'Z022', 'Z025', 'Z026', 'Z027', 'Z028', 'Z060', 'Z061', 'Z065', 'Z091', \n",
    "                 'Z093', 'Z094', 'Z095', 'Z096', 'Z102', 'Z104', 'Z105', 'Z106', 'Z112', 'Z115',\n",
    "                 'Z008', 'Z042', 'Z066', 'Z078', 'Z107', 'Z101', 'Z098']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load all needed clean data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tdt = read_parquet_s3(spark, bucket_clean, path_clean_datalake + 'f_transaction_detail/*/')\n",
    "dyd = read_parquet_s3(spark, bucket_clean, path_clean_datalake + 'f_delivery_detail/*/')\n",
    "cex = read_parquet_s3(spark, bucket_clean, path_clean_datalake + 'f_currency_exchange')\n",
    "\n",
    "sku = read_parquet_s3(spark, bucket_clean, path_clean_datalake + 'd_sku/')\n",
    "but = read_parquet_s3(spark, bucket_clean, path_clean_datalake + 'd_business_unit/')\n",
    "\n",
    "sapb = read_parquet_s3(spark, bucket_clean, path_clean_datalake + 'sites_attribut_0plant_branches_h/')\n",
    "sdm = read_parquet_s3(spark, bucket_clean, path_clean_datalake + 'd_sales_data_material_h/')\n",
    "gdw = read_parquet_s3(spark, bucket_clean, path_clean_datalake + 'd_general_data_warehouse_h/')\n",
    "\n",
    "day = read_parquet_s3(spark, bucket_clean, path_clean_datalake + 'd_day/')\n",
    "week = read_parquet_s3(spark, bucket_clean, path_clean_datalake + 'd_week/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create model_week_sales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get current CRE exchange rate\n",
    "# /!\\ TO DO: get a dynamic exchange rate when the right data source is identified\n",
    "cer = cex \\\n",
    "    .filter(cex['cpt_idr_cur_price'] == 6) \\\n",
    "    .filter(cex['cur_idr_currency_restit'] == 32) \\\n",
    "    .filter(current_timestamp().between(cex['hde_effect_date'], cex['hde_end_date'])) \\\n",
    "    .select(cex['cur_idr_currency_base'], \n",
    "            cex['cur_idr_currency_restit'],\n",
    "            cex['hde_share_price']) \\\n",
    "    .groupby(cex['cur_idr_currency_base'], \n",
    "             cex['cur_idr_currency_restit']) \\\n",
    "    .agg(mean(cex['hde_share_price']).alias('exchange_rate'))\n",
    "\n",
    "# Get offline sales\n",
    "model_week_sales_offline = tdt \\\n",
    "    .join(day, on=to_date(tdt['tdt_date_to_ordered'], 'yyyy-MM-dd') == day['day_id_day'], how='inner') \\\n",
    "    .join(week, on=day['wee_id_week'] == week['wee_id_week'], how='inner') \\\n",
    "    .join(sku, on=tdt['sku_idr_sku'] == sku['sku_idr_sku'], how='inner') \\\n",
    "    .join(but, on=tdt['but_idr_business_unit'] == but['but_idr_business_unit'], how='inner') \\\n",
    "    .join(cer, on=tdt['cur_idr_currency'] == cer['cur_idr_currency_base'], how='inner') \\\n",
    "    .join(sapb,\n",
    "          on=but['but_num_business_unit'].cast('string') == regexp_replace(sapb['plant_id'], '^0*|\\s', ''),\n",
    "          how='inner') \\\n",
    "    .filter(tdt['the_to_type'] == 'offline') \\\n",
    "    .filter(tdt['tdt_type_detail'] == 'sale') \\\n",
    "    .filter(day['wee_id_week'] >= first_historical_week) \\\n",
    "    .filter(day['wee_id_week'] < current_week) \\\n",
    "    .filter(~sku['unv_num_univers'].isin([0, 14, 89, 90])) \\\n",
    "    .filter(sku['mdl_num_model_r3'].isNotNull()) \\\n",
    "    .filter(but['but_num_typ_but'] == 7) \\\n",
    "    .filter(sapb['sapsrc'] == 'PRT') \\\n",
    "    .filter(sapb['purch_org'].isin(list_puch_org)) \\\n",
    "    .filter(current_timestamp().between(sapb['date_begin'], sapb['date_end'])) \\\n",
    "    .select(sku['mdl_num_model_r3'].alias('model_id'),\n",
    "            day['wee_id_week'].cast('int').alias('week_id'),\n",
    "            week['day_first_day_week'].alias('date'),\n",
    "            tdt['f_qty_item'],\n",
    "            tdt['f_pri_regular_sales_unit'],\n",
    "            tdt['f_to_tax_in'],\n",
    "            cer['exchange_rate'])\n",
    "\n",
    "# Get online sales\n",
    "model_week_sales_online = dyd \\\n",
    "    .join(day, on=to_date(dyd['tdt_date_to_ordered'], 'yyyy-MM-dd') == day['day_id_day'], how='inner') \\\n",
    "    .join(week, on=day['wee_id_week'] == week['wee_id_week'], how='inner') \\\n",
    "    .join(sku, on=dyd['sku_idr_sku'] == sku['sku_idr_sku'], how='inner') \\\n",
    "    .join(but, on=dyd['but_idr_business_unit_economical'] == but['but_idr_business_unit'], how='inner') \\\n",
    "    .join(cer, on=dyd['cur_idr_currency'] == cer['cur_idr_currency_base'], how='inner') \\\n",
    "    .join(sapb,\n",
    "          on=but['but_num_business_unit'].cast('string') == regexp_replace(sapb['plant_id'], '^0*|\\s', ''),\n",
    "          how='inner') \\\n",
    "    .filter(dyd['the_to_type'] == 'online') \\\n",
    "    .filter(dyd['tdt_type_detail'] == 'sale') \\\n",
    "    .filter(day['wee_id_week'] >= first_historical_week) \\\n",
    "    .filter(day['wee_id_week'] < current_week) \\\n",
    "    .filter(~sku['unv_num_univers'].isin([0, 14, 89, 90])) \\\n",
    "    .filter(sku['mdl_num_model_r3'].isNotNull()) \\\n",
    "    .filter(but['but_num_typ_but'] == 7) \\\n",
    "    .filter(sapb['sapsrc'] == 'PRT') \\\n",
    "    .filter(sapb['purch_org'].isin(list_puch_org)) \\\n",
    "    .filter(current_timestamp().between(sapb['date_begin'], sapb['date_end'])) \\\n",
    "    .select(sku['mdl_num_model_r3'].alias('model_id'),\n",
    "            day['wee_id_week'].cast('int').alias('week_id'),\n",
    "            week['day_first_day_week'].alias('date'),\n",
    "            dyd['f_qty_item'],\n",
    "            dyd['f_tdt_pri_regular_sales_unit'].alias('f_pri_regular_sales_unit'),\n",
    "            dyd['f_to_tax_in'],\n",
    "            cer['exchange_rate'])\n",
    "\n",
    "# Create model week sales\n",
    "model_week_sales = model_week_sales_offline.union(model_week_sales_online) \\\n",
    "    .groupby(['model_id', 'week_id', 'date']) \\\n",
    "    .agg(sum('f_qty_item').alias('sales_quantity'),\n",
    "         mean(col('f_pri_regular_sales_unit') * col('exchange_rate')).alias('average_price'),\n",
    "         sum(col('f_to_tax_in') * col('exchange_rate')).alias('sum_turnover')) \\\n",
    "    .filter(col('sales_quantity') > 0) \\\n",
    "    .filter(col('average_price') > 0) \\\n",
    "    .filter(col('sum_turnover') > 0) \\\n",
    "    .orderBy('model_id', 'week_id') \\\n",
    "    .cache()\n",
    "\n",
    "print(\"====> counting(cache) [model_week_sales] took \")\n",
    "start = time.time()\n",
    "model_week_sales_count = model_week_sales.count()\n",
    "get_timer(starting_time=start)\n",
    "print(\"[model_week_sales] length:\", model_week_sales_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create model_week_tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_week_tree = sku \\\n",
    "    .join(week, on=week['day_first_day_week'].between(sku['sku_date_begin'], sku['sku_date_end']), how='inner') \\\n",
    "    .filter(sku['sku_num_sku_r3'].isNotNull()) \\\n",
    "    .filter(sku['mdl_num_model_r3'].isNotNull()) \\\n",
    "    .filter(sku['fam_num_family'].isNotNull()) \\\n",
    "    .filter(sku['sdp_num_sub_department'].isNotNull()) \\\n",
    "    .filter(sku['dpt_num_department'].isNotNull()) \\\n",
    "    .filter(sku['unv_num_univers'].isNotNull()) \\\n",
    "    .filter(sku['pnt_num_product_nature'].isNotNull()) \\\n",
    "    .filter(~sku['unv_num_univers'].isin([0, 14, 89, 90])) \\\n",
    "    .filter(week['wee_id_week'] >= first_backtesting_cutoff) \\\n",
    "    .filter(week['wee_id_week'] < current_week) \\\n",
    "    .groupBy(week['wee_id_week'].cast('int').alias('week_id'),\n",
    "             sku['mdl_num_model_r3'].alias('model_id'),\n",
    "             sku['fam_num_family'].alias('family_id'),\n",
    "             sku['sdp_num_sub_department'].alias('sub_department_id'),\n",
    "             sku['dpt_num_department'].alias('department_id'),\n",
    "             sku['unv_num_univers'].alias('univers_id'),\n",
    "             sku['pnt_num_product_nature'].alias('product_nature_id')) \\\n",
    "    .agg(max(when(sku['mdl_label'].isNull(), 'UNKNOWN').otherwise(sku['mdl_label'])).alias('model_label'),\n",
    "         max(sku['family_label']).alias('family_label'),\n",
    "         max(sku['sdp_label']).alias('sub_department_label'),\n",
    "         max(sku['dpt_label']).alias('department_label'),\n",
    "         max(sku['unv_label']).alias('univers_label'),\n",
    "         max(when(sku['product_nature_label'].isNull(), \n",
    "                  'UNDEFINED').otherwise(sku['product_nature_label'])).alias('product_nature_label'),\n",
    "         max(sku['brd_label_brand']).alias('brand_label'),\n",
    "         max(sku['brd_type_brand_libelle']).alias('brand_type')) \\\n",
    "    .orderBy('week_id', 'model_id') \\\n",
    "    .cache()\n",
    "\n",
    "print(\"====> counting(cache) [model_week_tree] took \")\n",
    "start = time.time()\n",
    "model_week_tree_count = model_week_tree.count()\n",
    "get_timer(starting_time=start)\n",
    "print(\"[model_week_tree] length:\", model_week_tree_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create model_week_mrp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get sku mrp update\n",
    "smu = gdw \\\n",
    "    .join(sapb, on=gdw['sdw_plant_id'] == sapb['plant_id'], how='inner') \\\n",
    "    .join(sku, on=sku['sku_num_sku_r3'] == regexp_replace(gdw['sdw_material_id'], '^0*|\\s', ''), how='inner') \\\n",
    "    .filter(gdw['sdw_sap_source'] == 'PRT') \\\n",
    "    .filter(gdw['sdw_material_mrp'] != '    ') \\\n",
    "    .filter(sapb['sapsrc'] == 'PRT') \\\n",
    "    .filter(sapb['purch_org'].isin(list_puch_org)) \\\n",
    "    .filter(current_timestamp().between(sapb['date_begin'], sapb['date_end'])) \\\n",
    "    .filter(sku['mdl_num_model_r3'].isNotNull()) \\\n",
    "    .filter(~sku['unv_num_univers'].isin([0, 14, 89, 90])) \\\n",
    "    .filter(current_timestamp().between(sku['sku_date_begin'], sku['sku_date_end'])) \\\n",
    "    .select(gdw['date_begin'],\n",
    "            gdw['date_end'],\n",
    "            sku['sku_num_sku_r3'].alias('sku_id'),\n",
    "            sku['mdl_num_model_r3'].alias('model_id'),\n",
    "            gdw['sdw_material_mrp'].cast('int').alias('mrp')) \\\n",
    "    .drop_duplicates()\n",
    "\n",
    "# calculate model week mrp\n",
    "model_week_mrp = smu \\\n",
    "    .join(day, on=day['day_id_day'].between(smu['date_begin'], smu['date_end']), how='inner') \\\n",
    "    .filter(day['wee_id_week'] >= '201939') \\\n",
    "    .filter(day['wee_id_week'] < current_week) \\\n",
    "    .groupBy(day['wee_id_week'].cast('int').alias('week_id'),\n",
    "             smu['model_id']) \\\n",
    "    .agg(max(when(smu['mrp'].isin(2, 5), True).otherwise(False)).alias('is_mrp_active')) \\\n",
    "    .orderBy('model_id', 'week_id') \\\n",
    "    .cache()\n",
    "\n",
    "print(\"====> counting(cache) [model_week_mrp] took \")\n",
    "start = time.time()\n",
    "model_week_mrp_count = model_week_mrp.count()\n",
    "get_timer(starting_time=start)\n",
    "print(\"[model_week_mrp] length:\", model_week_mrp_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reduce tables according to the models found in model_week_sales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"====> Reducing tables according to the models found in model_week_sales...\")\n",
    "\n",
    "model_week_tree = model_week_tree.join(model_week_sales.select('model_id').drop_duplicates(), on='model_id',  how='inner')\n",
    "model_week_mrp = model_week_mrp.join(model_week_sales.select('model_id').drop_duplicates(), on='model_id',  how='inner')\n",
    "\n",
    "print(\"[model_week_tree] (new) length:\", model_week_tree.count())\n",
    "print(\"[model_week_mrp] (new) length:\", model_week_mrp.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fill missing MRP\n",
    "MRP are available since 201939 only.  \n",
    "We have to fill weeks between 201924 and 201938 using the 201939 values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"====> Filling missing MRP...\")\n",
    "\n",
    "model_week_mrp_201939 = model_week_mrp.filter(model_week_mrp['week_id'] == 201939)\n",
    "\n",
    "l_df = []\n",
    "for w in range(201924, 201939):\n",
    "    df = model_week_mrp_201939.withColumn('week_id', lit(w))\n",
    "    l_df.append(df)\n",
    "l_df.append(model_week_mrp)\n",
    "\n",
    "def unionAll(dfs):\n",
    "    return reduce(lambda df1, df2: df1.union(df2.select(df1.columns)), dfs)\n",
    "\n",
    "model_week_mrp = unionAll(l_df) \\\n",
    "    .coalesce(int(spark.conf.get(\"spark.sql.shuffle.partitions\"))) \\\n",
    "    .cache()\n",
    "\n",
    "print(\"[model_week_mrp] (new) length:\", model_week_mrp.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split sales, price & turnover into 3 tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"====> Spliting sales, price & turnover into 3 tables...\")\n",
    "\n",
    "model_week_price = model_week_sales.select(['model_id', 'week_id', 'date', 'average_price'])\n",
    "model_week_turnover = model_week_sales.select(['model_id', 'week_id', 'date', 'sum_turnover'])\n",
    "model_week_sales = model_week_sales.select(['model_id', 'week_id', 'date', 'sales_quantity'])\n",
    "\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save refined global tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check duplicates rows\n",
    "assert model_week_sales.groupBy(['model_id', 'week_id', 'date']).count().select(max(\"count\")).collect()[0][0] == 1\n",
    "assert model_week_price.groupBy(['model_id', 'week_id', 'date']).count().select(max(\"count\")).collect()[0][0] == 1\n",
    "assert model_week_turnover.groupBy(['model_id', 'week_id', 'date']).count().select(max(\"count\")).collect()[0][0] == 1\n",
    "assert model_week_tree.groupBy(['model_id', 'week_id']).count().select(max(\"count\")).collect()[0][0] == 1\n",
    "assert model_week_mrp.groupBy(['model_id', 'week_id']).count().select(max(\"count\")).collect()[0][0] == 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write\n",
    "print(\"====> Writing table [model_week_sales]\")\n",
    "start = time.time()\n",
    "write_parquet_s3(model_week_sales, bucket_refined, path_refined_global + 'model_week_sales')\n",
    "get_timer(starting_time=start)\n",
    "\n",
    "print(\"====> Writing table [model_week_price]\")\n",
    "start = time.time()\n",
    "write_parquet_s3(model_week_price, bucket_refined, path_refined_global + 'model_week_price')\n",
    "get_timer(starting_time=start)\n",
    "\n",
    "print(\"====> Writing table [model_week_turnover]\")\n",
    "start = time.time()\n",
    "write_parquet_s3(model_week_turnover, bucket_refined, path_refined_global + 'model_week_turnover')\n",
    "get_timer(starting_time=start)\n",
    "\n",
    "print(\"====> Writing table [model_week_tree]\")\n",
    "start = time.time()\n",
    "write_parquet_s3(model_week_tree, bucket_refined, path_refined_global + 'model_week_tree')\n",
    "get_timer(starting_time=start)\n",
    "\n",
    "print(\"====> Writing table [model_week_mrp]\")\n",
    "start = time.time()\n",
    "write_parquet_s3(model_week_mrp, bucket_refined, path_refined_global + 'model_week_mrp')\n",
    "get_timer(starting_time=start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Sparkmagic (PySpark)",
   "language": "",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 3
   },
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
