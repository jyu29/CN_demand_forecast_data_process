pipeline {

    agent any

    environment {
        key_pem = 'forecast-emr.pem'
        cluster_name = 'forecast-data-refining-demand'
        jenkins_job = 'forecast-data-refining-demand'
        cluster_id = '10.226.39.128'
    }

    stages {

        stage('spark app deployment and execution') {
            
            steps {
                
                wrap([$class: 'BuildUser']) {
                
                    sh('''
                    
                    export https_proxy=http://proxy-internet-aws-eu.subsidia.org:3128
                    
                    EMRName=$"forecast-dev-emr-${cluster_name}-${BUILD_USER}"
                    
                    cluster_id=$(aws emr list-clusters --active  --output=json | jq '.Clusters[] | select(.Name=="'${EMRName}'") | .Id ' -r)
                    
                    instance_fleet_id=$(aws emr describe-cluster --cluster-id ${cluster_id}  --output=json | jq '.Cluster.InstanceFleets[] | select(.InstanceFleetType=="MASTER") | .Id ' -r)
                    
                    master_ip=$(aws emr list-instances --cluster-id ${cluster_id}   --output=json | jq '.Instances[] | select(.InstanceFleetId=="'${instance_fleet_id}'") | .PrivateIpAddress ' -r)
    
                    scp -r -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -i /var/lib/jenkins/.ssh/${key_pem} ${WORKSPACE} hadoop@${master_ip}:/home/hadoop
    
                    ssh hadoop@${master_ip} -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -i /var/lib/jenkins/.ssh/${key_pem} "sudo chmod 755 /home/hadoop/${jenkins_job}/spark_submit_global.sh /home/hadoop/${jenkins_job}/spark_submit_specific.sh; export PYSPARK_PYTHON='/usr/bin/python3'; cd ${jenkins_job}; ./spark_submit_global.sh ${Environment}; ./spark_submit_specific.sh ${Environment}"
                    ''')
                }
            }
        }
    }
}