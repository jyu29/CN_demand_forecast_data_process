buckets:
  clean: fcst-clean-prod
  refined: fcst-workspace
paths:
  clean_datalake: datalake/
  refined_global: forecast-cn/fcst-refined-demand-forecast-dev/global/
  tableau: forecast-cn/fcst-data-exchange-dev/dashboard/tableau/raw_name/
functional_parameters:
  # if but_range is True, but_week must be two number, one is start, another one is end.
  but_range: False
  but_week:
    - 202101
    - 202102
    - 202103
    - 202104
    - 202105
    - 202106
    - 202107
    - 202108
    - 202109
    - 202110
    - 202111
    - 202112
    - 202113
    - 202114
    - 202115
    - 202116
    - 202117
    - 202118
    - 202119
    - 202120
    - 202121
    - 202122
    - 202123
    - 202124
    - 202125
    - 202126
    - 202127
    - 202128
    - 202129
    - 202130
    - 202131
    - 202132
    - 202133
    - 202134
    - 202135
    - 202136
    - 202137
    - 202138
    - 202139
    - 202140
    - 202141
    - 202142
    - 202143
    - 202144
    - 202145
    - 202146
    - 202147
    - 202148
    - 202149
    - 202150
    - 202151
    - 202152
    - 202201
    - 202202
    - 202203
    - 202204
    - 202205
    - 202206
    - 202207
    - 202208
    - 202209
    - 202210
  first_historical_week: 201601
  first_backtesting_cutoff: 201901
  list_purch_org:
    - Z015
    - Z024
    - Z067
    - Z069
    - Z108
technical_parameters:
  spark_conf:
    spark.yarn.isPython: "true"
    spark.serializer: "org.apache.spark.serializer.KryoSerializer"
    spark.sql.legacy.parquet.int96RebaseModeInRead: "CORRECTED"
    spark.sql.legacy.parquet.datetimeRebaseModeInWrite: "CORRECTED"
    spark.sql.legacy.parquet.datetimeRebaseModeInRead: "CORRECTED"
    spark.sql.legacy.timeParserPolicy: "LEGACY"
    spark.driver.maxResultSize: 8g
