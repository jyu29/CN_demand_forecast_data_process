buckets:
  clean: fcst-clean-prod
  refined: fcst-refined-demand-forecast-prod
paths:
  clean_datalake: datalake/
  refined_global: global/
functional_parameters:
  first_historical_week: 201507
  first_backtesting_cutoff: 201924
  list_puch_org:
  - Z001
  - Z002
  - Z003
  - Z004
  - Z005
  - Z006
  - Z011
  - Z012
  - Z013
  - Z017
  - Z019
  - Z022
  - Z025
  - Z026
  - Z027
  - Z028
  - Z060
  - Z061
  - Z065
  - Z091
  - Z093
  - Z094
  - Z095
  - Z096
  - Z102
  - Z104
  - Z105
  - Z106
  - Z112
  - Z115
technical_parameters:
    spark_conf:
        spark.yarn.isPython : "true"
        spark.serializer : "org.apache.spark.serializer.KryoSerializer"
        spark.maximizeResourceAllocation : "false"
        spark.dynamicAllocation.enabled : "false"
        spark.executor.cores : 5
        spark.executor.memory : "36g"
        spark.executor.memoryOverhead : "4g"
        spark.executor.instances : 8
        spark.default.parallelism : 80
        spark.sql.shuffle.partitions : 80
        spark.driver.cores : 5
        spark.driver.memory : "36g"
        spark.driver.memoryOverhead : "4g"
        spark.memory.fraction : 0.2
        spark.memory.storageFraction : 0.8
        spark.executor.extraJavaOptions : "-XX:+UseG1GC -XX:+UnlockDiagnosticVMOptions -XX:+G1SummarizeConcMark -XX:InitiatingHeapOccupancyPercent=35 -verbose:gc -XX:+PrintGCDetails -XX:+PrintGCDateStamps -XX:OnOutOfMemoryError='kill -9 %p'"
        spark.driver.extraJavaOptions : "-XX:+UseG1GC -XX:+UnlockDiagnosticVMOptions -XX:+G1SummarizeConcMark -XX:InitiatingHeapOccupancyPercent=35 -verbose:gc -XX:+PrintGCDetails -XX:+PrintGCDateStamps -XX:OnOutOfMemoryError='kill -9 %p'"
        yarn.nodemanager.vmem-check-enabled : "false"
        yarn.nodemanager.pmem-check-enabled : "false"